import copy
import os
import sys
import time
from datetime import datetime
import logging
import gc

import numpy as np
import torch
from torch.utils.data import DataLoader
from tqdm import tqdm

from config.globals import set_seed
from models.alexnet import AlexNet
from models.resnet import resnet18
from utils.args import parser_args
from utils.base import Experiment
from utils.dataset import get_data, DatasetSplit
from utils.trainer_ablation import TrainerAblation
from utils.trainer_private import TesterPrivate
import pandas as pd

set_seed()

# é…ç½® logging
args = parser_args()
log_file_name = args.log_file.replace('.logs', '_ablation.logs')  # æ¶ˆèå®éªŒä½¿ç”¨ä¸åŒçš„æ—¥å¿—æ–‡ä»¶
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    datefmt='%Y-%m-%d %H-%M-%S',
    handlers=[
        logging.StreamHandler(sys.stdout),
        logging.FileHandler(log_file_name, mode='a', encoding='utf-8')
    ]
)


class AblationExperiment(Experiment):
    """
    æ¶ˆèå®éªŒï¼šåªä½¿ç”¨ä¸»ä»»åŠ¡æŸå¤±ï¼Œä¸ä½¿ç”¨ä¸‰ä¸ªæ­£åˆ™é¡¹
    ä½†ä¿æŒæ°´å°åµŒå…¥é€»è¾‘ä¸å˜ï¼ˆä½¿ç”¨KeyMatrixManagerè¿›è¡Œå‚æ•°æ›¿æ¢ï¼‰
    """
    
    def __init__(self, args):
        super().__init__(args)
        
        self.args = args
        self.dp = args.dp
        self.sigma = args.sigma
        self.key_matrix_dir = getattr(args, 'key_matrix_dir', './save/key_matrix')
        
        logging.info('='*60)
        logging.info('æ¶ˆèå®éªŒï¼šåªä½¿ç”¨ä¸»ä»»åŠ¡æŸå¤±ï¼Œä¸ä½¿ç”¨ä¸‰ä¸ªæ­£åˆ™é¡¹')
        logging.info('æ°´å°åµŒå…¥é€»è¾‘ä¿æŒä¸å˜ï¼ˆä½¿ç”¨KeyMatrixManagerè¿›è¡Œå‚æ•°æ›¿æ¢ï¼‰')
        logging.info('='*60)
        logging.info('--------------------------------Start--------------------------------------')
        logging.info(args)
        logging.info('==> Preparing data...')
        
        # æ•°æ®é›†é…ç½®
        self.num_classes = args.num_classes
        self.in_channels = args.in_channels
            
        # ç¡®ä¿æ•°æ®æ ¹ç›®å½•å­˜åœ¨
        os.makedirs(self.data_root, exist_ok=True)

        self.train_set, self.test_set, self.dict_users = get_data(dataset_name=self.dataset,
                                                                  data_root=self.data_root,
                                                                  iid=self.iid,
                                                                  client_num=self.client_num,
                                                                  )
        logging.info('==> Training model...')
        self.logs = {'best_train_acc': -np.inf, 'best_train_loss': -np.inf,
                     'val_acc': [], 'val_loss': [],
                     'best_model_acc': -np.inf, 'best_model_loss': -np.inf,
                     'best_model_auc': -np.inf,
                     'best_model': [],
                     'local_loss': [],
                     # ç‹¬ç«‹è·Ÿè¸ªå†å²æœ€é«˜æŒ‡æ ‡
                     'highest_acc_ever': -np.inf,
                     'highest_auc_ever': -np.inf,
                     'acc_when_highest_auc': -np.inf,
                     'auc_when_highest_acc': -np.inf,
                     }

        self.construct_model()
        self.w_t = copy.deepcopy(self.model.state_dict())

        # è®¾ç½®å¯†é’¥çŸ©é˜µç›®å½•
        self.args.key_matrix_dir = self.key_matrix_dir
        self.args.use_key_matrix = True
        
        # ä½¿ç”¨æ¶ˆèå®éªŒè®­ç»ƒå™¨ï¼ˆåªä½¿ç”¨ä¸»ä»»åŠ¡æŸå¤±ï¼‰
        logging.info('==> ä½¿ç”¨æ¶ˆèå®éªŒè®­ç»ƒå™¨ï¼ˆåªä½¿ç”¨ä¸»ä»»åŠ¡æŸå¤±ï¼Œä¸ä½¿ç”¨æ­£åˆ™é¡¹ï¼‰')
        self.trainer = TrainerAblation(self.model, self.device, self.dp, self.sigma, self.args)
        self.tester = TesterPrivate(self.model, self.device, args=self.args)

    def construct_model(self):
        if self.model_name == 'resnet':
            model = resnet18(num_classes=self.num_classes, in_channels=self.in_channels, input_size=self.args.input_size)
        else:
            dropout_rate = getattr(self.args, 'dropout_rate', 0.5)
            model = AlexNet(self.in_channels, self.num_classes, input_size=self.args.input_size, dropout_rate=dropout_rate)
        self.model = model.to(self.device)

    def _cleanup_memory(self):
        """æ¸…ç†å†…å­˜å’ŒGPUç¼“å­˜"""
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        gc.collect()

    def _optimize_model_storage(self, model_state):
        """ä¼˜åŒ–æ¨¡å‹çŠ¶æ€å­˜å‚¨ï¼Œå‡å°‘å†…å­˜å ç”¨"""
        optimized_state = {}
        for key, value in model_state.items():
            optimized_state[key] = value.detach().cpu()
        return optimized_state

    def training(self):
        start = time.time()
        train_ldr = DataLoader(self.train_set, batch_size=self.batch_size, shuffle=False, num_workers=0, pin_memory=False)
        val_ldr = DataLoader(self.test_set, batch_size=self.batch_size * 2, shuffle=False, num_workers=0, pin_memory=False)

        local_train_loader = []

        for i in range(self.client_num):
            local_train_ldr = DataLoader(DatasetSplit(self.train_set, self.dict_users[i]),
                                         batch_size=self.batch_size,
                                         shuffle=True, num_workers=0, pin_memory=False)
            local_train_loader.append(local_train_ldr)

        idxs_users = []

        # Early Stopping é…ç½®
        patience = 10
        early_stop_counter = 0
        best_val_acc = -np.inf
        best_val_auc = -np.inf

        # å†³å®šæ¨¡å‹é€‰æ‹©ä¾æ®ï¼šChestMNIST æŒ‰ AUCï¼Œå…¶ä»–æŒ‰å‡†ç¡®ç‡
        dataset_key = (self.dataset or '').lower()
        select_by_auc = (dataset_key == 'chestmnist')

        # ç»Ÿè®¡è®°å½•
        stats_rows = []

        for epoch in range(self.epochs):
            # å‡åŒ€é‡‡æ ·
            self.m = max(int(self.frac * self.client_num), 1)
            idxs_users = np.random.choice(range(self.client_num), self.m, replace=False)

            local_ws, local_losses = [], []

            logging.info('Epoch: %d / %d' % (epoch + 1, self.epochs))
            
            # æ¶ˆèå®éªŒï¼šä¸éœ€è¦è‡ªç¼–ç å™¨å¾®è°ƒï¼Œåªè¿›è¡Œæ°´å°å‚æ•°æ›¿æ¢
            
            for i, idx in enumerate(tqdm(idxs_users, desc='Progress: %d / %d' % (epoch + 1, self.epochs))):
                self.model.load_state_dict(self.w_t)

                # è°ƒç”¨æ¶ˆèå®éªŒè®­ç»ƒå™¨
                local_w, local_loss, local_acc = self.trainer.local_update(
                    dataloader=local_train_loader[idx], 
                    local_ep=self.local_ep, 
                    lr=self.lr, 
                    client_id=idx,
                    current_epoch=epoch,
                    total_epochs=self.epochs
                )

                local_ws.append(copy.deepcopy(local_w))
                local_losses.append(local_loss)
                
                # æ¸…ç†ä¸´æ—¶å˜é‡
                del local_w, local_loss, local_acc

            # è®¡ç®—å‚ä¸è®­ç»ƒçš„å®¢æˆ·ç«¯çš„æƒé‡
            client_weights = []
            for idx in idxs_users:
                client_weight = len(DatasetSplit(self.train_set, self.dict_users[idx])) / len(self.train_set)
                client_weights.append(client_weight)

            # æ›´æ–°å…¨å±€æ¨¡å‹æƒé‡
            self._fed_avg(local_ws, client_weights, idxs_users)
            
            # åŠ è½½èšåˆåçš„æƒé‡
            self.model.load_state_dict(self.w_t)
            self.model.train()
            
            # æ¸…ç†ä¼˜åŒ–å™¨çŠ¶æ€
            if hasattr(self.trainer, 'optimizer') and self.trainer.optimizer is not None:
                self.trainer.optimizer.zero_grad()

            # è¯„ä¼°æ¨¡å‹
            if (epoch + 1) == self.epochs or (epoch + 1) % 1 == 0:
                train_metrics = self.trainer.test(train_ldr)
                val_metrics = self.trainer.test(val_ldr)

                # (loss, acc_label, auc, acc_sample)
                loss_train_mean, acc_train_label_mean, auc_train, acc_train_sample_mean = train_metrics
                loss_val_mean, acc_val_label_mean, auc_val, acc_val_sample_mean = val_metrics

                self.logs['val_acc'].append(acc_val_label_mean)
                self.logs['val_loss'].append(loss_val_mean)
                self.logs['local_loss'].append(np.mean(local_losses))

                # æ›´æ–°å†å²æœ€é«˜å€¼è·Ÿè¸ª
                if self.logs['highest_acc_ever'] < acc_val_label_mean:
                    self.logs['highest_acc_ever'] = acc_val_label_mean
                    self.logs['auc_when_highest_acc'] = auc_val
                    
                if self.logs['highest_auc_ever'] < auc_val:
                    self.logs['highest_auc_ever'] = auc_val
                    self.logs['acc_when_highest_auc'] = acc_val_label_mean

                # æ¨¡å‹é€‰æ‹©æ ‡å‡†ï¼šChestMNIST æŒ‰ AUCï¼›å¦åˆ™æŒ‰å‡†ç¡®ç‡
                if select_by_auc:
                    if self.logs['best_model_auc'] < auc_val:
                        self.logs['best_model_acc'] = acc_val_label_mean
                        self.logs['best_model_loss'] = loss_val_mean
                        self.logs['best_model_auc'] = auc_val
                        optimized_state = self._optimize_model_storage(self.model.state_dict())
                        self.logs['best_model'] = [optimized_state]
                        logging.info(f'ğŸŒŸ æœ€ä½³æ¨¡å‹å·²ä¿å­˜! AUCâ†‘{auc_val:.4f}')
                else:
                    if self.logs['best_model_acc'] < acc_val_label_mean:
                        self.logs['best_model_acc'] = acc_val_label_mean
                        self.logs['best_model_loss'] = loss_val_mean
                        self.logs['best_model_auc'] = auc_val
                        optimized_state = self._optimize_model_storage(self.model.state_dict())
                        self.logs['best_model'] = [optimized_state]
                        logging.info(f'ğŸŒŸ æœ€ä½³æ¨¡å‹å·²ä¿å­˜! ACCâ†‘{acc_val_label_mean:.4f}')

                if self.logs['best_train_acc'] < acc_train_label_mean:
                    self.logs['best_train_acc'] = acc_train_label_mean
                    self.logs['best_train_loss'] = loss_train_mean

                # åˆå¹¶è®­ç»ƒå’ŒéªŒè¯æŒ‡æ ‡åˆ°ä¸€è¡Œ
                logging.info(
                    f"è½®æ¬¡{epoch+1} | Train Loss:{loss_train_mean:.4f} Acc:{acc_train_label_mean:.4f} AUC:{auc_train:.4f} | "
                    f"Val Loss:{loss_val_mean:.4f} Acc:{acc_val_label_mean:.4f} AUC:{auc_val:.4f} | "
                    f"Best Acc:{self.logs['highest_acc_ever']:.4f} Best AUC:{self.logs['highest_auc_ever']:.4f}")
                
                # è®°å½•æœ¬è½®ç»Ÿè®¡æ•°æ®ï¼ˆæ¶ˆèå®éªŒä¸åŒ…å«MultiLossç»Ÿè®¡ï¼‰
                stats_row = {
                    'round': epoch + 1,
                    'lr': self.lr,
                    'train_loss': float(loss_train_mean),
                    'val_loss': float(loss_val_mean),
                    'train_acc_label': float(acc_train_label_mean),
                    'train_auc': float(auc_train),
                    'val_acc_label': float(acc_val_label_mean),
                    'val_auc': float(auc_val),
                    'best_val_acc_so_far': float(self.logs['highest_acc_ever']),
                    'best_val_auc_so_far': float(self.logs['highest_auc_ever']),
                    'train_acc_sample': float(acc_train_sample_mean),
                    'val_acc_sample': float(acc_val_sample_mean),
                }
                
                # Early Stoppingï¼šChestMNIST åŸºäº AUCï¼Œå…¶å®ƒæ•°æ®é›†åŸºäºå‡†ç¡®ç‡
                if select_by_auc:
                    if auc_val > best_val_auc:
                        best_val_auc = auc_val
                        early_stop_counter = 0
                    else:
                        early_stop_counter += 1
                        if early_stop_counter >= patience:
                            logging.info(f'Early stopping triggered at epoch {epoch + 1}. Best Val AUC: {best_val_auc:.4f}')
                            break
                else:
                    if acc_val_label_mean > best_val_acc:
                        best_val_acc = acc_val_label_mean
                        early_stop_counter = 0
                    else:
                        early_stop_counter += 1
                        if early_stop_counter >= patience:
                            logging.info(f'Early stopping triggered at epoch {epoch + 1}. Best Val ACC: {best_val_acc:.4f}')
                            break
                
                # æ¯è½®è®­ç»ƒåæ¸…ç†å†…å­˜
                self._cleanup_memory()
                
                # æ¸…ç†ä¸´æ—¶å˜é‡
                del train_metrics, val_metrics
                del loss_train_mean, acc_train_label_mean, auc_train, acc_train_sample_mean
                del loss_val_mean, acc_val_label_mean, auc_val, acc_val_sample_mean
                
                stats_rows.append(stats_row)

        logging.info('='*60 + ' è®­ç»ƒç»“æœ ' + '='*60)
        logging.info(
            f'æœ€ä½³æ¨¡å‹ | Loss:{self.logs["best_model_loss"]:.4f} Acc:{self.logs["best_model_acc"]:.4f} AUC:{self.logs["best_model_auc"]:.4f}')
        logging.info(
            f'å†å²æœ€é«˜ | Acc:{self.logs["highest_acc_ever"]:.4f}(AUC:{self.logs["auc_when_highest_acc"]:.4f}) | '
            f'AUC:{self.logs["highest_auc_ever"]:.4f}(Acc:{self.logs["acc_when_highest_auc"]:.4f})')
        end = time.time()
        logging.info(f'è®­ç»ƒè€—æ—¶: {(end - start) / 60:.1f} åˆ†é’Ÿ')
        
        # æœ€ç»ˆå†…å­˜æ¸…ç†
        self._cleanup_memory()
        logging.info('ğŸ§¹ è®­ç»ƒå®Œæˆï¼Œå·²æ¸…ç†å†…å­˜ç¼“å­˜')

        # å¯¼å‡ºExcel
        try:
            os.makedirs(self.args.save_excel_dir, exist_ok=True)
            # æ¶ˆèå®éªŒåªåŒ…å«åŸºç¡€åˆ—ï¼Œä¸åŒ…å«MultiLossç»Ÿè®¡
            columns = ['round', 'lr', 'train_loss', 'val_loss', 'train_acc_label', 'train_auc', 
                     'val_acc_label', 'val_auc', 'best_val_acc_so_far', 'best_val_auc_so_far', 
                     'train_acc_sample', 'val_acc_sample']
            df = pd.DataFrame(stats_rows, columns=columns)
            now = datetime.now().strftime('%Y%m%d%H%M%S')
            excel_path = f'{self.args.save_excel_dir}/metrics_ablation_{self.model_name}_{self.dataset}_{now}.xlsx'
            df.to_excel(excel_path, index=False, engine='openpyxl')
            logging.info(f'Excel metrics saved to: {excel_path}')
        except Exception as e:
            logging.warning(f'Failed to export Excel metrics: {e}')

        # è¿”å›ç”¨äºæ¨¡å‹é€‰æ‹©çš„æŒ‡æ ‡
        best_metric_value = self.logs['best_model_auc'] if select_by_auc else self.logs['best_model_acc']
        best_metric_name = 'auc' if select_by_auc else 'acc'
        return self.logs, best_metric_value, best_metric_name

    def _fed_avg(self, local_ws, client_weights, idxs_users):
        """è”é‚¦å¹³å‡ç®—æ³•ï¼ŒFedAvg with exclusive watermark aggregation"""
        # è®¡ç®—å‚ä¸è®­ç»ƒçš„å®¢æˆ·ç«¯æƒé‡æ€»å’Œ
        total_weight = sum(client_weights)
        
        # å½’ä¸€åŒ–æƒé‡
        normalized_weights = [w / total_weight for w in client_weights]
        
        # éªŒè¯æƒé‡å’Œæ˜¯å¦ä¸º1
        weight_sum = sum(normalized_weights)
        if abs(weight_sum - 1.0) > 1e-6:
            logging.warning(f"Weight sum is {weight_sum:.6f}, not 1.0. Normalizing...")
            normalized_weights = [w / weight_sum for w in normalized_weights]
        
        # åˆå§‹åŒ–å¹³å‡æƒé‡
        w_avg = {}
        for k in local_ws[0].keys():
            w_avg[k] = local_ws[0][k].clone() * normalized_weights[0]

        # ç´¯åŠ å…¶ä»–å®¢æˆ·ç«¯çš„æƒé‡
        for i in range(1, len(local_ws)):
            for k in w_avg.keys():
                w_avg[k] += local_ws[i][k] * normalized_weights[i]

        # æ°´å°èšåˆï¼šä½¿ç”¨å¯†é’¥çŸ©é˜µçš„ç‹¬å å¼èšåˆï¼ˆæ¶ˆèå®éªŒä¿æŒæ­¤é€»è¾‘ä¸å˜ï¼‰
        self._watermark_aggregation(local_ws, idxs_users, w_avg)

        # æ›´æ–°å…¨å±€æ¨¡å‹æƒé‡
        for k in w_avg.keys():
            self.w_t[k] = w_avg[k]

    def _watermark_aggregation(self, local_ws, idxs_users, w_avg):
        """
        æ°´å°èšåˆï¼šä½¿ç”¨å¯†é’¥çŸ©é˜µçš„ç‹¬å å¼èšåˆ
        æ¶ˆèå®éªŒä¿æŒæ­¤é€»è¾‘ä¸å˜
        """
        try:
            from utils.key_matrix_utils import KeyMatrixManager
            
            # åŠ è½½å¯†é’¥çŸ©é˜µç®¡ç†å™¨
            key_manager = KeyMatrixManager(self.args.key_matrix_path, args=self.args)
            
            # å¯¹æ¯ä¸ªå®¢æˆ·ç«¯çš„æ°´å°ä½ç½®è¿›è¡Œç‹¬å å¼èšåˆ
            for i, client_id in enumerate(idxs_users):
                try:
                    # è·å–è¯¥å®¢æˆ·ç«¯çš„æ°´å°ä½ç½®
                    positions = key_manager.load_positions(client_id)
                    
                    # å¯¹è¯¥å®¢æˆ·ç«¯çš„æ°´å°ä½ç½®ä½¿ç”¨ç‹¬å å¼èšåˆ
                    for param_name, param_idx in positions:
                        if param_name in local_ws[i] and param_name in w_avg:
                            # è·å–å‚æ•°å¼ é‡
                            local_param = local_ws[i][param_name]
                            avg_param = w_avg[param_name]
                            
                            # ç¡®ä¿å‚æ•°å½¢çŠ¶ä¸€è‡´
                            if local_param.shape == avg_param.shape:
                                # å°†å±€éƒ¨ç´¢å¼•è½¬æ¢ä¸ºæ‰å¹³åŒ–ç´¢å¼•
                                param_flat = avg_param.view(-1)
                                local_flat = local_param.view(-1)
                                
                                # ä½¿ç”¨å±€éƒ¨ç´¢å¼•ç›´æ¥æ›¿æ¢
                                if param_idx < param_flat.numel():
                                    param_flat[param_idx] = local_flat[param_idx]
                                    
                except Exception as e:
                    logging.warning(f"Failed to apply watermark aggregation for client {client_id}: {e}")
                    continue
                    
        except Exception as e:
            logging.warning(f"Failed to load key matrix manager for watermark aggregation: {e}")


def main(args):
    logs = {'net_info': None,
            'arguments': {
                'frac': args.frac,
                'local_ep': args.local_ep,
                'local_bs': args.batch_size,
                'lr_inner': args.lr,
                'iid': args.iid,
                'wd': args.wd,
                'optim': args.optim,
                'model_name': args.model_name,
                'dataset': args.dataset,
                'log_interval': args.log_interval,
                'num_classes': args.num_classes,
                'epochs': args.epochs,
                'client_num': args.client_num,
                'console_log': os.path.basename(log_file_name),
            }
            }
    fl = AblationExperiment(args)
    logg, best_metric_value, best_metric_name = fl.training()
    logs['net_info'] = logg
    # å…¼å®¹ï¼šæ€»æ˜¯è®°å½• test_aucï¼›å½“æŒ‡æ ‡ä¸º acc æ—¶ä¹Ÿé¢å¤–è®°å½•
    logs['test_auc'] = {'value': logg.get('best_model_auc', best_metric_value if best_metric_name == 'auc' else 0.0)}
    if best_metric_name == 'acc':
        logs['test_acc'] = {'value': best_metric_value}
    logs['bp_local'] = {'value': False}

    save_dir = os.path.join(args.save_model_dir, args.model_name, args.dataset)
    os.makedirs(save_dir, exist_ok=True)

    now = datetime.now()
    formatted_now = now.strftime("%Y%m%d%H%M")
    # æ„å»ºæ–‡ä»¶å
    enhanced = "_enhanced" if args.watermark_mode == 'enhanced' else ""

    # æ ¹æ®é€‰æ‹©æŒ‡æ ‡å‘½åæ–‡ä»¶
    watermark_suffix = f"wm_{args.watermark_mode}" if hasattr(args, 'watermark_mode') and args.watermark_mode else "wm_basic"
    file_name = '{}_Dp_{}_iid_{}_{}_ep_{}_le_{}_cn_{}_fra_{:.4f}_{}_{{:.4f}}{}.pkl'.format(
        formatted_now, args.sigma, args.iid, watermark_suffix,
        args.epochs, args.local_ep, args.client_num, args.frac, best_metric_name, enhanced
    )
    file_name = file_name.format(best_metric_value)
    torch.save(logs, os.path.join(save_dir, file_name))
    logging.info(f"è®­ç»ƒæ—¥å¿—å·²ä¿å­˜: {file_name}")
    logging.info('-------------------------------Finish--------------------------------------')

    return


if __name__ == '__main__':
    os.environ["CUDA_DEVICE_ORDER"] = "PCI_BUS_ID"
    os.environ["CUDA_VISIBLE_DEVICES"] = args.gpu
    main(args)

