import copy
import os
import sys
import time
from datetime import datetime
import logging
import gc

import numpy as np
import torch
from torch.utils.data import DataLoader
from tqdm import tqdm

from config.globals import set_seed
from models.alexnet import AlexNet
from models.resnet import resnet18
from utils.args import parser_args
from utils.base import Experiment
from utils.dataset import get_data, DatasetSplit
from utils.trainer_private import TrainerPrivate, TesterPrivate
import pandas as pd

set_seed()

# é…ç½® logging
args = parser_args()
log_file_name = args.log_file.replace('.log', '_baseline.log')  # åŸºå‡†æ¨¡å¼ä½¿ç”¨ä¸åŒçš„æ—¥å¿—æ–‡ä»¶
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    datefmt='%Y-%m-%d %H-%M-%S',  # æ—¥æœŸæ ¼å¼
    handlers=[
        logging.StreamHandler(sys.stdout),  # è¾“å‡ºåˆ°æ§åˆ¶å°
        logging.FileHandler(log_file_name, mode='a', encoding='utf-8')  # è¿½åŠ æ¨¡å¼
    ]
)


class BaselineFederatedLearning(Experiment):
    """åŸºå‡†è”é‚¦å­¦ä¹ è®­ç»ƒç±» - ä¸åŒ…å«ä»»ä½•æ°´å°åŠŸèƒ½"""
    
    def __init__(self, args):
        super().__init__(args)
        
        self.args = args
        self.dp = args.dp
        self.sigma = args.sigma
        
        # å¼ºåˆ¶è®¾ç½®ä¸ºåŸºå‡†æ¨¡å¼
        self.args.enable_watermark = False
        self.args.watermark_mode = 'baseline'
        
        logging.info("=" * 60)
        logging.info("ğŸš€ å¯åŠ¨åŸºå‡†è”é‚¦å­¦ä¹ è®­ç»ƒ")
        logging.info(f"ğŸ“Š æ•°æ®é›†: {args.dataset}")
        logging.info(f"ğŸ—ï¸ æ¨¡å‹: {args.model_name}")
        logging.info(f"ğŸ‘¥ å®¢æˆ·ç«¯æ•°é‡: {args.client_num}")
        logging.info(f"ğŸ“ˆ è®­ç»ƒè½®æ¬¡: {args.epochs}")
        logging.info(f"ğŸ”’ å·®åˆ†éšç§: {'å¯ç”¨' if args.dp else 'ç¦ç”¨'}")
        if args.dp:
            logging.info(f"ğŸ“Š å™ªå£°å‚æ•°: {args.sigma}")
        logging.info("=" * 60)

    def get_model(self):
        """è·å–æ¨¡å‹"""
        if self.args.model_name == 'alexnet':
            dropout_rate = getattr(self.args, 'dropout_rate', 0.5)
            model = AlexNet(self.args.in_channels, self.args.num_classes, 
                           input_size=self.args.input_size, dropout_rate=dropout_rate)
        elif self.args.model_name in ['resnet', 'resnet18']:
            model = resnet18(num_classes=self.args.num_classes, 
                           in_channels=self.args.in_channels, 
                           input_size=self.args.input_size)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹: {self.args.model_name}")
        
        logging.info(f"âœ… æ¨¡å‹ {self.args.model_name} åˆå§‹åŒ–å®Œæˆ")
        return model

    def get_trainer(self, model, device):
        """è·å–è®­ç»ƒå™¨ - ä½¿ç”¨åŸºç¡€è®­ç»ƒå™¨ï¼Œä¸åŒ…å«æ°´å°åŠŸèƒ½"""
        trainer = TrainerPrivate(model, device, self.dp, self.sigma, None, args=self.args)
        logging.info("âœ… åŸºå‡†è®­ç»ƒå™¨åˆå§‹åŒ–å®Œæˆ")
        return trainer

    def train(self):
        """æ‰§è¡Œè”é‚¦å­¦ä¹ è®­ç»ƒ"""
        logging.info("ğŸ¯ å¼€å§‹åŸºå‡†è”é‚¦å­¦ä¹ è®­ç»ƒ...")
        
        # è·å–æ•°æ®
        train_data, test_data, user_groups = get_data(self.args.dataset, self.data_root, self.iid, self.client_num)
        logging.info(f"ğŸ“Š æ•°æ®åŠ è½½å®Œæˆ: è®­ç»ƒé›† {len(train_data)} æ ·æœ¬, æµ‹è¯•é›† {len(test_data)} æ ·æœ¬")
        
        # è·å–å…¨å±€æ¨¡å‹
        global_model = self.get_model()
        
        # åˆå§‹åŒ–è®­ç»ƒå™¨
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        trainer = self.get_trainer(global_model, device)
        
        # è®­ç»ƒå†å²è®°å½•
        train_losses = []
        train_accs = []
        val_losses = []
        val_accs = []
        val_aucs = []
        
        # è”é‚¦å­¦ä¹ ä¸»å¾ªç¯
        for epoch in range(self.args.epochs):
            logging.info(f"\nğŸ”„ è½®æ¬¡ {epoch + 1}/{self.args.epochs}")
            
            # é€‰æ‹©å‚ä¸è®­ç»ƒçš„å®¢æˆ·ç«¯
            m = max(int(self.args.frac * self.args.client_num), 1)
            idxs_users = np.random.choice(range(self.args.client_num), m, replace=False)
            
            # å…¨å±€æ¨¡å‹å‚æ•°
            global_weights = global_model.state_dict()
            
            # å®¢æˆ·ç«¯æœ¬åœ°è®­ç»ƒ
            local_weights = []
            local_losses = []
            
            for idx in idxs_users:
                # åˆ›å»ºå®¢æˆ·ç«¯æ•°æ®
                local_data = DatasetSplit(train_data, user_groups[idx])
                local_loader = DataLoader(local_data, batch_size=self.args.batch_size, shuffle=True)
                
                # æœ¬åœ°è®­ç»ƒï¼ˆä¼ é€’å…¨å±€è½®æ¬¡ä¿¡æ¯ç”¨äºå­¦ä¹ ç‡è°ƒåº¦ï¼‰
                w, loss = trainer.local_update(
                    local_loader, 
                    self.args.local_ep, 
                    self.args.lr, 
                    idx,
                    current_epoch=epoch,
                    total_epochs=self.args.epochs
                )
                local_weights.append(copy.deepcopy(w))
                local_losses.append(loss)
            
            # è”é‚¦å¹³å‡
            global_weights = self.federated_averaging(global_weights, local_weights)
            global_model.load_state_dict(global_weights)
            
            # è®¡ç®—å¹³å‡è®­ç»ƒæŸå¤±
            avg_train_loss = np.mean(local_losses)
            train_losses.append(avg_train_loss)
            
            # æµ‹è¯•å…¨å±€æ¨¡å‹
            test_loader = DataLoader(test_data, batch_size=self.args.batch_size, shuffle=False)
            test_loss, test_acc, test_auc, test_sample_acc = trainer.test(test_loader)
            
            val_losses.append(test_loss)
            val_accs.append(test_acc)  # æ ‡ç­¾çº§å‡†ç¡®ç‡
            val_aucs.append(test_auc)
            
            # è®°å½•è®­ç»ƒè¿›åº¦
            logging.info(f"ğŸ“Š è®­ç»ƒæŸå¤±: {avg_train_loss:.4f}")
            logging.info(f"ğŸ“Š æµ‹è¯•æŸå¤±: {test_loss:.4f}")
            logging.info(f"ğŸ“Š æµ‹è¯•å‡†ç¡®ç‡: {test_acc:.2f}%")
            logging.info(f"ğŸ“Š æµ‹è¯•AUC: {test_auc:.4f}")
            
            # æ¸…ç†å†…å­˜
            del local_weights, local_losses
            gc.collect()
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
        
        # ä¿å­˜è®­ç»ƒç»“æœ
        self.save_results(train_losses, train_accs, val_losses, val_accs, val_aucs)
        
        logging.info("ğŸ‰ åŸºå‡†è”é‚¦å­¦ä¹ è®­ç»ƒå®Œæˆ!")
        return global_model

    def federated_averaging(self, global_weights, local_weights):
        """è”é‚¦å¹³å‡ç®—æ³•"""
        # è®¡ç®—å¹³å‡æƒé‡
        avg_weights = copy.deepcopy(global_weights)
        
        for key in avg_weights.keys():
            avg_weights[key] = torch.zeros_like(avg_weights[key])
            for local_weight in local_weights:
                avg_weights[key] += local_weight[key]
            avg_weights[key] = avg_weights[key] / len(local_weights)
        
        return avg_weights

    def save_results(self, train_losses, train_accs, val_losses, val_accs, val_aucs):
        """ä¿å­˜è®­ç»ƒç»“æœ"""
        # åˆ›å»ºç»“æœç›®å½•
        save_dir = os.path.join(self.args.save_model_dir, self.args.model_name, self.args.dataset)
        os.makedirs(save_dir, exist_ok=True)
        
        # æ„å»ºç»“æœæ•°æ®
        results = {
            'epoch': list(range(1, len(train_losses) + 1)),
            'train_loss': train_losses,
            'train_acc': train_accs,
            'val_loss': val_losses,
            'val_acc': val_accs,
            'val_auc': val_aucs
        }
        
        # ä¿å­˜ä¸ºCSV
        df = pd.DataFrame(results)
        timestamp = datetime.now().strftime("%Y%m%d%H%M")
        csv_filename = f'{timestamp}_baseline_{self.args.dataset}_{self.args.model_name}_results.csv'
        csv_path = os.path.join(save_dir, csv_filename)
        df.to_csv(csv_path, index=False)
        
        logging.info(f"ğŸ“ è®­ç»ƒç»“æœå·²ä¿å­˜: {csv_path}")
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        best_epoch = np.argmax(val_aucs)
        best_auc = val_aucs[best_epoch]
        
        model_filename = f'{timestamp}_baseline_{self.args.dataset}_{self.args.model_name}_best_auc_{best_auc:.4f}.pth'
        model_path = os.path.join(save_dir, model_filename)
        
        # è¿™é‡Œå¯ä»¥ä¿å­˜æ¨¡å‹æƒé‡
        logging.info(f"ğŸ† æœ€ä½³æ¨¡å‹ (AUC: {best_auc:.4f}, è½®æ¬¡: {best_epoch + 1})")
        logging.info(f"ğŸ“ æ¨¡å‹è·¯å¾„: {model_path}")


def main():
    """ä¸»å‡½æ•°"""
    args = parser_args()
    
    # å¼ºåˆ¶è®¾ç½®ä¸ºåŸºå‡†æ¨¡å¼
    args.enable_watermark = False
    args.watermark_mode = 'baseline'
    
    logging.info("ğŸš€ å¯åŠ¨åŸºå‡†è”é‚¦å­¦ä¹ è®­ç»ƒè„šæœ¬")
    logging.info(f"ğŸ“‹ é…ç½®å‚æ•°:")
    logging.info(f"  - æ•°æ®é›†: {args.dataset}")
    logging.info(f"  - æ¨¡å‹: {args.model_name}")
    logging.info(f"  - å®¢æˆ·ç«¯æ•°: {args.client_num}")
    logging.info(f"  - è®­ç»ƒè½®æ¬¡: {args.epochs}")
    logging.info(f"  - å­¦ä¹ ç‡: {args.lr}")
    logging.info(f"  - æœ¬åœ°è½®æ¬¡: {args.local_ep}")
    logging.info(f"  - æ‰¹æ¬¡å¤§å°: {args.batch_size}")
    logging.info(f"  - å·®åˆ†éšç§: {'å¯ç”¨' if args.dp else 'ç¦ç”¨'}")
    if args.dp:
        logging.info(f"  - å™ªå£°å‚æ•°: {args.sigma}")
    
    # åˆ›å»ºå®éªŒå®ä¾‹
    experiment = BaselineFederatedLearning(args)
    
    # å¼€å§‹è®­ç»ƒ
    start_time = time.time()
    try:
        model = experiment.train()
        end_time = time.time()
        
        logging.info("=" * 60)
        logging.info("ğŸ‰ åŸºå‡†è”é‚¦å­¦ä¹ è®­ç»ƒæˆåŠŸå®Œæˆ!")
        logging.info(f"â±ï¸ æ€»è®­ç»ƒæ—¶é—´: {end_time - start_time:.2f} ç§’")
        logging.info("=" * 60)
        
    except Exception as e:
        logging.error(f"âŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        raise


if __name__ == '__main__':
    main()

