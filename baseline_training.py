import copy
import os
import sys
import time
from datetime import datetime
import logging
import gc
from types import SimpleNamespace

import numpy as np
import torch
from torch.utils.data import DataLoader
from tqdm import tqdm

from config.globals import set_seed
from models.alexnet import AlexNet
from models.resnet import resnet18
from utils.base import Experiment
from utils.dataset import get_data, DatasetSplit
from utils.trainer_private import TrainerPrivate, TesterPrivate
import pandas as pd

set_seed()


# ========================= åŸºå‡†æµ‹è¯•é…ç½®å‚æ•° ========================
def get_baseline_config():
    """
    åŸºå‡†æµ‹è¯•é…ç½® - æ‰€æœ‰å‚æ•°éƒ½åœ¨è¿™é‡Œè®¾ç½®
    ä¸ä¾èµ– args.py å’Œå‘½ä»¤è¡Œå‚æ•°
    """
    config = SimpleNamespace()
    
    # ==================== å®éªŒåŸºç¡€é…ç½® ====================
    config.gpu = '0'  # GPUè®¾å¤‡ID
    config.dataset = 'chestmnist'  # æ•°æ®é›†: 'chestmnist', 'cifar10', 'cifar100'
    config.model_name = 'alexnet'  # æ¨¡å‹: 'alexnet', 'resnet'
    
    # ==================== è”é‚¦å­¦ä¹ å‚æ•° ====================
    config.epochs = 150  # å…¨å±€è®­ç»ƒè½®æ¬¡
    config.local_ep = 2  # æ¯ä¸ªå®¢æˆ·ç«¯çš„æœ¬åœ°è®­ç»ƒè½®æ¬¡
    config.batch_size = 128  # æ‰¹æ¬¡å¤§å°
    config.client_num = 5  # å®¢æˆ·ç«¯æ•°é‡
    config.frac = 1.0  # å‚ä¸è®­ç»ƒçš„å®¢æˆ·ç«¯æ¯”ä¾‹ (1.0 = 100%)
    config.iid = True  # IIDæ•°æ®åˆ†å¸ƒ
    
    # ==================== ä¼˜åŒ–å™¨å‚æ•° ====================
    config.optim = 'adam'  # ä¼˜åŒ–å™¨: 'sgd', 'adam'
    config.lr = 0.001  # å­¦ä¹ ç‡
    config.wd = 0.0001  # æƒé‡è¡°å‡ (L2æ­£åˆ™åŒ–)
    config.use_lr_scheduler = True  # ä½¿ç”¨ä½™å¼¦é€€ç«å­¦ä¹ ç‡è°ƒåº¦å™¨
    config.dropout_rate = 0.5  # Dropoutç‡
    
    # ==================== å·®åˆ†éšç§å‚æ•° ====================
    config.dp = False  # å¯ç”¨å·®åˆ†éšç§
    config.sigma = 0.1  # é«˜æ–¯å™ªå£°æ ‡å‡†å·®
    
    # ==================== æ•°æ®é›†ç‰¹å®šå‚æ•° ====================
    # è¿™äº›å‚æ•°ä¼šæ ¹æ®æ•°æ®é›†è‡ªåŠ¨è®¾ç½®
    if config.dataset == 'chestmnist':
        config.num_classes = 14
        config.in_channels = 1  # ç°åº¦å›¾åƒ
        config.input_size = 28  # ChestMNISTå›¾åƒå°ºå¯¸
        config.task_type = 'multilabel'  # å¤šæ ‡ç­¾åˆ†ç±»
    elif config.dataset == 'cifar10':
        config.num_classes = 10
        config.in_channels = 3  # RGBå›¾åƒ
        config.input_size = 32  # CIFAR-10å›¾åƒå°ºå¯¸
        config.task_type = 'multiclass'  # å¤šç±»åˆ«åˆ†ç±»
    elif config.dataset == 'cifar100':
        config.num_classes = 100
        config.in_channels = 3  # RGBå›¾åƒ
        config.input_size = 32  # CIFAR-100å›¾åƒå°ºå¯¸
        config.task_type = 'multiclass'  # å¤šç±»åˆ«åˆ†ç±»
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„æ•°æ®é›†: {config.dataset}")
    
    # ==================== æŸå¤±å‡½æ•°å‚æ•° ====================
    config.class_weights = False  # ä¸ä½¿ç”¨ç±»åˆ«æƒé‡
    config.pos_weight_factor = 1.0  # æ­£æ ·æœ¬æƒé‡å› å­ï¼ˆä»…åœ¨class_weights=Trueæ—¶ä½¿ç”¨ï¼‰
    config.use_multiloss = False  # åŸºå‡†æ¨¡å¼ä¸ä½¿ç”¨å¤šé‡æŸå¤±
    config.use_focal_loss = False  # åŸºå‡†æ¨¡å¼ä¸ä½¿ç”¨Focal Loss
    
    # ==================== æ°´å°ç›¸å…³å‚æ•°ï¼ˆåŸºå‡†æ¨¡å¼ç¦ç”¨ï¼‰====================
    config.enable_watermark = False  # åŸºå‡†æ¨¡å¼ç¦ç”¨æ°´å°
    config.watermark_mode = 'baseline'
    config.use_key_matrix = False
    
    # ==================== ä¿å­˜è·¯å¾„å‚æ•° ====================
    config.save_model_dir = 'save'  # æ¨¡å‹ä¿å­˜ç›®å½•
    config.save_excel_dir = 'save/excel'  # Excelä¿å­˜ç›®å½•
    config.log_file = './logs/console_baseline.logs'  # æ—¥å¿—æ–‡ä»¶è·¯å¾„
    config.data_root = './data'  # æ•°æ®é›†æ ¹ç›®å½•
    
    # ==================== å…¶ä»–å‚æ•° ====================
    config.log_interval = 1  # è¯„ä¼°é—´éš”
    config.baseline_mode = True  # åŸºå‡†æ¨¡å¼æ ‡å¿—
    
    return config


# ========================= é…ç½®æ—¥å¿—ç³»ç»Ÿ ========================
def setup_logging(log_file):
    """é…ç½®æ—¥å¿—ç³»ç»Ÿ"""
    os.makedirs(os.path.dirname(log_file), exist_ok=True)
    
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        datefmt='%Y-%m-%d %H-%M-%S',
        handlers=[
            logging.StreamHandler(sys.stdout),  # è¾“å‡ºåˆ°æ§åˆ¶å°
            logging.FileHandler(log_file, mode='a', encoding='utf-8')  # è¿½åŠ æ¨¡å¼
        ]
    )


class BaselineFederatedLearning(Experiment):
    """åŸºå‡†è”é‚¦å­¦ä¹ è®­ç»ƒç±» - ä¸åŒ…å«ä»»ä½•æ°´å°åŠŸèƒ½"""
    
    def __init__(self, args):
        super().__init__(args)
        
        self.args = args
        self.dp = args.dp
        self.sigma = args.sigma
        
        # å¼ºåˆ¶è®¾ç½®ä¸ºåŸºå‡†æ¨¡å¼
        self.args.enable_watermark = False
        self.args.watermark_mode = 'baseline'
        
        logging.info("=" * 60)
        logging.info("ğŸš€ å¯åŠ¨åŸºå‡†è”é‚¦å­¦ä¹ è®­ç»ƒ")
        logging.info(f"ğŸ“Š æ•°æ®é›†: {args.dataset}")
        logging.info(f"ğŸ—ï¸ æ¨¡å‹: {args.model_name}")
        logging.info(f"ğŸ‘¥ å®¢æˆ·ç«¯æ•°é‡: {args.client_num}")
        logging.info(f"ğŸ“ˆ è®­ç»ƒè½®æ¬¡: {args.epochs}")
        logging.info(f"ğŸ”’ å·®åˆ†éšç§: {'å¯ç”¨' if args.dp else 'ç¦ç”¨'}")
        if args.dp:
            logging.info(f"ğŸ“Š å™ªå£°å‚æ•°: {args.sigma}")
        logging.info("=" * 60)

    def get_model(self):
        """è·å–æ¨¡å‹"""
        if self.args.model_name == 'alexnet':
            dropout_rate = getattr(self.args, 'dropout_rate', 0.5)
            model = AlexNet(self.args.in_channels, self.args.num_classes, 
                           input_size=self.args.input_size, dropout_rate=dropout_rate)
        elif self.args.model_name in ['resnet', 'resnet18']:
            model = resnet18(num_classes=self.args.num_classes, 
                           in_channels=self.args.in_channels, 
                           input_size=self.args.input_size)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹: {self.args.model_name}")
        
        logging.info(f"âœ… æ¨¡å‹ {self.args.model_name} åˆå§‹åŒ–å®Œæˆ")
        return model

    def get_trainer(self, model, device):
        """è·å–è®­ç»ƒå™¨ - ä½¿ç”¨åŸºç¡€è®­ç»ƒå™¨ï¼Œä¸åŒ…å«æ°´å°åŠŸèƒ½"""
        trainer = TrainerPrivate(model, device, self.dp, self.sigma, None, args=self.args)
        logging.info("âœ… åŸºå‡†è®­ç»ƒå™¨åˆå§‹åŒ–å®Œæˆ")
        return trainer

    def train(self):
        """æ‰§è¡Œè”é‚¦å­¦ä¹ è®­ç»ƒ"""
        logging.info("ğŸ¯ å¼€å§‹åŸºå‡†è”é‚¦å­¦ä¹ è®­ç»ƒ...")
        
        # è·å–æ•°æ®
        train_data, test_data, user_groups = get_data(self.args.dataset, self.data_root, self.iid, self.client_num)
        logging.info(f"ğŸ“Š æ•°æ®åŠ è½½å®Œæˆ: è®­ç»ƒé›† {len(train_data)} æ ·æœ¬, æµ‹è¯•é›† {len(test_data)} æ ·æœ¬")
        
        # è·å–å…¨å±€æ¨¡å‹
        global_model = self.get_model()
        
        # åˆå§‹åŒ–è®­ç»ƒå™¨
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        trainer = self.get_trainer(global_model, device)
        
        # è®­ç»ƒå†å²è®°å½•
        train_losses = []
        train_accs = []
        train_sample_accs = []
        val_losses = []
        val_accs = []
        val_sample_accs = []
        val_aucs = []
        
        # ç”¨äºä¿å­˜æœ€ä½³æ¨¡å‹
        best_model_state = None
        best_auc = 0.0
        best_epoch = 0
        
        # è”é‚¦å­¦ä¹ ä¸»å¾ªç¯
        for epoch in range(self.args.epochs):
            logging.info(f"\nğŸ”„ è½®æ¬¡ {epoch + 1}/{self.args.epochs}")
            
            # é€‰æ‹©å‚ä¸è®­ç»ƒçš„å®¢æˆ·ç«¯
            m = max(int(self.args.frac * self.args.client_num), 1)
            idxs_users = np.random.choice(range(self.args.client_num), m, replace=False)
            
            # å…¨å±€æ¨¡å‹å‚æ•°
            global_weights = global_model.state_dict()
            
            # å®¢æˆ·ç«¯æœ¬åœ°è®­ç»ƒ
            local_weights = []
            local_losses = []
            
            for idx in idxs_users:
                # åˆ›å»ºå®¢æˆ·ç«¯æ•°æ®
                local_data = DatasetSplit(train_data, user_groups[idx])
                local_loader = DataLoader(local_data, batch_size=self.args.batch_size, shuffle=True)
                
                # æœ¬åœ°è®­ç»ƒï¼ˆä¼ é€’å…¨å±€è½®æ¬¡ä¿¡æ¯ç”¨äºå­¦ä¹ ç‡è°ƒåº¦ï¼‰
                w, loss, acc = trainer.local_update(
                    local_loader, 
                    self.args.local_ep, 
                    self.args.lr, 
                    idx,
                    current_epoch=epoch,
                    total_epochs=self.args.epochs
                )
                local_weights.append(copy.deepcopy(w))
                local_losses.append(loss)
            
            # è”é‚¦å¹³å‡
            global_weights = self.federated_averaging(global_weights, local_weights)
            global_model.load_state_dict(global_weights)
            
            # è®¡ç®—å¹³å‡è®­ç»ƒæŸå¤±
            avg_train_loss = np.mean(local_losses)
            train_losses.append(avg_train_loss)
            
            # åœ¨è®­ç»ƒé›†ä¸Šè¯„ä¼°ï¼ˆè®¡ç®—è®­ç»ƒå‡†ç¡®ç‡ï¼‰
            # ä½¿ç”¨éšæœºæŠ½æ ·çš„è®­ç»ƒæ•°æ®å­é›†ä»¥æé«˜æ•ˆç‡
            train_eval_indices = np.random.choice(len(train_data), min(len(test_data), len(train_data)), replace=False)
            train_eval_data = torch.utils.data.Subset(train_data, train_eval_indices)
            train_eval_loader = DataLoader(train_eval_data, batch_size=self.args.batch_size, shuffle=False)
            train_loss, train_acc, train_auc, train_sample_acc = trainer.test(train_eval_loader)
            train_accs.append(train_acc)
            train_sample_accs.append(train_sample_acc)
            
            # æµ‹è¯•å…¨å±€æ¨¡å‹
            test_loader = DataLoader(test_data, batch_size=self.args.batch_size, shuffle=False)
            test_loss, test_acc, test_auc, test_sample_acc = trainer.test(test_loader)
            
            val_losses.append(test_loss)
            val_accs.append(test_acc)  # æ ‡ç­¾çº§å‡†ç¡®ç‡
            val_sample_accs.append(test_sample_acc)  # æ ·æœ¬çº§å‡†ç¡®ç‡
            val_aucs.append(test_auc)
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if test_auc > best_auc:
                best_auc = test_auc
                best_epoch = epoch
                best_model_state = copy.deepcopy(global_model.state_dict())
                logging.info(f"â­ æ–°çš„æœ€ä½³æ¨¡å‹! AUC: {best_auc:.4f}")
            
            # è®°å½•è®­ç»ƒè¿›åº¦
            logging.info(f"ğŸ“Š è®­ç»ƒæŸå¤±: {avg_train_loss:.4f}, è®­ç»ƒå‡†ç¡®ç‡: {train_acc:.2f}% (æ ·æœ¬çº§: {train_sample_acc:.2f}%)")
            logging.info(f"ğŸ“Š æµ‹è¯•æŸå¤±: {test_loss:.4f}, æµ‹è¯•å‡†ç¡®ç‡: {test_acc:.2f}% (æ ·æœ¬çº§: {test_sample_acc:.2f}%)")
            logging.info(f"ğŸ“Š æµ‹è¯•AUC: {test_auc:.4f}")
            logging.info(f"ğŸ† å½“å‰æœ€ä½³AUC: {best_auc:.4f} (è½®æ¬¡ {best_epoch + 1})")
            
            # æ¸…ç†å†…å­˜
            del local_weights, local_losses
            gc.collect()
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
        
        # ä¿å­˜è®­ç»ƒç»“æœ
        self.save_results(train_losses, train_accs, train_sample_accs, 
                         val_losses, val_accs, val_sample_accs, val_aucs,
                         best_model_state, best_epoch, best_auc)
        
        logging.info("ğŸ‰ åŸºå‡†è”é‚¦å­¦ä¹ è®­ç»ƒå®Œæˆ!")
        return global_model

    def federated_averaging(self, global_weights, local_weights):
        """è”é‚¦å¹³å‡ç®—æ³•"""
        # è®¡ç®—å¹³å‡æƒé‡
        avg_weights = copy.deepcopy(global_weights)
        
        # ç¡®å®šç›®æ ‡è®¾å¤‡ï¼ˆä½¿ç”¨å…¨å±€æƒé‡çš„è®¾å¤‡ï¼‰
        device = next(iter(global_weights.values())).device
        
        for key in avg_weights.keys():
            avg_weights[key] = torch.zeros_like(avg_weights[key])
            for local_weight in local_weights:
                # å°†æœ¬åœ°æƒé‡ç§»åˆ°æ­£ç¡®çš„è®¾å¤‡
                avg_weights[key] += local_weight[key].to(device)
            avg_weights[key] = avg_weights[key] / len(local_weights)
        
        return avg_weights

    def save_results(self, train_losses, train_accs, train_sample_accs, 
                    val_losses, val_accs, val_sample_accs, val_aucs,
                    best_model_state, best_epoch, best_auc):
        """ä¿å­˜è®­ç»ƒç»“æœå’Œæœ€ä½³æ¨¡å‹"""
        # åˆ›å»ºç»“æœç›®å½•
        save_dir = os.path.join(self.args.save_model_dir, self.args.model_name, self.args.dataset)
        os.makedirs(save_dir, exist_ok=True)
        
        # æ„å»ºç»“æœæ•°æ®
        results = {
            'epoch': list(range(1, len(train_losses) + 1)),
            'train_loss': train_losses,
            'train_acc_label': train_accs,
            'train_acc_sample': train_sample_accs,
            'val_loss': val_losses,
            'val_acc_label': val_accs,
            'val_acc_sample': val_sample_accs,
            'val_auc': val_aucs
        }
        
        # ä¿å­˜ä¸ºCSV
        df = pd.DataFrame(results)
        timestamp = datetime.now().strftime("%Y%m%d%H%M")
        csv_filename = f'{timestamp}_baseline_{self.args.dataset}_{self.args.model_name}_results.csv'
        csv_path = os.path.join(save_dir, csv_filename)
        df.to_csv(csv_path, index=False)
        
        logging.info(f"ğŸ“ è®­ç»ƒç»“æœå·²ä¿å­˜: {csv_path}")
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹æƒé‡
        if best_model_state is not None:
            model_filename = f'{timestamp}_baseline_{self.args.dataset}_{self.args.model_name}_best_auc_{best_auc:.4f}.pth'
            model_path = os.path.join(save_dir, model_filename)
            torch.save(best_model_state, model_path)
            
            logging.info(f"ğŸ† æœ€ä½³æ¨¡å‹å·²ä¿å­˜ (AUC: {best_auc:.4f}, è½®æ¬¡: {best_epoch + 1})")
            logging.info(f"ğŸ“ æ¨¡å‹è·¯å¾„: {model_path}")
        else:
            logging.warning("âš ï¸ æ²¡æœ‰æ‰¾åˆ°æœ€ä½³æ¨¡å‹çŠ¶æ€ï¼Œè·³è¿‡æ¨¡å‹ä¿å­˜")


def main():
    """ä¸»å‡½æ•°"""
    # è·å–åŸºå‡†æµ‹è¯•é…ç½®ï¼ˆæ‰€æœ‰å‚æ•°éƒ½åœ¨é…ç½®å‡½æ•°ä¸­å®šä¹‰ï¼‰
    args = get_baseline_config()
    
    # é…ç½®æ—¥å¿—ç³»ç»Ÿ
    setup_logging(args.log_file)
    
    # è®¾ç½®GPU
    os.environ['CUDA_VISIBLE_DEVICES'] = args.gpu
    
    logging.info("=" * 80)
    logging.info("ğŸš€ å¯åŠ¨åŸºå‡†è”é‚¦å­¦ä¹ è®­ç»ƒè„šæœ¬ (Baseline Mode)")
    logging.info("=" * 80)
    logging.info("")
    logging.info("ğŸ“‹ å®éªŒé…ç½®å‚æ•°:")
    logging.info("-" * 80)
    logging.info(f"  ğŸ”¹ æ•°æ®é›†é…ç½®:")
    logging.info(f"     - æ•°æ®é›†: {args.dataset}")
    logging.info(f"     - ç±»åˆ«æ•°: {args.num_classes}")
    logging.info(f"     - è¾“å…¥é€šé“: {args.in_channels}")
    logging.info(f"     - æ•°æ®æ ¹ç›®å½•: {args.data_root}")
    logging.info("")
    logging.info(f"  ğŸ”¹ æ¨¡å‹é…ç½®:")
    logging.info(f"     - æ¨¡å‹æ¶æ„: {args.model_name}")
    logging.info(f"     - Dropoutç‡: {args.dropout_rate}")
    logging.info("")
    logging.info(f"  ğŸ”¹ è”é‚¦å­¦ä¹ å‚æ•°:")
    logging.info(f"     - å…¨å±€è®­ç»ƒè½®æ¬¡: {args.epochs}")
    logging.info(f"     - æœ¬åœ°è®­ç»ƒè½®æ¬¡: {args.local_ep}")
    logging.info(f"     - å®¢æˆ·ç«¯æ•°é‡: {args.client_num}")
    logging.info(f"     - å‚ä¸æ¯”ä¾‹: {args.frac * 100:.0f}%")
    logging.info(f"     - æ•°æ®åˆ†å¸ƒ: {'IID' if args.iid else 'Non-IID'}")
    logging.info("")
    logging.info(f"  ğŸ”¹ ä¼˜åŒ–å™¨å‚æ•°:")
    logging.info(f"     - ä¼˜åŒ–å™¨: {args.optim.upper()}")
    logging.info(f"     - å­¦ä¹ ç‡: {args.lr}")
    logging.info(f"     - æƒé‡è¡°å‡: {args.wd}")
    logging.info(f"     - æ‰¹æ¬¡å¤§å°: {args.batch_size}")
    logging.info(f"     - å­¦ä¹ ç‡è°ƒåº¦å™¨: {'å¯ç”¨' if args.use_lr_scheduler else 'ç¦ç”¨'}")
    logging.info("")
    logging.info(f"  ğŸ”¹ éšç§ä¿æŠ¤:")
    logging.info(f"     - å·®åˆ†éšç§: {'âœ… å¯ç”¨' if args.dp else 'âŒ ç¦ç”¨'}")
    if args.dp:
        logging.info(f"     - å™ªå£°å‚æ•° Ïƒ: {args.sigma}")
    logging.info("")
    logging.info(f"  ğŸ”¹ æ°´å°çŠ¶æ€:")
    logging.info(f"     - æ°´å°åµŒå…¥: {'å¯ç”¨' if args.enable_watermark else 'âŒ ç¦ç”¨ (åŸºå‡†æ¨¡å¼)'}")
    logging.info("")
    logging.info(f"  ğŸ”¹ ä¿å­˜è·¯å¾„:")
    logging.info(f"     - æ¨¡å‹ä¿å­˜: {args.save_model_dir}")
    logging.info(f"     - æ—¥å¿—æ–‡ä»¶: {args.log_file}")
    logging.info("-" * 80)
    logging.info("")
    
    # åˆ›å»ºå®éªŒå®ä¾‹
    experiment = BaselineFederatedLearning(args)
    
    # å¼€å§‹è®­ç»ƒ
    start_time = time.time()
    try:
        logging.info("ğŸ å¼€å§‹è®­ç»ƒ...")
        logging.info("")
        model = experiment.train()
        end_time = time.time()
        
        # è®¡ç®—è®­ç»ƒæ—¶é—´
        total_seconds = end_time - start_time
        hours = int(total_seconds // 3600)
        minutes = int((total_seconds % 3600) // 60)
        seconds = int(total_seconds % 60)
        
        logging.info("")
        logging.info("=" * 80)
        logging.info("ğŸ‰ åŸºå‡†è”é‚¦å­¦ä¹ è®­ç»ƒæˆåŠŸå®Œæˆ!")
        logging.info(f"â±ï¸  æ€»è®­ç»ƒæ—¶é—´: {hours}å°æ—¶ {minutes}åˆ†é’Ÿ {seconds}ç§’ ({total_seconds:.2f}ç§’)")
        logging.info("=" * 80)
        
    except KeyboardInterrupt:
        logging.warning("\nâš ï¸  è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­")
        raise
    except Exception as e:
        logging.error(f"\nâŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        logging.error(traceback.format_exc())
        raise


if __name__ == '__main__':
    main()

