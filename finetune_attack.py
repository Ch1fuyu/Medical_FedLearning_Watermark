#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¾®è°ƒæ”»å‡»å®éªŒä»£ç 
é’ˆå¯¹æ¨¡å‹å¾®è°ƒæ”»å‡»çš„å®éªŒï¼Œç»§ç»­å¯¹ä¸»ä»»åŠ¡è¿›è¡Œè®­ç»ƒï¼Œè§‚å¯Ÿæ°´å°å®Œæ•´æ€§ã€Î”PCCå€¼å˜åŒ–ä»¥åŠåˆ¤æ–­æ˜¯å¦ä¾µæƒ
"""

import copy
import os
import sys
from datetime import datetime

import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from torchvision import datasets, transforms
from tqdm import tqdm

# Windowså¤šè¿›ç¨‹å…¼å®¹æ€§å¤„ç†
if sys.platform.startswith('win'):
    import multiprocessing
    multiprocessing.set_start_method('spawn', force=True)

from models.resnet import resnet18
from utils.dataset import LocalChestMNISTDataset
from utils.watermark_reconstruction import WatermarkReconstructor
from utils.delta_pcc_utils import evaluate_delta_pcc, calculate_fixed_tau, format_delta_pcc_result, print_delta_pcc_summary


def create_safe_dataloader(dataset, batch_size, shuffle=False, num_workers=None):
    """
    åˆ›å»ºå®‰å…¨çš„æ•°æ®åŠ è½½å™¨ï¼Œè‡ªåŠ¨å¤„ç†Windowså¤šè¿›ç¨‹é—®é¢˜
    
    Args:
        dataset: æ•°æ®é›†
        batch_size: æ‰¹æ¬¡å¤§å°
        shuffle: æ˜¯å¦æ‰“ä¹±æ•°æ®
        num_workers: å·¥ä½œè¿›ç¨‹æ•°ï¼ŒNoneæ—¶è‡ªåŠ¨é€‰æ‹©
        
    Returns:
        æ•°æ®åŠ è½½å™¨
    """
    import sys
    
    # ä¼˜åŒ–å¤šè¿›ç¨‹è®¾ç½®ï¼Œæé«˜æ•°æ®åŠ è½½æ•ˆç‡
    if num_workers is None:
        if sys.platform.startswith('win'):
            num_workers = 2  # Windowsä¸Šä½¿ç”¨2ä¸ªè¿›ç¨‹
        else:
            num_workers = 4  # Linux/Macä¸Šä½¿ç”¨4ä¸ªè¿›ç¨‹
    
    try:
        return DataLoader(
            dataset, 
            batch_size=batch_size, 
            shuffle=shuffle, 
            num_workers=num_workers,
            pin_memory=True,  # å¯ç”¨å†…å­˜å›ºå®šï¼Œæé«˜GPUä¼ è¾“æ•ˆç‡
            persistent_workers=True if num_workers > 0 else False,  # ä¿æŒå·¥ä½œè¿›ç¨‹ï¼Œå‡å°‘é‡å¯å¼€é”€
            drop_last=False  # ä¿ç•™æœ€åä¸€ä¸ªä¸å®Œæ•´çš„batch
        )
    except Exception as e:
        print(f"âš ï¸  å¤šè¿›ç¨‹æ•°æ®åŠ è½½å¤±è´¥ï¼Œå›é€€åˆ°å•è¿›ç¨‹æ¨¡å¼: {e}")
        return DataLoader(
            dataset, 
            batch_size=batch_size, 
            shuffle=shuffle, 
            num_workers=0,
            pin_memory=False
        )


def load_mnist_test_data(batch_size: int = 128, data_dir: str = './data'):
    """
    åŠ è½½MNISTæµ‹è¯•æ•°æ®ï¼Œç”¨äºè‡ªç¼–ç å™¨æ€§èƒ½è¯„ä¼°

    Args:
        batch_size: æ‰¹æ¬¡å¤§å°
        data_dir: æ•°æ®ç›®å½•

    Returns:
        MNISTæµ‹è¯•æ•°æ®åŠ è½½å™¨
    """
    # ä½¿ç”¨ä¸train_autoencoder.pyç›¸åŒçš„æ•°æ®é¢„å¤„ç†
    transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.1307,), (0.3081,))
    ])

    # åŠ è½½MNISTæµ‹è¯•é›†
    test_dataset = datasets.MNIST(data_dir, train=False, download=True, transform=transform)
    test_loader = create_safe_dataloader(test_dataset, batch_size=batch_size, shuffle=False)

    print(f"âœ“ å·²åŠ è½½MNISTæµ‹è¯•é›†: {len(test_dataset)} ä¸ªæ ·æœ¬")
    return test_loader


def load_chestmnist_data(data_root: str = './data'):
    """
    åŠ è½½ChestMNISTæ•°æ®é›†ç”¨äºå¾®è°ƒè®­ç»ƒ

    Args:
        data_root: æ•°æ®æ ¹ç›®å½•

    Returns:
        è®­ç»ƒå’Œæµ‹è¯•æ•°æ®åŠ è½½å™¨
    """
    # ChestMNISTæ•°æ®é¢„å¤„ç†
    normalize = transforms.Normalize(mean=[0.5], std=[0.5])

    transform_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.ToTensor(),
        normalize,
    ])
    transform_test = transforms.Compose([
        transforms.ToTensor(),
        normalize,
    ])

    # åŠ è½½ChestMNISTæ•°æ®é›†
    dataset_path = os.path.join(data_root, 'chestmnist.npz')
    if not os.path.exists(dataset_path):
        raise FileNotFoundError(f"ChestMNISTæ•°æ®é›†æ–‡ä»¶ä¸å­˜åœ¨: {dataset_path}")

    train_set = LocalChestMNISTDataset(dataset_path, split='train', transform=transform_train)
    test_set = LocalChestMNISTDataset(dataset_path, split='test', transform=transform_test)

    print(f"âœ“ å·²åŠ è½½ChestMNISTæ•°æ®é›† - è®­ç»ƒé›†: {len(train_set)}, æµ‹è¯•é›†: {len(test_set)}")

    return train_set, test_set


def load_main_task_model(model_path: str, device: str = 'cuda'):
    """
    åŠ è½½ä¸»ä»»åŠ¡æ¨¡å‹ï¼ˆResNet18 for ChestMNISTï¼‰

    Args:
        model_path: æ¨¡å‹æ–‡ä»¶è·¯å¾„
        device: è®¾å¤‡ç±»å‹

    Returns:
        åŠ è½½çš„æ¨¡å‹
    """
    # åˆ›å»ºæ¨¡å‹å®ä¾‹
    model = resnet18(num_classes=14, in_channels=3, input_size=28)

    # åŠ è½½æ¨¡å‹æƒé‡
    if os.path.exists(model_path):
        checkpoint = torch.load(model_path, map_location=device, weights_only=False)
        model.load_state_dict(checkpoint['net_info']['best_model'][0])
        print(f"âœ“ å·²åŠ è½½ä¸»ä»»åŠ¡æ¨¡å‹: {model_path}")
    else:
        print(f"âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
        return None

    model = model.to(device)
    model.train()  # è®¾ç½®ä¸ºè®­ç»ƒæ¨¡å¼ï¼Œå› ä¸ºè¦è¿›è¡Œå¾®è°ƒ

    return model


def finetune_model(model, train_loader, test_loader, epochs: int, lr: float = 0.001,
                   device: str = 'cuda', eval_interval: int = 10, pcc_interval: int = 10,
                   reconstructor=None, original_model_state=None, mnist_test_loader=None, fixed_tau=None):
    """
    å¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒè®­ç»ƒï¼ˆç²¾ç®€è¾“å‡ºï¼‰
    
    Args:
        model: è¦å¾®è°ƒçš„æ¨¡å‹
        train_loader: è®­ç»ƒæ•°æ®åŠ è½½å™¨
        test_loader: æµ‹è¯•æ•°æ®åŠ è½½å™¨
        epochs: è®­ç»ƒè½®æ•°
        lr: å­¦ä¹ ç‡
        device: è®¾å¤‡
        eval_interval: åŸºæœ¬è¯„ä¼°é—´éš”ï¼ˆæ¯è½®æ˜¾ç¤ºè®­ç»ƒ/æµ‹è¯•æŒ‡æ ‡ï¼‰
        pcc_interval: PCCè®¡ç®—é—´éš”ï¼ˆæ¯å‡ è½®è®¡ç®—ä¸€æ¬¡Î”PCCå’Œä¾µæƒæ£€æµ‹ï¼‰
        reconstructor: æ°´å°é‡å»ºå™¨
        original_model_state: åŸå§‹æ¨¡å‹çŠ¶æ€
        mnist_test_loader: MNISTæµ‹è¯•æ•°æ®åŠ è½½å™¨
        fixed_tau: å›ºå®šé˜ˆå€¼Ï„
    """
    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)
    criterion = nn.BCEWithLogitsLoss()
    # ç¡®ä¿step_sizeè‡³å°‘ä¸º1ï¼Œé¿å…é™¤é›¶é”™è¯¯
    step_size = max(1, epochs//3) if epochs > 0 else 1
    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=0.1)

    model_states, performance_metrics = [], []
    print(f"å¼€å§‹å¾®è°ƒè®­ç»ƒï¼Œå…± {epochs} è½®ï¼Œæ¯ {eval_interval} è½®è¯„ä¼°ä¸€æ¬¡ï¼Œæ¯ {pcc_interval} è½®è®¡ç®—PCC")
    
    # åˆå§‹åŒ–delta_pcc_resultå˜é‡
    delta_pcc_result = None

    for epoch in range(epochs):
        model.train()
        total_loss = 0.0

        try:
            for data, target in train_loader:
                data, target = data.to(device), target.to(device)
                optimizer.zero_grad()
                loss = criterion(model(data), target.float())
                loss.backward()
                optimizer.step()
                total_loss += loss.item()
        except Exception as e:
            print(f"âŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
            print("å°è¯•é‡æ–°åˆ›å»ºæ•°æ®åŠ è½½å™¨...")
            # é‡æ–°åˆ›å»ºæ•°æ®åŠ è½½å™¨
            train_loader = create_safe_dataloader(train_loader.dataset, train_loader.batch_size, shuffle=True, num_workers=0)
            continue

        avg_loss = total_loss / len(train_loader)
        scheduler.step()

        # æ¯è½®éƒ½è¿›è¡ŒåŸºæœ¬è¯„ä¼°ï¼ˆæŸå¤±ã€AUCã€å‡†ç¡®ç‡ï¼‰
        model.eval()
        test_loss, all_predictions, all_targets = 0.0, [], []
        with torch.no_grad():
            for data, target in test_loader:
                data, target = data.to(device), target.to(device)
                output = model(data)
                test_loss += criterion(output, target.float()).item()
                all_predictions.append(torch.sigmoid(output).cpu().numpy())
                all_targets.append(target.cpu().numpy())

        avg_test_loss = test_loss / len(test_loader)
        all_predictions = np.concatenate(all_predictions, axis=0)
        all_targets = np.concatenate(all_targets, axis=0)

        try:
            from sklearn.metrics import roc_auc_score
            auc_scores = [
                roc_auc_score(all_targets[:, i], all_predictions[:, i])
                for i in range(all_targets.shape[1])
                if len(np.unique(all_targets[:, i])) > 1
            ]
            mean_auc = np.mean(auc_scores) if auc_scores else 0.0
        except ImportError:
            mean_auc = 0.0

        pred_binary = (all_predictions > 0.5).astype(int)
        accuracy = np.mean((pred_binary == all_targets).astype(float))

        # æ‰“å°åŸºæœ¬æŒ‡æ ‡ï¼ˆæ¯è½®éƒ½æ˜¾ç¤ºï¼‰
        print(f"\n=== ç¬¬ {epoch+1} è½®è¯„ä¼° ===")
        print(f"è®­ç»ƒæŸå¤±: {avg_loss:.4f} | æµ‹è¯•æŸå¤±: {avg_test_loss:.4f} | "
              f"AUC: {mean_auc:.4f} | å‡†ç¡®ç‡: {accuracy:.2%}")

        # æ ¹æ®pcc_intervalå‚æ•°è®¡ç®—Î”PCCå’Œä¾µæƒæ£€æµ‹ï¼ˆè®¡ç®—é‡å¤§çš„æ“ä½œï¼‰
        delta_pcc_result = None
        if (epoch + 1) % pcc_interval == 0:
            print("ğŸ” è¿›è¡ŒÎ”PCCå’Œä¾µæƒæ£€æµ‹è¯„ä¼°...")
            # ä¿å­˜çŠ¶æ€
            model_states.append(copy.deepcopy(model.state_dict()))
            
            if reconstructor and original_model_state and mnist_test_loader:
                # ä½¿ç”¨torch.no_grad()å‡å°‘å†…å­˜ä½¿ç”¨
                with torch.no_grad():
                    delta_pcc_result = evaluate_delta_pcc(
                        original_model_state, model_states[-1], reconstructor,
                        mnist_test_loader, device, perf_fail_ratio=0.1, fixed_tau=fixed_tau
                    )
            
            # æ‰“å°Î”PCCç»“æœ
            print_delta_pcc_summary(delta_pcc_result)
            
            # æ¸…ç†å†…å­˜
            if torch.cuda.is_available():
                torch.cuda.empty_cache()

        # ä¿å­˜æ€§èƒ½æŒ‡æ ‡ï¼ˆæ¯è½®éƒ½ä¿å­˜ï¼‰
        metrics = {
            'epoch': epoch + 1,
            'train_loss': avg_loss,
            'test_loss': avg_test_loss,
            'test_auc': mean_auc,
            'test_accuracy': accuracy,
            'learning_rate': optimizer.param_groups[0]['lr']
        }
        
        # æ·»åŠ Î”PCCå’Œä¾µæƒåˆ¤æ–­ä¿¡æ¯ï¼ˆæ¯10è½®æ›´æ–°ï¼‰
        if delta_pcc_result:
            metrics.update({
                'perf_before': delta_pcc_result['perf_before'],
                'perf_fail': delta_pcc_result['perf_fail'],
                'tau': delta_pcc_result['tau'],
                'delta_perf': delta_pcc_result['delta_perf'],
                'delta_pcc': delta_pcc_result['delta_pcc'],
                'is_infringement': delta_pcc_result['is_infringement'],
                'result_text': delta_pcc_result['result_text']
            })
        else:
            # å¦‚æœæ²¡æœ‰Î”PCCç»“æœï¼Œå¡«å……é»˜è®¤å€¼
            metrics.update({
                'perf_before': None,
                'perf_fail': None,
                'tau': None,
                'delta_perf': None,
                'delta_pcc': None,
                'is_infringement': None,
                'result_text': 'N/A'
            })
        
        performance_metrics.append(metrics)
        
        # æ›´æ–°metricsä¸­çš„Î”PCCä¿¡æ¯
        current_metrics = performance_metrics[-1]
        current_metrics.update(format_delta_pcc_result(delta_pcc_result))
        
        # æ¸…ç†delta_pcc_resultå†…å­˜
        if delta_pcc_result:
            del delta_pcc_result
            delta_pcc_result = None

        print("-" * 50)
        
        # æ¸…ç†åŸºæœ¬è¯„ä¼°çš„ä¸´æ—¶å˜é‡
        del all_predictions, all_targets

    return model_states, performance_metrics


def evaluate_watermark_integrity(model_state_dict, reconstructor):
    """
    è¯„ä¼°æ°´å°å®Œæ•´æ€§

    Args:
        model_state_dict: æ¨¡å‹çŠ¶æ€å­—å…¸
        reconstructor: æ°´å°é‡å»ºå™¨

    Returns:
        æ°´å°å®Œæ•´æ€§è¯„ä¼°ç»“æœ
    """
    try:
        # ä»æ¨¡å‹çŠ¶æ€å­—å…¸é‡å»ºè‡ªç¼–ç å™¨
        reconstructed_autoencoder = reconstructor.reconstruct_autoencoder_from_all_clients(model_state_dict)

        if reconstructed_autoencoder is None:
            return {
                'watermark_integrity': 0.0,
                'reconstruction_success': False,
                'total_watermark_params': 0,
                'damaged_watermark_params': 0
            }

        # è®¡ç®—æ°´å°å‚æ•°ç»Ÿè®¡
        key_manager = reconstructor.key_manager
        all_client_ids = key_manager.list_clients()

        total_watermark_params = 0
        damaged_watermark_params = 0

        for cid in all_client_ids:
            try:
                # æå–æ°´å°å‚æ•°
                watermark_values = key_manager.extract_watermark(model_state_dict, cid, check_pruning=True)
                total_watermark_params += len(watermark_values)

                # æ£€æŸ¥è¢«ç ´åçš„æ°´å°å‚æ•°ï¼ˆå®Œå…¨ç­‰äº0çš„å‚æ•°ï¼‰
                damaged_count = (watermark_values == 0.0).sum().item()
                damaged_watermark_params += damaged_count

            except Exception as e:
                pass  # é™é»˜å¤„ç†é”™è¯¯

        # è®¡ç®—æ°´å°å®Œæ•´æ€§
        if total_watermark_params > 0:
            watermark_integrity = 1.0 - (damaged_watermark_params / total_watermark_params)
        else:
            watermark_integrity = 0.0

        return {
            'watermark_integrity': watermark_integrity,
            'reconstruction_success': True,
            'total_watermark_params': total_watermark_params,
            'damaged_watermark_params': damaged_watermark_params
        }

    except Exception as e:
        print(f"âŒ æ°´å°å®Œæ•´æ€§è¯„ä¼°å¤±è´¥: {e}")
        return {
            'watermark_integrity': 0.0,
            'reconstruction_success': False,
            'total_watermark_params': 0,
            'damaged_watermark_params': 0
        }



def save_results(results, save_dir: str = './save/finetune_attack'):
    """
    ä¿å­˜å®éªŒç»“æœ

    Args:
        results: å®éªŒç»“æœ
        save_dir: ä¿å­˜ç›®å½•
    """
    os.makedirs(save_dir, exist_ok=True)

    # ä¿å­˜è¯¦ç»†ç»“æœ
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    results_file = os.path.join(save_dir, f'finetune_attack_results_{timestamp}.pkl')
    torch.save(results, results_file)

    # ä¿å­˜CSVæ ¼å¼çš„ç®€åŒ–ç»“æœ
    csv_file = os.path.join(save_dir, f'finetune_attack_summary_{timestamp}.csv')

    import pandas as pd
    df_data = []
    for result in results:
        df_data.append({
            'epoch': result['epoch'],
            'train_loss': result['train_loss'],
            'test_loss': result['test_loss'],
            'test_auc': result['test_auc'],
            'test_accuracy': result['test_accuracy'],
            'learning_rate': result['learning_rate'],
            'perf_before': result.get('perf_before', None),
            'perf_fail': result.get('perf_fail', None),
            'tau': result.get('tau', None),
            'delta_perf': result.get('delta_perf', None),
            'delta_pcc': result.get('delta_pcc', None),
            'is_infringement': result.get('is_infringement', None),
            'result_text': result.get('result_text', 'N/A')
        })

    df = pd.DataFrame(df_data)
    df.to_csv(csv_file, index=False, encoding='utf-8-sig')

    print(f"âœ“ ç»“æœå·²ä¿å­˜åˆ°: {save_dir}")
    print(f"  - è¯¦ç»†ç»“æœ: {results_file}")
    print(f"  - æ±‡æ€»ç»“æœ: {csv_file}")


def main():
    """ä¸»å‡½æ•°"""
    # è®¾ç½®è®¾å¤‡
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")

    # é…ç½®å‚æ•°
    model_path = './save/resnet/chestmnist/202510161541_Dp_0.1_iid_True_ns_1_wt_gamma_lt_sign_ep_100_le_2_cn_10_fra_1.0000_auc_0.7033_enhanced.pkl'
    key_matrix_dir = './save/key_matrix'
    autoencoder_dir = './save/autoencoder'

    # å¾®è°ƒå‚æ•°
    finetune_epochs = 100
    eval_interval = 1  # æ¯è½®éƒ½è¿›è¡ŒåŸºæœ¬è¯„ä¼°
    pcc_interval = 10  # PCCè®¡ç®—é—´éš”ï¼Œå¯ä»¥è°ƒæ•´ï¼ˆå»ºè®®5-20ä¹‹é—´ï¼Œå€¼è¶Šå¤§è®¡ç®—è¶Šå°‘ä½†ç›‘æ§è¶Šç²—ç³™ï¼‰
    learning_rate = 0.001
    batch_size = 128

    print(f"å¾®è°ƒæ”»å‡»å®éªŒå‚æ•°:")
    print(f"  - å¾®è°ƒè½®æ•°: {finetune_epochs}")
    print(f"  - åŸºæœ¬è¯„ä¼°: æ¯è½® | Î”PCCè¯„ä¼°: æ¯{pcc_interval}è½®")
    print(f"  - å­¦ä¹ ç‡: {learning_rate}")
    print(f"  - æ‰¹æ¬¡å¤§å°: {batch_size}")
    print("-" * 60)

    # åŠ è½½æ•°æ®
    print("åŠ è½½æ•°æ®...")
    mnist_test_loader = load_mnist_test_data(batch_size=128)
    chestmnist_train_set, chestmnist_test_set = load_chestmnist_data()

    # åˆ›å»ºæ•°æ®åŠ è½½å™¨ (ä½¿ç”¨å®‰å…¨çš„æ•°æ®åŠ è½½å™¨åˆ›å»ºå‡½æ•°)
    train_loader = create_safe_dataloader(chestmnist_train_set, batch_size=batch_size, shuffle=True)
    test_loader = create_safe_dataloader(chestmnist_test_set, batch_size=batch_size*2, shuffle=False)

    # åŠ è½½ä¸»ä»»åŠ¡æ¨¡å‹
    print("åŠ è½½ä¸»ä»»åŠ¡æ¨¡å‹...")
    model = load_main_task_model(model_path, device)
    if model is None:
        print("âŒ ä¸»ä»»åŠ¡æ¨¡å‹åŠ è½½å¤±è´¥")
        return

    # ä¿å­˜åŸå§‹æ¨¡å‹çŠ¶æ€
    original_model_state = copy.deepcopy(model.state_dict())

    # åˆå§‹åŒ–æ°´å°é‡å»ºå™¨ï¼ˆä½¿ç”¨argsä¸­çš„ç»Ÿä¸€è®¾ç½®ï¼‰
    from utils.args import parser_args
    args = parser_args()
    reconstructor = WatermarkReconstructor(
        key_matrix_dir, 
        autoencoder_dir, 
        enable_scaling=args.enable_watermark_scaling, 
        scaling_factor=args.scaling_factor
    )
    
    # é¢„è®¡ç®—å›ºå®šé˜ˆå€¼Ï„ï¼Œé¿å…é‡å¤è®¡ç®—
    print("é¢„è®¡ç®—å›ºå®šé˜ˆå€¼Ï„...")
    fixed_tau = None
    if reconstructor and original_model_state and mnist_test_loader:
        fixed_tau = calculate_fixed_tau(original_model_state, reconstructor, mnist_test_loader, device, perf_fail_ratio=0.05)
        if fixed_tau is None:
            print("âŒ æ— æ³•è®¡ç®—å›ºå®šé˜ˆå€¼ï¼Œå°†ä½¿ç”¨åŠ¨æ€é˜ˆå€¼")
        else:
            print(f"âœ“ å›ºå®šé˜ˆå€¼Ï„={fixed_tau:.6f}")

    print("å¼€å§‹å¾®è°ƒæ”»å‡»å®éªŒ...")
    print("=" * 80)
    
    
    # ç¬¬0è½®ï¼šæµ‹è¯•å¾®è°ƒå‰çš„æ°´å°æ£€æµ‹
    print("=== ç¬¬0è½®è¯„ä¼°ï¼ˆå¾®è°ƒå‰ï¼‰===")
    
    # å…ˆè¿›è¡ŒAUCè¯„ä¼°
    model.eval()
    test_loss, all_predictions, all_targets = 0.0, [], []
    criterion = nn.BCEWithLogitsLoss()
    
    with torch.no_grad():
        for data, target in test_loader:
            data, target = data.to(device), target.to(device)
            output = model(data)
            test_loss += criterion(output, target.float()).item()
            all_predictions.append(torch.sigmoid(output).cpu().numpy())
            all_targets.append(target.cpu().numpy())

    avg_test_loss = test_loss / len(test_loader)
    all_predictions = np.concatenate(all_predictions, axis=0)
    all_targets = np.concatenate(all_targets, axis=0)

    # è®¡ç®—AUCï¼ˆä½¿ç”¨ä¸è®­ç»ƒæ—¶ç›¸åŒçš„é€»è¾‘ï¼‰
    try:
        from sklearn.metrics import roc_auc_score
        auc_scores = [
            roc_auc_score(all_targets[:, i], all_predictions[:, i])
            for i in range(all_targets.shape[1])
            if len(np.unique(all_targets[:, i])) > 1
        ]
        mean_auc = np.mean(auc_scores) if auc_scores else 0.0
    except ImportError:
        mean_auc = 0.0

    # è®¡ç®—å‡†ç¡®ç‡
    pred_binary = (all_predictions > 0.5).astype(int)
    accuracy = np.mean((pred_binary == all_targets).astype(float))
    
    print(f"æµ‹è¯•æŸå¤±: {avg_test_loss:.4f} | AUC: {mean_auc:.4f} | å‡†ç¡®ç‡: {accuracy:.2%}")
    
    # ==================== æ°´å°æ£€æµ‹å®¹å¿åº¦è®¾ç½® ====================
    PERF_FAIL_RATIO = 0.1
    # =========================================================
    
    # å›ºå®šé˜ˆå€¼Ï„å·²åœ¨ä¸Šé¢é¢„è®¡ç®—ï¼Œè¿™é‡Œç›´æ¥ä½¿ç”¨
    
    # è¿›è¡ŒÎ”PCCè¯„ä¼°
    delta_pcc_result_0 = evaluate_delta_pcc(
        original_model_state, original_model_state, reconstructor,
        mnist_test_loader, device, perf_fail_ratio=PERF_FAIL_RATIO, fixed_tau=fixed_tau
    )
    
    # åˆ›å»ºç¬¬0è½®çš„ç»“æœè®°å½•
    initial_result = {
        'epoch': 0,
        'train_loss': 0.0,  # ç¬¬0è½®æ²¡æœ‰è®­ç»ƒ
        'test_loss': avg_test_loss,   # ç¬¬0è½®æµ‹è¯•æŸå¤±
        'test_auc': mean_auc,    # ç¬¬0è½®æµ‹è¯•AUC
        'test_accuracy': accuracy,  # ç¬¬0è½®æµ‹è¯•å‡†ç¡®ç‡
        'learning_rate': 0.0,  # ç¬¬0è½®æ²¡æœ‰å­¦ä¹ ç‡
    }
    
    # æ·»åŠ ç¬¬0è½®çš„Î”PCCå’Œä¾µæƒåˆ¤æ–­ä¿¡æ¯
    initial_result.update(format_delta_pcc_result(delta_pcc_result_0))

    # è¿›è¡Œå¾®è°ƒè®­ç»ƒ
    model_states, performance_metrics = finetune_model(
        model, train_loader, test_loader,
        epochs=finetune_epochs, lr=learning_rate,
        device=device, eval_interval=eval_interval, pcc_interval=pcc_interval,
        reconstructor=reconstructor, original_model_state=original_model_state,
        mnist_test_loader=mnist_test_loader, fixed_tau=fixed_tau
    )

    # å¾®è°ƒè®­ç»ƒå·²å®Œæˆï¼ŒÎ”PCCå’Œä¾µæƒåˆ¤æ–­å·²åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è¯„ä¼°
    print("\n" + "=" * 80)
    print("å¾®è°ƒæ”»å‡»å®éªŒå®Œæˆ")
    print("=" * 80)

    # å°†ç¬¬0è½®ç»“æœå’Œè®­ç»ƒç»“æœåˆå¹¶
    results = [initial_result] + performance_metrics

    # ä¿å­˜ç»“æœ
    print("\n" + "=" * 80)
    print("ä¿å­˜å®éªŒç»“æœ...")
    save_results(results)

    # è¾“å‡ºæ€»ç»“
    print("\n" + "=" * 80)
    print("å¾®è°ƒæ”»å‡»å®éªŒæ€»ç»“")
    print("=" * 80)
    print(f"{'è½®æ¬¡':<4} {'è®­ç»ƒæŸå¤±':<10} {'æµ‹è¯•æŸå¤±':<10} {'æµ‹è¯•AUC':<8} {'æµ‹è¯•å‡†ç¡®ç‡%':<10} {'Î”PCC':<8} {'ä¾µæƒåˆ¤æ–­':<8}")
    print("-" * 80)

    for result in results:
        delta_pcc_str = f"{result['delta_pcc']:.4f}" if result['delta_pcc'] is not None else "N/A"
        infringement_str = "æ˜¯" if result['is_infringement'] else "å¦" if result['is_infringement'] is not None else "N/A"
        
        print(f"{result['epoch']:>3}  "
              f"{result['train_loss']:>8.4f}  "
              f"{result['test_loss']:>8.4f}  "
              f"{result['test_auc']:>6.4f}  "
              f"{result['test_accuracy']:>8.2%}  "
              f"{delta_pcc_str:>6}  "
              f"{infringement_str:>6}")

    # åˆ†æè¶‹åŠ¿
    print("\nè¶‹åŠ¿åˆ†æ:")
    if len(results) > 1:
        initial_auc = results[0]['test_auc']
        final_auc = results[-1]['test_auc']
        auc_change = final_auc - initial_auc

        initial_acc = results[0]['test_accuracy']
        final_acc = results[-1]['test_accuracy']
        acc_change = final_acc - initial_acc

        print(f"æµ‹è¯•AUCå˜åŒ–: {initial_auc:.4f} â†’ {final_auc:.4f} (å˜åŒ–: {auc_change:+.4f})")
        print(f"æµ‹è¯•å‡†ç¡®ç‡å˜åŒ–: {initial_acc:.2%} â†’ {final_acc:.2%} (å˜åŒ–: {acc_change:+.2%})")
        
        # åˆ†æÎ”PCCè¶‹åŠ¿
        delta_pcc_values = [r['delta_pcc'] for r in results if r['delta_pcc'] is not None]
        if len(delta_pcc_values) > 1:
            initial_delta_pcc = delta_pcc_values[0]
            final_delta_pcc = delta_pcc_values[-1]
            delta_pcc_change = final_delta_pcc - initial_delta_pcc
            print(f"Î”PCCå˜åŒ–: {initial_delta_pcc:.4f} â†’ {final_delta_pcc:.4f} (å˜åŒ–: {delta_pcc_change:+.4f})")
        
        # åˆ†æä¾µæƒåˆ¤æ–­
        infringement_count = sum(1 for r in results if r['is_infringement'] is True)
        total_evaluations = sum(1 for r in results if r['is_infringement'] is not None)
        if total_evaluations > 0:
            infringement_rate = infringement_count / total_evaluations
            print(f"ä¾µæƒåˆ¤æ–­: {infringement_count}/{total_evaluations} è½®è¢«åˆ¤å®šä¸ºä¾µæƒ ({infringement_rate:.1%})")

    print("\nå¾®è°ƒæ”»å‡»å®éªŒå®Œæˆï¼")


if __name__ == '__main__':
    main()
