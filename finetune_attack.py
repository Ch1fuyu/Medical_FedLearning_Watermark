#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¾®è°ƒæ”»å‡»å®éªŒä»£ç 
é’ˆå¯¹æ¨¡å‹å¾®è°ƒæ”»å‡»çš„å®éªŒï¼Œç»§ç»­å¯¹ä¸»ä»»åŠ¡è¿›è¡Œè®­ç»ƒï¼Œè§‚å¯Ÿæ°´å°å®Œæ•´æ€§ã€Î”PCCå€¼å˜åŒ–ä»¥åŠåˆ¤æ–­æ˜¯å¦ä¾µæƒ
"""

import copy
import os
import sys
from datetime import datetime

import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from torchvision import datasets, transforms

# Windowså¤šè¿›ç¨‹å…¼å®¹æ€§å¤„ç†
if sys.platform.startswith('win'):
    import multiprocessing
    multiprocessing.set_start_method('spawn', force=True)

from models.resnet import resnet18
from utils.dataset import LocalChestMNISTDataset
from utils.watermark_reconstruction import WatermarkReconstructor
from utils.delta_pcc_utils import evaluate_delta_pcc, calculate_fixed_tau, format_delta_pcc_result, print_delta_pcc_summary


def extract_model_info_from_path(model_path):
    """
    ä»æ¨¡å‹è·¯å¾„ä¸­æå–æ•°æ®é›†å’Œæ¨¡å‹åä¿¡æ¯
    
    Args:
        model_path: æ¨¡å‹æ–‡ä»¶è·¯å¾„
        
    Returns:
        dict: åŒ…å«datasetå’Œmodel_nameçš„å­—å…¸
    """
    try:
        # æ ‡å‡†åŒ–è·¯å¾„åˆ†éš”ç¬¦
        normalized_path = model_path.replace('\\', '/')
        
        # åˆ†å‰²è·¯å¾„
        path_parts = normalized_path.split('/')
        
        # æŸ¥æ‰¾æ•°æ®é›†å’Œæ¨¡å‹å
        dataset = 'unknown'
        model_name = 'unknown'
        
        # ä»è·¯å¾„ä¸­æå–ä¿¡æ¯
        for i, part in enumerate(path_parts):
            if part in ['cifar10', 'cifar100', 'chestmnist']:
                dataset = part
            elif part in ['resnet', 'cnn', 'vgg', 'densenet']:
                model_name = part
            elif part == 'resnet18':
                model_name = 'resnet'
            elif part == 'cnn_simple':
                model_name = 'cnn'
        
        return {
            'dataset': dataset,
            'model_name': model_name
        }
        
    except Exception as e:
        print(f"è­¦å‘Š: æ— æ³•ä»è·¯å¾„ä¸­æå–æ¨¡å‹ä¿¡æ¯: {e}")
        return {
            'dataset': 'unknown',
            'model_name': 'unknown'
        }


def create_safe_dataloader(dataset, batch_size, shuffle=False, num_workers=None):
    """
    åˆ›å»ºå®‰å…¨çš„æ•°æ®åŠ è½½å™¨ï¼Œè‡ªåŠ¨å¤„ç†Windowså¤šè¿›ç¨‹é—®é¢˜
    
    Args:
        dataset: æ•°æ®é›†
        batch_size: æ‰¹æ¬¡å¤§å°
        shuffle: æ˜¯å¦æ‰“ä¹±æ•°æ®
        num_workers: å·¥ä½œè¿›ç¨‹æ•°ï¼ŒNoneæ—¶è‡ªåŠ¨é€‰æ‹©
        
    Returns:
        æ•°æ®åŠ è½½å™¨
    """
    import sys
    
    # ä¼˜åŒ–å¤šè¿›ç¨‹è®¾ç½®ï¼Œæé«˜æ•°æ®åŠ è½½æ•ˆç‡
    if num_workers is None:
        if sys.platform.startswith('win'):
            num_workers = 2  # Windowsä¸Šä½¿ç”¨2ä¸ªè¿›ç¨‹
        else:
            num_workers = 4  # Linux/Macä¸Šä½¿ç”¨4ä¸ªè¿›ç¨‹
    
    try:
        return DataLoader(
            dataset, 
            batch_size=batch_size, 
            shuffle=shuffle, 
            num_workers=num_workers,
            pin_memory=True,  # å¯ç”¨å†…å­˜å›ºå®šï¼Œæé«˜GPUä¼ è¾“æ•ˆç‡
            persistent_workers=True if num_workers > 0 else False,  # ä¿æŒå·¥ä½œè¿›ç¨‹ï¼Œå‡å°‘é‡å¯å¼€é”€
            drop_last=False  # ä¿ç•™æœ€åä¸€ä¸ªä¸å®Œæ•´çš„batch
        )
    except Exception as e:
        print(f"âš ï¸  å¤šè¿›ç¨‹æ•°æ®åŠ è½½å¤±è´¥ï¼Œå›é€€åˆ°å•è¿›ç¨‹æ¨¡å¼: {e}")
        return DataLoader(
            dataset, 
            batch_size=batch_size, 
            shuffle=shuffle, 
            num_workers=0,
            pin_memory=False
        )


def load_mnist_test_data(batch_size: int = 128, data_dir: str = './data'):
    """
    åŠ è½½MNISTæµ‹è¯•æ•°æ®ï¼Œç”¨äºè‡ªç¼–ç å™¨æ€§èƒ½è¯„ä¼°

    Args:
        batch_size: æ‰¹æ¬¡å¤§å°
        data_dir: æ•°æ®ç›®å½•

    Returns:
        MNISTæµ‹è¯•æ•°æ®åŠ è½½å™¨
    """
    # ä½¿ç”¨ä¸train_autoencoder.pyç›¸åŒçš„æ•°æ®é¢„å¤„ç†
    transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.1307,), (0.3081,))
    ])

    # åŠ è½½MNISTæµ‹è¯•é›†
    test_dataset = datasets.MNIST(data_dir, train=False, download=True, transform=transform)
    test_loader = create_safe_dataloader(test_dataset, batch_size=batch_size, shuffle=False)

    print(f"âœ“ å·²åŠ è½½MNISTæµ‹è¯•é›†: {len(test_dataset)} ä¸ªæ ·æœ¬")
    return test_loader


def load_chestmnist_data(data_root: str = './data'):
    """
    åŠ è½½ChestMNISTæ•°æ®é›†ç”¨äºå¾®è°ƒè®­ç»ƒ

    Args:
        data_root: æ•°æ®æ ¹ç›®å½•

    Returns:
        è®­ç»ƒå’Œæµ‹è¯•æ•°æ®åŠ è½½å™¨
    """
    # ChestMNISTæ•°æ®é¢„å¤„ç†
    normalize = transforms.Normalize(mean=[0.5], std=[0.5])

    transform_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.ToTensor(),
        normalize,
    ])
    transform_test = transforms.Compose([
        transforms.ToTensor(),
        normalize,
    ])

    # åŠ è½½ChestMNISTæ•°æ®é›†
    dataset_path = os.path.join(data_root, 'chestmnist.npz')
    if not os.path.exists(dataset_path):
        raise FileNotFoundError(f"ChestMNISTæ•°æ®é›†æ–‡ä»¶ä¸å­˜åœ¨: {dataset_path}")

    train_set = LocalChestMNISTDataset(dataset_path, split='train', transform=transform_train)
    test_set = LocalChestMNISTDataset(dataset_path, split='test', transform=transform_test)

    print(f"âœ“ å·²åŠ è½½ChestMNISTæ•°æ®é›† - è®­ç»ƒé›†: {len(train_set)}, æµ‹è¯•é›†: {len(test_set)}")

    return train_set, test_set


def load_cifar10_data(batch_size: int = 128, data_root: str = './data'):
    """
    åŠ è½½CIFAR-10æ•°æ®é›†ç”¨äºå¾®è°ƒè®­ç»ƒ

    Args:
        batch_size: æ‰¹æ¬¡å¤§å°
        data_root: æ•°æ®æ ¹ç›®å½•

    Returns:
        è®­ç»ƒå’Œæµ‹è¯•æ•°æ®åŠ è½½å™¨
    """
    # CIFAR-10æ•°æ®é¢„å¤„ç†
    transform_train = transforms.Compose([
        transforms.RandomCrop(32, padding=4),
        transforms.RandomHorizontalFlip(),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2470, 0.2435, 0.2616])
    ])
    
    transform_test = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2470, 0.2435, 0.2616])
    ])

    # åŠ è½½CIFAR-10æ•°æ®é›†
    train_dataset = datasets.CIFAR10(
        root=data_root,
        train=True,
        download=True,
        transform=transform_train
    )
    
    test_dataset = datasets.CIFAR10(
        root=data_root,
        train=False,
        download=True,
        transform=transform_test
    )

    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_loader = create_safe_dataloader(train_dataset, batch_size=batch_size, shuffle=True)
    test_loader = create_safe_dataloader(test_dataset, batch_size=batch_size*2, shuffle=False)

    print(f"âœ“ å·²åŠ è½½CIFAR-10æ•°æ®é›†: è®­ç»ƒé›† {len(train_dataset)} ä¸ªæ ·æœ¬, æµ‹è¯•é›† {len(test_dataset)} ä¸ªæ ·æœ¬")
    return train_loader, test_loader


def load_cifar100_data(batch_size: int = 128, data_root: str = './data'):
    """
    åŠ è½½CIFAR-100æ•°æ®é›†ç”¨äºå¾®è°ƒè®­ç»ƒ

    Args:
        batch_size: æ‰¹æ¬¡å¤§å°
        data_root: æ•°æ®æ ¹ç›®å½•

    Returns:
        è®­ç»ƒå’Œæµ‹è¯•æ•°æ®åŠ è½½å™¨
    """
    # CIFAR-100æ•°æ®é¢„å¤„ç†
    transform_train = transforms.Compose([
        transforms.RandomCrop(32, padding=4),
        transforms.RandomHorizontalFlip(),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.5071, 0.4867, 0.4408], std=[0.2675, 0.2565, 0.2761])
    ])
    
    transform_test = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.5071, 0.4867, 0.4408], std=[0.2675, 0.2565, 0.2761])
    ])

    # åŠ è½½CIFAR-100æ•°æ®é›†
    train_dataset = datasets.CIFAR100(
        root=data_root,
        train=True,
        download=True,
        transform=transform_train
    )
    
    test_dataset = datasets.CIFAR100(
        root=data_root,
        train=False,
        download=True,
        transform=transform_test
    )

    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_loader = create_safe_dataloader(train_dataset, batch_size=batch_size, shuffle=True)
    test_loader = create_safe_dataloader(test_dataset, batch_size=batch_size*2, shuffle=False)

    print(f"âœ“ å·²åŠ è½½CIFAR-100æ•°æ®é›†: è®­ç»ƒé›† {len(train_dataset)} ä¸ªæ ·æœ¬, æµ‹è¯•é›† {len(test_dataset)} ä¸ªæ ·æœ¬")
    return train_loader, test_loader


def load_main_task_model(model_path: str, device: str = 'cuda'):
    """
    åŠ è½½ä¸»ä»»åŠ¡æ¨¡å‹ï¼Œè‡ªåŠ¨ä»checkpointæ¨æ–­æ•°æ®é›†å‚æ•°

    Args:
        model_path: æ¨¡å‹æ–‡ä»¶è·¯å¾„
        device: è®¾å¤‡ç±»å‹

    Returns:
        åŠ è½½çš„æ¨¡å‹
    """
    if not os.path.exists(model_path):
        print(f"âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
        return None
    
    # åŠ è½½checkpointè·å–å‚æ•°ä¿¡æ¯
    checkpoint = torch.load(model_path, map_location=device, weights_only=False)
    
    # ä»checkpointä¸­è·å–æ•°æ®é›†å‚æ•°
    net_info = checkpoint.get('net_info', {})
    arguments = checkpoint.get('arguments', {})
    
    # ä¼˜å…ˆä»argumentsè·å–ï¼Œå¦åˆ™ä½¿ç”¨é»˜è®¤å€¼
    dataset = arguments.get('dataset', 'chestmnist')
    in_channels = arguments.get('in_channels', 3)
    
    # æ ¹æ®æ•°æ®é›†è®¾ç½®é»˜è®¤ç±»åˆ«æ•°
    if dataset.lower() == 'cifar10':
        num_classes = arguments.get('num_classes', 10)
    elif dataset.lower() == 'cifar100':
        num_classes = arguments.get('num_classes', 100)
    elif dataset.lower() == 'chestmnist':
        num_classes = arguments.get('num_classes', 14)
    else:
        num_classes = arguments.get('num_classes', 14)
    
    # æ ¹æ®æ•°æ®é›†è®¾ç½®input_size
    if dataset.lower() == 'cifar10' or dataset.lower() == 'cifar100':
        input_size = 32
    elif dataset.lower() == 'imagenet':
        input_size = 224
    else:  # chestmnistç­‰
        input_size = 28
    
    print(f"âœ“ æ£€æµ‹åˆ°æ•°æ®é›†: {dataset}, ç±»åˆ«æ•°: {num_classes}, è¾“å…¥é€šé“: {in_channels}, è¾“å…¥å°ºå¯¸: {input_size}")

    # åˆ›å»ºæ¨¡å‹å®ä¾‹
    model = resnet18(num_classes=num_classes, in_channels=in_channels, input_size=input_size)

    # åŠ è½½æ¨¡å‹æƒé‡
    model.load_state_dict(checkpoint['net_info']['best_model'][0])
    print(f"âœ“ å·²åŠ è½½ä¸»ä»»åŠ¡æ¨¡å‹: {model_path}")

    model = model.to(device)
    model.train()  # è®¾ç½®ä¸ºè®­ç»ƒæ¨¡å¼ï¼Œå› ä¸ºè¦è¿›è¡Œå¾®è°ƒ

    return model


def finetune_model(model, train_loader, test_loader, epochs: int, lr: float = 0.001,
                   device: str = 'cuda', eval_interval: int = 10, pcc_interval: int = 10,
                   reconstructor=None, original_model_state=None, mnist_test_loader=None, fixed_tau=None,
                   optimizer_type: str = 'adam', dataset_type: str = 'chestmnist'):
    """
    å¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒè®­ç»ƒï¼ˆç²¾ç®€è¾“å‡ºï¼‰
    
    Args:
        model: è¦å¾®è°ƒçš„æ¨¡å‹
        train_loader: è®­ç»ƒæ•°æ®åŠ è½½å™¨
        test_loader: æµ‹è¯•æ•°æ®åŠ è½½å™¨
        epochs: è®­ç»ƒè½®æ•°
        lr: å­¦ä¹ ç‡
        device: è®¾å¤‡
        eval_interval: åŸºæœ¬è¯„ä¼°é—´éš”ï¼ˆæ¯è½®æ˜¾ç¤ºè®­ç»ƒ/æµ‹è¯•æŒ‡æ ‡ï¼‰
        pcc_interval: PCCè®¡ç®—é—´éš”ï¼ˆæ¯å‡ è½®è®¡ç®—ä¸€æ¬¡Î”PCCå’Œä¾µæƒæ£€æµ‹ï¼‰
        reconstructor: æ°´å°é‡å»ºå™¨
        original_model_state: åŸå§‹æ¨¡å‹çŠ¶æ€
        mnist_test_loader: MNISTæµ‹è¯•æ•°æ®åŠ è½½å™¨
        fixed_tau: å›ºå®šé˜ˆå€¼Ï„
    """
    # æ ¹æ®optimizer_typeå‚æ•°é€‰æ‹©ä¼˜åŒ–å™¨
    if optimizer_type.lower() == 'sgd':
        optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=1e-4)
    else:  # é»˜è®¤ä½¿ç”¨Adam
        optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)
    
    # æ ¹æ®æ•°æ®é›†ç±»å‹é€‰æ‹©åˆé€‚çš„æŸå¤±å‡½æ•°
    if dataset_type == 'chestmnist':
        criterion = nn.BCEWithLogitsLoss()
    else:  # cifar10 ç­‰å¤šåˆ†ç±»ä»»åŠ¡
        criterion = nn.CrossEntropyLoss()
    
    # ç¡®ä¿step_sizeè‡³å°‘ä¸º1ï¼Œé¿å…é™¤é›¶é”™è¯¯
    step_size = max(1, epochs//3) if epochs > 0 else 1
    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=0.1)

    model_states, performance_metrics = [], []
    print(f"å¼€å§‹å¾®è°ƒè®­ç»ƒï¼Œå…± {epochs} è½®ï¼Œæ¯ {eval_interval} è½®è¯„ä¼°ä¸€æ¬¡ï¼Œæ¯ {pcc_interval} è½®è®¡ç®—PCC")
    
    # åˆå§‹åŒ–delta_pcc_resultå˜é‡
    delta_pcc_result = None

    for epoch in range(epochs):
        model.train()
        total_loss = 0.0

        try:
            for data, target in train_loader:
                data, target = data.to(device), target.to(device)
                optimizer.zero_grad()
                
                if dataset_type == 'chestmnist':
                    loss = criterion(model(data), target.float())
                else:  # å¤šåˆ†ç±»ä»»åŠ¡
                    loss = criterion(model(data), target.long())
                
                loss.backward()
                optimizer.step()
                total_loss += loss.item()
        except Exception as e:
            print(f"âŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
            print("å°è¯•é‡æ–°åˆ›å»ºæ•°æ®åŠ è½½å™¨...")
            # é‡æ–°åˆ›å»ºæ•°æ®åŠ è½½å™¨
            train_loader = create_safe_dataloader(train_loader.dataset, train_loader.batch_size, shuffle=True, num_workers=0)
            continue

        avg_loss = total_loss / len(train_loader)
        scheduler.step()

        # æ¯è½®éƒ½è¿›è¡ŒåŸºæœ¬è¯„ä¼°ï¼ˆæŸå¤±ã€AUCã€å‡†ç¡®ç‡ï¼‰
        model.eval()
        test_loss, all_predictions, all_targets = 0.0, [], []
        with torch.no_grad():
            for data, target in test_loader:
                data, target = data.to(device), target.to(device)
                output = model(data)
                
                if dataset_type == 'chestmnist':
                    test_loss += criterion(output, target.float()).item()
                    all_predictions.append(torch.sigmoid(output).cpu().numpy())
                else:  # å¤šåˆ†ç±»ä»»åŠ¡
                    test_loss += criterion(output, target.long()).item()
                    all_predictions.append(torch.softmax(output, dim=1).cpu().numpy())
                
                all_targets.append(target.cpu().numpy())

        avg_test_loss = test_loss / len(test_loader)
        all_predictions = np.concatenate(all_predictions, axis=0)
        all_targets = np.concatenate(all_targets, axis=0)

        # è®¡ç®—AUCå’Œå‡†ç¡®ç‡ï¼ˆæ ¹æ®æ•°æ®é›†ç±»å‹ï¼‰
        if dataset_type == 'chestmnist':
            # å¤šæ ‡ç­¾äºŒåˆ†ç±»ä»»åŠ¡
            try:
                from sklearn.metrics import roc_auc_score
                auc_scores = [
                    roc_auc_score(all_targets[:, i], all_predictions[:, i])
                    for i in range(all_targets.shape[1])
                    if len(np.unique(all_targets[:, i])) > 1
                ]
                mean_auc = np.mean(auc_scores) if auc_scores else 0.0
            except ImportError:
                mean_auc = 0.0
            
            # è®¡ç®—å‡†ç¡®ç‡ï¼ˆå¤šæ ‡ç­¾ï¼‰
            pred_binary = (all_predictions > 0.5).astype(int)
            accuracy = np.mean((pred_binary == all_targets).astype(float))
        else:
            # å¤šåˆ†ç±»ä»»åŠ¡
            try:
                from sklearn.metrics import roc_auc_score
                # ä½¿ç”¨one-vs-restç­–ç•¥è®¡ç®—å¤šåˆ†ç±»AUC
                mean_auc = roc_auc_score(all_targets, all_predictions, multi_class='ovr', average='macro')
            except ImportError:
                mean_auc = 0.0
            
            # è®¡ç®—å‡†ç¡®ç‡ï¼ˆå¤šåˆ†ç±»ï¼‰
            pred_classes = np.argmax(all_predictions, axis=1)
            accuracy = np.mean((pred_classes == all_targets).astype(float))

        # æ‰“å°åŸºæœ¬æŒ‡æ ‡ï¼ˆæ¯è½®éƒ½æ˜¾ç¤ºï¼‰
        print(f"\n=== ç¬¬ {epoch+1} è½®è¯„ä¼° ===")
        if dataset_type == 'chestmnist':
            print(f"è®­ç»ƒæŸå¤±: {avg_loss:.4f} | æµ‹è¯•æŸå¤±: {avg_test_loss:.4f} | "
                  f"AUC: {mean_auc:.4f} [ä¸»è¦] | å‡†ç¡®ç‡: {accuracy:.2%} [å‚è€ƒ]")
        else:
            print(f"è®­ç»ƒæŸå¤±: {avg_loss:.4f} | æµ‹è¯•æŸå¤±: {avg_test_loss:.4f} | "
                  f"AUC: {mean_auc:.4f} [å‚è€ƒ] | å‡†ç¡®ç‡: {accuracy:.2%} [ä¸»è¦]")

        # æ ¹æ®pcc_intervalå‚æ•°è®¡ç®—Î”PCCå’Œä¾µæƒæ£€æµ‹ï¼ˆè®¡ç®—é‡å¤§çš„æ“ä½œï¼‰
        delta_pcc_result = None
        if ((epoch + 1) == 1) or ((epoch + 1) % pcc_interval == 0):
            print("ğŸ” è¿›è¡ŒÎ”PCCå’Œä¾µæƒæ£€æµ‹è¯„ä¼°...")
            # ä¿å­˜çŠ¶æ€
            model_states.append(copy.deepcopy(model.state_dict()))
            
            if reconstructor and original_model_state and mnist_test_loader:
                # ä½¿ç”¨torch.no_grad()å‡å°‘å†…å­˜ä½¿ç”¨
                with torch.no_grad():
                    delta_pcc_result = evaluate_delta_pcc(
                        original_model_state, model_states[-1], reconstructor,
                        mnist_test_loader, device, perf_fail_ratio=0.1, fixed_tau=fixed_tau
                    )
            
            # æ‰“å°Î”PCCç»“æœ
            print_delta_pcc_summary(delta_pcc_result)
            
            # æ¸…ç†å†…å­˜
            if torch.cuda.is_available():
                torch.cuda.empty_cache()

        # ä¿å­˜æ€§èƒ½æŒ‡æ ‡ï¼ˆæ¯è½®éƒ½ä¿å­˜ï¼‰
        metrics = {
            'epoch': epoch + 1,
            'train_loss': avg_loss,
            'test_loss': avg_test_loss,
            'test_auc': mean_auc,
            'test_accuracy': accuracy,
            'learning_rate': optimizer.param_groups[0]['lr']
        }
        
        # æ·»åŠ Î”PCCå’Œä¾µæƒåˆ¤æ–­ä¿¡æ¯ï¼ˆæ¯10è½®æ›´æ–°ï¼‰
        if delta_pcc_result:
            metrics.update({
                'perf_before': delta_pcc_result['perf_before'],
                'perf_fail': delta_pcc_result['perf_fail'],
                'tau': delta_pcc_result['tau'],
                'delta_perf': delta_pcc_result['delta_perf'],
                'delta_pcc': delta_pcc_result['delta_pcc'],
                'is_infringement': delta_pcc_result['is_infringement'],
                'result_text': delta_pcc_result['result_text']
            })
        else:
            # å¦‚æœæ²¡æœ‰Î”PCCç»“æœï¼Œå¡«å……é»˜è®¤å€¼
            metrics.update({
                'perf_before': None,
                'perf_fail': None,
                'tau': None,
                'delta_perf': None,
                'delta_pcc': None,
                'is_infringement': None,
                'result_text': 'N/A'
            })
        
        performance_metrics.append(metrics)
        
        # æ›´æ–°metricsä¸­çš„Î”PCCä¿¡æ¯
        current_metrics = performance_metrics[-1]
        current_metrics.update(format_delta_pcc_result(delta_pcc_result))
        
        # æ¸…ç†delta_pcc_resultå†…å­˜
        if delta_pcc_result:
            del delta_pcc_result
            delta_pcc_result = None

        print("-" * 50)
        
        # æ¸…ç†åŸºæœ¬è¯„ä¼°çš„ä¸´æ—¶å˜é‡
        del all_predictions, all_targets

    return model_states, performance_metrics


def evaluate_watermark_integrity(model_state_dict, reconstructor):
    """
    è¯„ä¼°æ°´å°å®Œæ•´æ€§

    Args:
        model_state_dict: æ¨¡å‹çŠ¶æ€å­—å…¸
        reconstructor: æ°´å°é‡å»ºå™¨

    Returns:
        æ°´å°å®Œæ•´æ€§è¯„ä¼°ç»“æœ
    """
    try:
        # ä»æ¨¡å‹çŠ¶æ€å­—å…¸é‡å»ºè‡ªç¼–ç å™¨
        reconstructed_autoencoder = reconstructor.reconstruct_autoencoder_from_all_clients(model_state_dict)

        if reconstructed_autoencoder is None:
            return {
                'watermark_integrity': 0.0,
                'reconstruction_success': False,
                'total_watermark_params': 0,
                'damaged_watermark_params': 0
            }

        # è®¡ç®—æ°´å°å‚æ•°ç»Ÿè®¡
        key_manager = reconstructor.key_manager
        all_client_ids = key_manager.list_clients()

        total_watermark_params = 0
        damaged_watermark_params = 0

        for cid in all_client_ids:
            try:
                # æå–æ°´å°å‚æ•°
                watermark_values = key_manager.extract_watermark(model_state_dict, cid, check_pruning=True)
                total_watermark_params += len(watermark_values)

                # æ£€æŸ¥è¢«ç ´åçš„æ°´å°å‚æ•°ï¼ˆå®Œå…¨ç­‰äº0çš„å‚æ•°ï¼‰
                damaged_count = (watermark_values == 0.0).sum().item()
                damaged_watermark_params += damaged_count

            except Exception as e:
                pass  # é™é»˜å¤„ç†é”™è¯¯

        # è®¡ç®—æ°´å°å®Œæ•´æ€§
        if total_watermark_params > 0:
            watermark_integrity = 1.0 - (damaged_watermark_params / total_watermark_params)
        else:
            watermark_integrity = 0.0

        return {
            'watermark_integrity': watermark_integrity,
            'reconstruction_success': True,
            'total_watermark_params': total_watermark_params,
            'damaged_watermark_params': damaged_watermark_params
        }

    except Exception as e:
        print(f"âŒ æ°´å°å®Œæ•´æ€§è¯„ä¼°å¤±è´¥: {e}")
        return {
            'watermark_integrity': 0.0,
            'reconstruction_success': False,
            'total_watermark_params': 0,
            'damaged_watermark_params': 0
        }



def save_results(results, save_dir: str = './save/finetune_attack'):
    """
    ä¿å­˜å®éªŒç»“æœ

    Args:
        results: å®éªŒç»“æœ
        save_dir: ä¿å­˜ç›®å½•
    """
    os.makedirs(save_dir, exist_ok=True)

    # ç”Ÿæˆæ—¶é—´æˆ³
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    filename_prefix = f'finetune_attack_{timestamp}'
    
    # ä¿å­˜è¯¦ç»†ç»“æœ
    results_file = os.path.join(save_dir, f'{filename_prefix}.pkl')
    torch.save(results, results_file)

    # ä¿å­˜CSVæ ¼å¼çš„ç®€åŒ–ç»“æœ
    csv_file = os.path.join(save_dir, f'{filename_prefix}.csv')

    import pandas as pd
    df_data = []
    for result in results:
        df_data.append({
            'epoch': result['epoch'],
            'train_loss': result['train_loss'],
            'test_loss': result['test_loss'],
            'test_auc': result['test_auc'],
            'test_accuracy': result['test_accuracy'],
            'learning_rate': result['learning_rate'],
            'perf_fail': result.get('perf_fail', None),
            'tau': result.get('tau', None),
            'delta_perf': result.get('delta_perf', None),
            'delta_pcc': result.get('delta_pcc', None),
            'is_infringement': result.get('is_infringement', None),
            'result_text': result.get('result_text', 'N/A')
        })

    df = pd.DataFrame(df_data)
    
    # æ ¼å¼åŒ–æ•°å€¼åˆ—ï¼Œä¿ç•™6ä½å°æ•°
    numeric_columns = ['train_loss', 'test_loss', 'test_auc', 'test_accuracy', 'learning_rate', 
                      'perf_fail', 'tau', 'delta_perf', 'delta_pcc']
    for col in numeric_columns:
        if col in df.columns:
            df[col] = df[col].apply(lambda x: f"{x:.6f}" if pd.notna(x) and isinstance(x, (int, float)) else x)
    
    # åœ¨CSVæ–‡ä»¶æœ€åä¸€è¡Œæ·»åŠ PKLæ–‡ä»¶åä¿¡æ¯
    pkl_filename = os.path.basename(results_file)
    df.loc[len(df)] = ['PKL_FILE'] + [''] * (len(df.columns) - 2) + [pkl_filename]
    
    df.to_csv(csv_file, index=False, encoding='utf-8-sig')

    print(f"âœ“ ç»“æœå·²ä¿å­˜åˆ°: {save_dir}")
    print(f"  - è¯¦ç»†ç»“æœ: {results_file}")
    print(f"  - æ±‡æ€»ç»“æœ: {csv_file}")


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    # é¦–å…ˆåŠ è½½args.pyä¸­çš„å‚æ•°é…ç½®
    from utils.args import parser_args
    base_args = parser_args()
    
    # è§£æå¾®è°ƒæ”»å‡»ç‰¹å®šçš„å‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser(description='å¾®è°ƒæ”»å‡»å®éªŒ')
    parser.add_argument('--model_path', type=str, 
                       default='./save/resnet/chestmnist/202510281303_Dp_0.1_iid_True_wm_enhanced_ep_150_le_2_cn_10_fra_1.0000_auc_0.7646_enhanced.pkl',
                       help='æ¨¡å‹æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--model_type', type=str, default='resnet',
                       choices=['resnet', 'cnn', 'vgg', 'densenet'],
                       help='æ¨¡å‹ç±»å‹')
    parser.add_argument('--client_num', type=int, default=10,
                       help='å®¢æˆ·ç«¯æ•°é‡')
    parser.add_argument('--dataset', type=str, default='chestmnist',
                       choices=['cifar10', 'cifar100', 'chestmnist'],
                       help='æ•°æ®é›†ç±»å‹')
    parser.add_argument('--key_matrix_dir', type=str, default='./save/key_matrix',
                       help='å¯†é’¥çŸ©é˜µåŸºç¡€ç›®å½•')
    parser.add_argument('--autoencoder_dir', type=str, default='./save/autoencoder',
                       help='è‡ªç¼–ç å™¨ç›®å½•')
    parser.add_argument('--optimizer', type=str, default='adam', choices=['sgd', 'adam'],
                       help='ä¼˜åŒ–å™¨ç±»å‹ï¼ˆé»˜è®¤ä½¿ç”¨args.pyä¸­çš„optimï¼‰')
    parser.add_argument('--finetune_epochs', type=int, default=50,
                       help='å¾®è°ƒè½®æ•°')
    parser.add_argument('--learning_rate', type=float, default=0.001,
                       help='å­¦ä¹ ç‡ï¼ˆé»˜è®¤ä½¿ç”¨args.pyä¸­çš„lrï¼‰')
    parser.add_argument('--batch_size', type=int, default=None,
                       help='æ‰¹æ¬¡å¤§å°ï¼ˆé»˜è®¤ä½¿ç”¨args.pyä¸­çš„batch_sizeï¼‰')
    parser.add_argument('--enable_scaling', action='store_true', default=True,
                       help='å¯ç”¨æ°´å°å‚æ•°ç¼©æ”¾')
    parser.add_argument('--scaling_factor', type=float, default=1.0,
                       help='æ°´å°å‚æ•°ç¼©æ”¾å› å­')

    # è§£æå‘½ä»¤è¡Œå‚æ•°
    cmd_args = parser.parse_args()
    
    # åˆå¹¶å‚æ•°ï¼šå‘½ä»¤è¡Œå‚æ•°ä¼˜å…ˆï¼Œå¦åˆ™ä½¿ç”¨args.pyä¸­çš„å‚æ•°
    args = argparse.Namespace()
    args.model_path = cmd_args.model_path
    args.model_type = cmd_args.model_type
    args.client_num = cmd_args.client_num
    args.key_matrix_dir = cmd_args.key_matrix_dir
    args.autoencoder_dir = cmd_args.autoencoder_dir
    args.finetune_epochs = cmd_args.finetune_epochs
    args.learning_rate = cmd_args.learning_rate if cmd_args.learning_rate is not None else base_args.lr
    args.batch_size = cmd_args.batch_size if cmd_args.batch_size is not None else base_args.batch_size
    args.optimizer = cmd_args.optimizer if cmd_args.optimizer is not None else base_args.optim
    args.enable_scaling = cmd_args.enable_scaling
    args.scaling_factor = cmd_args.scaling_factor
    args.dataset = cmd_args.dataset
    
    # ä½¿ç”¨key_matrix_utilsç”Ÿæˆæ­£ç¡®çš„å¯†é’¥çŸ©é˜µè·¯å¾„
    from utils.key_matrix_utils import get_key_matrix_path
    args.key_matrix_path = get_key_matrix_path(cmd_args.key_matrix_dir, cmd_args.model_type, cmd_args.client_num)
    
    # ä»args.pyè·å–å…¶ä»–å¿…è¦å‚æ•°
    args.data_root = base_args.data_root
    
    # è®¾ç½®è®¾å¤‡
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")

    # é…ç½®å‚æ•°
    model_path = args.model_path
    key_matrix_dir = args.key_matrix_path  # ä½¿ç”¨ç”Ÿæˆçš„å®Œæ•´è·¯å¾„
    autoencoder_dir = args.autoencoder_dir
    
    # å¾®è°ƒå‚æ•°
    finetune_epochs = args.finetune_epochs
    eval_interval = 1  # æ¯è½®éƒ½è¿›è¡ŒåŸºæœ¬è¯„ä¼°
    pcc_interval = 5  # PCCè®¡ç®—é—´éš”ï¼Œå¯ä»¥è°ƒæ•´ï¼ˆå»ºè®®5-20ä¹‹é—´ï¼Œå€¼è¶Šå¤§è®¡ç®—è¶Šå°‘ä½†ç›‘æ§è¶Šç²—ç³™ï¼‰
    learning_rate = args.learning_rate
    batch_size = args.batch_size
    optimizer_type = args.optimizer

    print(f"å¾®è°ƒæ”»å‡»å®éªŒå‚æ•°:")
    print(f"  - å¾®è°ƒè½®æ•°: {finetune_epochs}")
    print(f"  - åŸºæœ¬è¯„ä¼°: æ¯è½® | Î”PCCè¯„ä¼°: æ¯{pcc_interval}è½®")
    print(f"  - å­¦ä¹ ç‡: {learning_rate}")
    print(f"  - æ‰¹æ¬¡å¤§å°: {batch_size}")
    print(f"  - ä¼˜åŒ–å™¨: {optimizer_type}")
    print(f"  - æ¨¡å‹ç±»å‹: {args.model_type}")
    print(f"  - å®¢æˆ·ç«¯æ•°é‡: {args.client_num}")
    print(f"  - æ•°æ®é›†: {args.dataset}")
    print(f"  - å¯†é’¥çŸ©é˜µè·¯å¾„: {key_matrix_dir}")
    print(f"  - è‡ªç¼–ç å™¨è·¯å¾„: {autoencoder_dir}")
    print(f"  - æ°´å°ç¼©æ”¾: {args.enable_scaling}, ç¼©æ”¾å› å­: {args.scaling_factor}")
    print("-" * 60)

    # åŠ è½½æ•°æ®
    print("åŠ è½½æ•°æ®...")
    # ä½¿ç”¨å‘½ä»¤è¡Œå‚æ•°æŒ‡å®šçš„æ•°æ®é›†
    dataset = args.dataset
    dataset_type = dataset  # ç”¨äºæŸå¤±å‡½æ•°é€‰æ‹©
    
    # æ ¹æ®æ•°æ®é›†ç±»å‹åŠ è½½ç›¸åº”çš„æ•°æ®
    if dataset == 'cifar10':
        train_loader, test_loader = load_cifar10_data(batch_size=batch_size, data_root=args.data_root)
        mnist_test_loader = load_mnist_test_data(batch_size=128, data_dir=args.data_root)
    elif dataset == 'cifar100':
        print("ä½¿ç”¨CIFAR-100æ•°æ®é›†è¿›è¡Œå¾®è°ƒæ”»å‡»å®éªŒ")
        train_loader, test_loader = load_cifar100_data(batch_size=batch_size, data_root=args.data_root)
        dataset_type = 'cifar100'
        mnist_test_loader = load_mnist_test_data(batch_size=128, data_dir=args.data_root)
    elif dataset == 'chestmnist':
        train_set, test_set = load_chestmnist_data(data_root=args.data_root)
        train_loader = create_safe_dataloader(train_set, batch_size=batch_size, shuffle=True)
        test_loader = create_safe_dataloader(test_set, batch_size=batch_size, shuffle=False)
        mnist_test_loader = load_mnist_test_data(batch_size=128, data_dir=args.data_root)
    else:
        print(f"âŒ ä¸æ”¯æŒçš„æ•°æ®é›†: {dataset}")
        return

    # åŠ è½½ä¸»ä»»åŠ¡æ¨¡å‹
    print("åŠ è½½ä¸»ä»»åŠ¡æ¨¡å‹...")
    model = load_main_task_model(model_path, device)
    if model is None:
        print("âŒ ä¸»ä»»åŠ¡æ¨¡å‹åŠ è½½å¤±è´¥")
        return

    # ä¿å­˜åŸå§‹æ¨¡å‹çŠ¶æ€
    original_model_state = copy.deepcopy(model.state_dict())

    # åˆå§‹åŒ–æ°´å°é‡å»ºå™¨ï¼ˆä½¿ç”¨argsä¸­çš„ç»Ÿä¸€è®¾ç½®ï¼‰
    reconstructor = WatermarkReconstructor(
        key_matrix_dir, 
        autoencoder_dir, 
        enable_scaling=args.enable_scaling, 
        scaling_factor=args.scaling_factor
    )
    
    # é¢„è®¡ç®—å›ºå®šé˜ˆå€¼Ï„ï¼Œé¿å…é‡å¤è®¡ç®—
    print("é¢„è®¡ç®—å›ºå®šé˜ˆå€¼Ï„...")
    fixed_tau = None
    if reconstructor and original_model_state and mnist_test_loader:
        fixed_tau = calculate_fixed_tau(original_model_state, reconstructor, mnist_test_loader, device, perf_fail_ratio=0.05)
        if fixed_tau is None:
            print("âŒ æ— æ³•è®¡ç®—å›ºå®šé˜ˆå€¼ï¼Œå°†ä½¿ç”¨åŠ¨æ€é˜ˆå€¼")
        else:
            print(f"âœ“ å›ºå®šé˜ˆå€¼Ï„={fixed_tau:.6f}")

    print("å¼€å§‹å¾®è°ƒæ”»å‡»å®éªŒ...")
    print("=" * 80)
    
    
    # ç¬¬0è½®ï¼šæµ‹è¯•å¾®è°ƒå‰çš„æ°´å°æ£€æµ‹
    print("=== ç¬¬0è½®è¯„ä¼°ï¼ˆå¾®è°ƒå‰ï¼‰===")
    
    # å…ˆè¿›è¡ŒAUCè¯„ä¼°
    model.eval()
    test_loss, all_predictions, all_targets = 0.0, [], []
    
    # æ ¹æ®æ•°æ®é›†ç±»å‹é€‰æ‹©åˆé€‚çš„æŸå¤±å‡½æ•°
    if dataset_type == 'chestmnist':
        criterion = nn.BCEWithLogitsLoss()
        activation_fn = torch.sigmoid
    else:  # cifar10, cifar100, mnistç­‰å¤šåˆ†ç±»ä»»åŠ¡
        criterion = nn.CrossEntropyLoss()
        activation_fn = torch.softmax
    
    with torch.no_grad():
        for data, target in test_loader:
            data, target = data.to(device), target.to(device)
            output = model(data)
            
            if dataset_type == 'chestmnist':
                test_loss += criterion(output, target.float()).item()
                all_predictions.append(activation_fn(output).cpu().numpy())
            else:  # å¤šåˆ†ç±»ä»»åŠ¡
                test_loss += criterion(output, target.long()).item()
                all_predictions.append(activation_fn(output, dim=1).cpu().numpy())
            
            all_targets.append(target.cpu().numpy())

    avg_test_loss = test_loss / len(test_loader)
    all_predictions = np.concatenate(all_predictions, axis=0)
    all_targets = np.concatenate(all_targets, axis=0)

    # è®¡ç®—AUCå’Œå‡†ç¡®ç‡ï¼ˆæ ¹æ®æ•°æ®é›†ç±»å‹ï¼‰
    if dataset_type == 'chestmnist':
        # å¤šæ ‡ç­¾äºŒåˆ†ç±»ä»»åŠ¡
        try:
            from sklearn.metrics import roc_auc_score
            auc_scores = [
                roc_auc_score(all_targets[:, i], all_predictions[:, i])
                for i in range(all_targets.shape[1])
                if len(np.unique(all_targets[:, i])) > 1
            ]
            mean_auc = np.mean(auc_scores) if auc_scores else 0.0
        except ImportError:
            mean_auc = 0.0
        
        # è®¡ç®—å‡†ç¡®ç‡ï¼ˆå¤šæ ‡ç­¾ï¼‰
        pred_binary = (all_predictions > 0.5).astype(int)
        accuracy = np.mean((pred_binary == all_targets).astype(float))
    else:
        # å¤šåˆ†ç±»ä»»åŠ¡
        try:
            from sklearn.metrics import roc_auc_score
            # ä½¿ç”¨one-vs-restç­–ç•¥è®¡ç®—å¤šåˆ†ç±»AUC
            mean_auc = roc_auc_score(all_targets, all_predictions, multi_class='ovr', average='macro')
        except ImportError:
            mean_auc = 0.0
        
        # è®¡ç®—å‡†ç¡®ç‡ï¼ˆå¤šåˆ†ç±»ï¼‰
        pred_classes = np.argmax(all_predictions, axis=1)
        accuracy = np.mean((pred_classes == all_targets).astype(float))
    
    print(f"æµ‹è¯•æŸå¤±: {avg_test_loss:.4f} | AUC: {mean_auc:.4f} | å‡†ç¡®ç‡: {accuracy:.2%}")
    
    # æ ¹æ®æ•°æ®é›†ç±»å‹æ˜¾ç¤ºæŒ‡æ ‡é‡è¦æ€§
    if dataset_type == 'chestmnist':
        print(f"ğŸ“Š ChestMNISTå¤šæ ‡ç­¾ä»»åŠ¡ - AUCä¸ºä¸»è¦æŒ‡æ ‡ï¼Œå‡†ç¡®ç‡ä¸ºå‚è€ƒæŒ‡æ ‡")
    else:
        print(f"ğŸ“Š {dataset_type.upper()}å¤šåˆ†ç±»ä»»åŠ¡ - å‡†ç¡®ç‡ä¸ºä¸»è¦æŒ‡æ ‡ï¼ŒAUCä¸ºå‚è€ƒæŒ‡æ ‡")
    
    # ==================== æ°´å°æ£€æµ‹å®¹å¿åº¦è®¾ç½® ====================
    PERF_FAIL_RATIO = 0.3
    # =========================================================
    print(f"æ°´å°æ£€æµ‹å®¹å¿åº¦è®¾ç½®: {PERF_FAIL_RATIO}")
    
    # è¿›è¡ŒÎ”PCCè¯„ä¼°
    delta_pcc_result_0 = evaluate_delta_pcc(
        original_model_state, original_model_state, reconstructor,
        mnist_test_loader, device, perf_fail_ratio=PERF_FAIL_RATIO, fixed_tau=fixed_tau
    )
    
    # åˆ›å»ºç¬¬0è½®çš„ç»“æœè®°å½•
    initial_result = {
        'epoch': 0,
        'train_loss': 0.0,  # ç¬¬0è½®æ²¡æœ‰è®­ç»ƒ
        'test_loss': avg_test_loss,   # ç¬¬0è½®æµ‹è¯•æŸå¤±
        'test_auc': mean_auc,    # ç¬¬0è½®æµ‹è¯•AUC
        'test_accuracy': accuracy,  # ç¬¬0è½®æµ‹è¯•å‡†ç¡®ç‡
        'learning_rate': 0.0,  # ç¬¬0è½®æ²¡æœ‰å­¦ä¹ ç‡
    }
    
    # æ·»åŠ ç¬¬0è½®çš„Î”PCCå’Œä¾µæƒåˆ¤æ–­ä¿¡æ¯
    initial_result.update(format_delta_pcc_result(delta_pcc_result_0))

    # è¿›è¡Œå¾®è°ƒè®­ç»ƒ
    model_states, performance_metrics = finetune_model(
        model, train_loader, test_loader,
        epochs=finetune_epochs, lr=learning_rate,
        device=device, eval_interval=eval_interval, pcc_interval=pcc_interval,
        reconstructor=reconstructor, original_model_state=original_model_state,
        mnist_test_loader=mnist_test_loader, fixed_tau=fixed_tau,
        optimizer_type=optimizer_type, dataset_type=dataset_type
    )

    # å¾®è°ƒè®­ç»ƒå·²å®Œæˆï¼ŒÎ”PCCå’Œä¾µæƒåˆ¤æ–­å·²åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è¯„ä¼°
    print("\n" + "=" * 80)
    print("å¾®è°ƒæ”»å‡»å®éªŒå®Œæˆ")
    print("=" * 80)

    # å°†ç¬¬0è½®ç»“æœå’Œè®­ç»ƒç»“æœåˆå¹¶
    results = [initial_result] + performance_metrics

    # ä¿å­˜ç»“æœ
    print("\n" + "=" * 80)
    print("ä¿å­˜å®éªŒç»“æœ...")
    save_results(results, save_dir='./save/finetune_attack')

    # è¾“å‡ºæ€»ç»“
    print("\n" + "=" * 80)
    print("å¾®è°ƒæ”»å‡»å®éªŒæ€»ç»“")
    print("=" * 80)
    
    # æ ¹æ®æ•°æ®é›†ç±»å‹è°ƒæ•´æ˜¾ç¤ºæ ¼å¼
    if dataset_type == 'chestmnist':
        print(f"{'è½®æ¬¡':<4} {'è®­ç»ƒæŸå¤±':<10} {'æµ‹è¯•æŸå¤±':<10} {'æµ‹è¯•AUC':<10} {'æµ‹è¯•å‡†ç¡®ç‡%':<8} {'Î”PCC':<8} {'ä¾µæƒåˆ¤æ–­':<8}")
        print("-" * 80)
        
        for result in results:
            delta_pcc_str = f"{result['delta_pcc']:.4f}" if result['delta_pcc'] is not None else "N/A"
            infringement_str = "æ˜¯" if result['is_infringement'] else "å¦" if result['is_infringement'] is not None else "N/A"
            
            print(f"{result['epoch']:>3}  "
                  f"{result['train_loss']:>8.4f}  "
                  f"{result['test_loss']:>8.4f}  "
                  f"{result['test_auc']:>8.4f}  "  # AUCæ›´å®½æ˜¾ç¤º
                  f"{result['test_accuracy']:>6.2%}  "  # å‡†ç¡®ç‡ç¨çª„
                  f"{delta_pcc_str:>6}  "
                  f"{infringement_str:>6}")
    else:
        # CIFAR10ç­‰å¤šåˆ†ç±»ä»»åŠ¡
        print(f"{'è½®æ¬¡':<4} {'è®­ç»ƒæŸå¤±':<10} {'æµ‹è¯•æŸå¤±':<10} {'æµ‹è¯•AUC':<8} {'æµ‹è¯•å‡†ç¡®ç‡%':<10} {'Î”PCC':<8} {'ä¾µæƒåˆ¤æ–­':<8}")
        print("-" * 80)
        
        for result in results:
            delta_pcc_str = f"{result['delta_pcc']:.4f}" if result['delta_pcc'] is not None else "N/A"
            infringement_str = "æ˜¯" if result['is_infringement'] else "å¦" if result['is_infringement'] is not None else "N/A"
            
            print(f"{result['epoch']:>3}  "
                  f"{result['train_loss']:>8.4f}  "
                  f"{result['test_loss']:>8.4f}  "
                  f"{result['test_auc']:>6.4f}  "
                  f"{result['test_accuracy']:>8.2%}  "
                  f"{delta_pcc_str:>6}  "
                  f"{infringement_str:>6}")

    # åˆ†æè¶‹åŠ¿
    print("\nè¶‹åŠ¿åˆ†æ:")
    if len(results) > 1:
        initial_auc = results[0]['test_auc']
        final_auc = results[-1]['test_auc']
        auc_change = final_auc - initial_auc

        initial_acc = results[0]['test_accuracy']
        final_acc = results[-1]['test_accuracy']
        acc_change = final_acc - initial_acc

        if dataset_type == 'chestmnist':
            print(f"æµ‹è¯•AUCå˜åŒ–: {initial_auc:.4f} â†’ {final_auc:.4f} (å˜åŒ–: {auc_change:+.4f}) [ä¸»è¦æŒ‡æ ‡]")
            print(f"æµ‹è¯•å‡†ç¡®ç‡å˜åŒ–: {initial_acc:.2%} â†’ {final_acc:.2%} (å˜åŒ–: {acc_change:+.2%}) [å‚è€ƒæŒ‡æ ‡]")
        else:
            print(f"æµ‹è¯•AUCå˜åŒ–: {initial_auc:.4f} â†’ {final_auc:.4f} (å˜åŒ–: {auc_change:+.4f}) [å‚è€ƒæŒ‡æ ‡]")
            print(f"æµ‹è¯•å‡†ç¡®ç‡å˜åŒ–: {initial_acc:.2%} â†’ {final_acc:.2%} (å˜åŒ–: {acc_change:+.2%}) [ä¸»è¦æŒ‡æ ‡]")
        
        # åˆ†æÎ”PCCè¶‹åŠ¿
        delta_pcc_values = [r['delta_pcc'] for r in results if r['delta_pcc'] is not None]
        if len(delta_pcc_values) > 1:
            initial_delta_pcc = delta_pcc_values[0]
            final_delta_pcc = delta_pcc_values[-1]
            delta_pcc_change = final_delta_pcc - initial_delta_pcc
            print(f"Î”PCCå˜åŒ–: {initial_delta_pcc:.4f} â†’ {final_delta_pcc:.4f} (å˜åŒ–: {delta_pcc_change:+.4f})")
        
        # åˆ†æä¾µæƒåˆ¤æ–­
        infringement_count = sum(1 for r in results if r['is_infringement'] is True)
        total_evaluations = sum(1 for r in results if r['is_infringement'] is not None)
        if total_evaluations > 0:
            infringement_rate = infringement_count / total_evaluations
            print(f"ä¾µæƒåˆ¤æ–­: {infringement_count}/{total_evaluations} è½®è¢«åˆ¤å®šä¸ºä¾µæƒ ({infringement_rate:.1%})")

    print("\nå¾®è°ƒæ”»å‡»å®éªŒå®Œæˆï¼")


if __name__ == '__main__':
    main()
