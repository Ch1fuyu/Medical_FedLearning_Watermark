import copy
import os
import sys
import time
from datetime import datetime
import logging
import gc

import numpy as np
import torch
from torch.utils.data import DataLoader
from tqdm import tqdm

from config.globals import set_seed
from models.alexnet import AlexNet
from models.resnet import resnet18
from utils.args import parser_args
from utils.base import Experiment
from utils.dataset import get_data, DatasetSplit
from utils.trainer_private import TrainerPrivate, TesterPrivate
from utils.trainer_private_enhanced import TrainerPrivateEnhanced
from utils.autoencoder_finetuner import AutoencoderFinetuner, finetune_autoencoder_encoder
import pandas as pd

set_seed()

# é…ç½® logging
args = parser_args()
log_file_name = args.log_file
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    datefmt='%Y-%m-%d %H-%M-%S',  # æ—¥æœŸæ ¼å¼
    handlers=[
        logging.StreamHandler(sys.stdout),  # è¾“å‡ºåˆ°æ§åˆ¶å°
        logging.FileHandler(log_file_name, mode='a', encoding='utf-8')  # è¿½åŠ æ¨¡å¼
    ]
)


class FederatedLearningOnChestMNIST(Experiment):
    def __init__(self, args):
        super().__init__(args)
        
        self.random_positions = None
        self.args = args
        self.dp = args.dp
        self.sigma = args.sigma
        self.key_matrix_dir = getattr(args, 'key_matrix_dir', './save/key_matrix')
        
        logging.info('--------------------------------Start--------------------------------------')
        logging.info(args)
        logging.info('==> Preparing data...')
        
        # æ•°æ®é›†é…ç½®
        self.num_classes = args.num_classes
        self.in_channels = args.in_channels
            
        # ç¡®ä¿æ•°æ®æ ¹ç›®å½•å­˜åœ¨ä»¥é¿å…Windowsæƒé™/è·¯å¾„é—®é¢˜
        os.makedirs(self.data_root, exist_ok=True)

        self.train_set, self.test_set, self.dict_users = get_data(dataset_name=self.dataset,
                                                                  data_root=self.data_root,
                                                                  iid=self.iid,
                                                                  client_num=self.client_num,
                                                                  )
        logging.info('==> Training model...')
        self.logs = {'best_train_acc': -np.inf, 'best_train_loss': -np.inf,
                     'val_acc': [], 'val_loss': [],
                     'best_model_acc': -np.inf, 'best_model_loss': -np.inf,
                     'best_model_auc': -np.inf,
                     'best_model': [],
                     'local_loss': [],
                     # ç‹¬ç«‹è·Ÿè¸ªå†å²æœ€é«˜æŒ‡æ ‡
                     'highest_acc_ever': -np.inf,     # å†å²æœ€é«˜å‡†ç¡®ç‡ï¼ˆçº¯å‡†ç¡®ç‡ï¼‰
                     'highest_auc_ever': -np.inf,     # å†å²æœ€é«˜AUCï¼ˆçº¯AUCï¼‰
                     'acc_when_highest_auc': -np.inf, # è¾¾åˆ°å†å²æœ€é«˜AUCæ—¶çš„å‡†ç¡®ç‡
                     'auc_when_highest_acc': -np.inf, # è¾¾åˆ°å†å²æœ€é«˜å‡†ç¡®ç‡æ—¶çš„AUC
                     }

        self.construct_model()
        self.w_t = copy.deepcopy(self.model.state_dict())

        # æ ¹æ®å‚æ•°é€‰æ‹©æ°´å°æ¨¡å¼
        self.random_positions = {}
        # è®¾ç½®å¯†é’¥çŸ©é˜µç›®å½•
        self.args.key_matrix_dir = self.key_matrix_dir
        self.args.use_key_matrix = True
        
        # æ ¹æ®watermark_modeå‚æ•°é€‰æ‹©trainer
        if self.args.watermark_mode == 'enhanced':
            logging.info('==> ä½¿ç”¨å¢å¼ºæ°´å°ç³»ç»Ÿï¼ˆå¯†é’¥çŸ©é˜µ + è‡ªç¼–ç å™¨ï¼‰')
            self.trainer = TrainerPrivateEnhanced(self.model, self.device, self.dp, self.sigma, self.args)
            
            # åˆå§‹åŒ–è‡ªç¼–ç å™¨å¾®è°ƒå™¨
            self.autoencoder_finetuner = AutoencoderFinetuner(self.device)
            logging.info('==> è‡ªç¼–ç å™¨å¾®è°ƒå™¨å·²åˆå§‹åŒ–')
        else:
            logging.info('==> ä½¿ç”¨æ™®é€šæ°´å°ç³»ç»Ÿ')
            self.trainer = TrainerPrivate(self.model, self.device, self.dp, self.sigma, self.random_positions, self.args)
            self.autoencoder_finetuner = None
            
        self.tester = TesterPrivate(self.model, self.device, args=self.args)

    def construct_model(self):
        if self.model_name == 'resnet':
            model = resnet18(num_classes=self.num_classes, in_channels=self.in_channels, input_size=self.args.input_size)
        else:
            dropout_rate = getattr(self.args, 'dropout_rate', 0.5)
            model = AlexNet(self.in_channels, self.num_classes, input_size=self.args.input_size, dropout_rate=dropout_rate)
        self.model = model.to(self.device)

    def _cleanup_memory(self):
        """æ¸…ç†å†…å­˜å’ŒGPUç¼“å­˜"""
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        gc.collect()

    def _optimize_model_storage(self, model_state):
        """ä¼˜åŒ–æ¨¡å‹çŠ¶æ€å­˜å‚¨ï¼Œå‡å°‘å†…å­˜å ç”¨"""
        # å°†æ¨¡å‹çŠ¶æ€ç§»åˆ°CPUï¼Œä½¿ç”¨detach()é¿å…æ¢¯åº¦è¿½è¸ª
        optimized_state = {}
        for key, value in model_state.items():
            optimized_state[key] = value.detach().cpu()
        return optimized_state

    def training(self):
        start = time.time()
        # these dataloader would only be used in calculating accuracy and loss
        train_ldr = DataLoader(self.train_set, batch_size=self.batch_size, shuffle=False, num_workers=0, pin_memory=False)
        val_ldr = DataLoader(self.test_set, batch_size=self.batch_size * 2, shuffle=False, num_workers=0, pin_memory=False)

        local_train_loader = []

        for i in range(self.client_num):
            local_train_ldr = DataLoader(DatasetSplit(self.train_set, self.dict_users[i]),
                                         batch_size=self.batch_size,
                                         shuffle=True, num_workers=0, pin_memory=False)
            local_train_loader.append(local_train_ldr)

        idxs_users = []

        # Early Stopping é…ç½®
        patience = 10
        early_stop_counter = 0
        best_val_acc = -np.inf
        best_val_auc = -np.inf

        # å†³å®šæ¨¡å‹é€‰æ‹©ä¾æ®ï¼šChestMNIST æŒ‰ AUCï¼Œå…¶ä»–ï¼ˆå¦‚ CIFAR-10/100ï¼‰æŒ‰å‡†ç¡®ç‡
        dataset_key = (self.dataset or '').lower()
        select_by_auc = (dataset_key == 'chestmnist')

        # ç»Ÿè®¡è®°å½•
        stats_rows = []

        for epoch in range(self.epochs): # å‡åŒ€é‡‡æ ·ï¼Œfrac é»˜è®¤ä¸º 1ï¼Œå³æ¯è½®ä¸­å…¨ä½“å®¢æˆ·ç«¯å‚ä¸è®­ç»ƒ
            # å‡åŒ€é‡‡æ ·
            self.m = max(int(self.frac * self.client_num), 1)
            idxs_users = np.random.choice(range(self.client_num), self.m, replace=False)

            local_ws, local_losses = [], []

            logging.info('Epoch: %d / %d' % (epoch + 1, self.epochs))
            
            # è‡ªç¼–ç å™¨å¾®è°ƒï¼šæ¯ä¸€è½®è”é‚¦è®­ç»ƒå¼€å§‹æ—¶éƒ½æ‰§è¡Œ
            if self.autoencoder_finetuner is not None and hasattr(self.trainer, 'autoencoder'):
                # ä»…é¦–è½®å’Œæ¯10è½®æ‰“å°æ—¥å¿—
                if (epoch + 1) % 10 == 0 or epoch == 0:
                    logging.info(f'==> è½®æ¬¡{epoch+1}: å¾®è°ƒè‡ªç¼–ç å™¨...')
                try:
                    # å¾®è°ƒè‡ªç¼–ç å™¨çš„ç¼–ç å™¨éƒ¨åˆ†
                    success = self.autoencoder_finetuner.finetune_encoder(
                        autoencoder=self.trainer.autoencoder,
                        epochs=1,  # æ¯è½®åªå¾®è°ƒ1ä¸ªepoch
                        lr=0.005,
                        batch_size=128  # å‡å°‘æ‰¹å¤„ç†å¤§å°ä»¥é™ä½å†…å­˜ä½¿ç”¨
                    )
                    
                    if not success:
                        logging.error(f'âŒ ç¬¬{epoch+1}è½®è‡ªç¼–ç å™¨å¾®è°ƒå¤±è´¥')
                        logging.error('è‡ªç¼–ç å™¨å¾®è°ƒå¤±è´¥ï¼Œç¨‹åºç»ˆæ­¢')
                        import sys
                        sys.exit(1)
                        
                except Exception as e:
                    logging.error(f'âŒ ç¬¬{epoch+1}è½®è‡ªç¼–ç å™¨å¾®è°ƒè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}')
                    logging.error('è‡ªç¼–ç å™¨å¾®è°ƒå¤±è´¥ï¼Œç¨‹åºç»ˆæ­¢')
                    import sys
                    sys.exit(1)
            for i, idx in enumerate(tqdm(idxs_users, desc='Progress: %d / %d' % (epoch + 1, self.epochs))):
                self.model.load_state_dict(self.w_t)

                # ç»Ÿä¸€è°ƒç”¨ï¼šå§‹ç»ˆä¼ å…¥ current_epoch/total_epochsï¼›
                # æ™®é€š Trainer ä¼šé€šè¿‡ **kwargs å¿½ç•¥
                local_w, local_loss, local_acc = self.trainer.local_update(
                    dataloader=local_train_loader[idx], 
                    local_ep=self.local_ep, 
                    lr=self.lr, 
                    client_id=idx,
                    current_epoch=epoch,
                    total_epochs=self.epochs
                )

                local_ws.append(copy.deepcopy(local_w))
                local_losses.append(local_loss)
                
                # æ¸…ç†ä¸´æ—¶å˜é‡ï¼Œé‡Šæ”¾å†…å­˜
                del local_w, local_loss, local_acc

            # å¯é€‰ï¼šå¦‚éœ€è°ƒåº¦å¯åœ¨æ­¤å¤„åŠ å…¥è‡ªå®šä¹‰é€»è¾‘

            # è®¡ç®—å‚ä¸è®­ç»ƒçš„å®¢æˆ·ç«¯çš„æƒé‡ï¼ˆç›¸å¯¹äºæ€»æ•°æ®é›†ï¼‰
            client_weights = []
            for idx in idxs_users:
                client_weight = len(DatasetSplit(self.train_set, self.dict_users[idx])) / len(self.train_set)
                client_weights.append(client_weight)

            # æ›´æ–°å…¨å±€æ¨¡å‹æƒé‡
            self._fed_avg(local_ws, client_weights, idxs_users)
            
            # åŠ è½½èšåˆåçš„æƒé‡å¹¶ç¡®ä¿æ¨¡å‹å¤„äºæ­£ç¡®çŠ¶æ€
            self.model.load_state_dict(self.w_t)
            self.model.train()  # ç¡®ä¿æ¨¡å‹å¤„äºè®­ç»ƒçŠ¶æ€
            
            # æ¸…ç†ä¼˜åŒ–å™¨çŠ¶æ€ï¼Œé¿å…æ¢¯åº¦ç´¯ç§¯
            if hasattr(self.trainer, 'optimizer') and self.trainer.optimizer is not None:
                self.trainer.optimizer.zero_grad()
            
            # æ¢¯åº¦ç»Ÿè®¡ï¼ˆä»…æ¯10è½®æ‰“å°ä¸€æ¬¡ï¼‰
            if epoch >= 0 and hasattr(self.trainer, 'multi_loss') and (epoch + 1) % 10 == 0:
                try:
                    stats = self.trainer.get_gradient_stats()
                    if stats:
                        logging.info(f'è½®æ¬¡{epoch+1}æ¢¯åº¦ç»Ÿè®¡ - GM:{stats.get("prevGM", 0):.6f} GH:{stats.get("prevGH", 0):.6f} Ratio:{stats.get("prevRatio", 1):.6f}')
                except Exception as e:
                    pass  # é™é»˜å¤„ç†é”™è¯¯


            if (epoch + 1) == self.epochs or (epoch + 1) % 1 == 0:
                train_metrics = self.trainer.test(train_ldr)
                val_metrics = self.trainer.test(val_ldr)

                # (loss, acc_label, auc, acc_sample)
                loss_train_mean, acc_train_label_mean, auc_train, acc_train_sample_mean = train_metrics
                loss_val_mean, acc_val_label_mean, auc_val, acc_val_sample_mean = val_metrics

                self.logs['val_acc'].append(acc_val_label_mean)
                self.logs['val_loss'].append(loss_val_mean)
                self.logs['local_loss'].append(np.mean(local_losses))

                # æ›´æ–°å†å²æœ€é«˜å€¼è·Ÿè¸ª
                if self.logs['highest_acc_ever'] < acc_val_label_mean:
                    self.logs['highest_acc_ever'] = acc_val_label_mean
                    self.logs['auc_when_highest_acc'] = auc_val
                    
                if self.logs['highest_auc_ever'] < auc_val:
                    self.logs['highest_auc_ever'] = auc_val
                    self.logs['acc_when_highest_auc'] = acc_val_label_mean

                # æ¨¡å‹é€‰æ‹©æ ‡å‡†ï¼šChestMNIST æŒ‰ AUCï¼›å¦åˆ™æŒ‰å‡†ç¡®ç‡
                if select_by_auc:
                    if self.logs['best_model_auc'] < auc_val:
                        self.logs['best_model_acc'] = acc_val_label_mean
                        self.logs['best_model_loss'] = loss_val_mean
                        self.logs['best_model_auc'] = auc_val
                        # ä¼˜åŒ–æ¨¡å‹å­˜å‚¨ï¼Œå‡å°‘å†…å­˜å ç”¨
                        optimized_state = self._optimize_model_storage(self.model.state_dict())
                        self.logs['best_model'] = [optimized_state]
                        logging.info(f'ğŸŒŸ æœ€ä½³æ¨¡å‹å·²ä¿å­˜! AUCâ†‘{auc_val:.4f}')
                else:
                    if self.logs['best_model_acc'] < acc_val_label_mean:
                        self.logs['best_model_acc'] = acc_val_label_mean
                        self.logs['best_model_loss'] = loss_val_mean
                        self.logs['best_model_auc'] = auc_val
                        optimized_state = self._optimize_model_storage(self.model.state_dict())
                        self.logs['best_model'] = [optimized_state]
                        logging.info(f'ğŸŒŸ æœ€ä½³æ¨¡å‹å·²ä¿å­˜! ACCâ†‘{acc_val_label_mean:.4f}')

                if self.logs['best_train_acc'] < acc_train_label_mean:
                    self.logs['best_train_acc'] = acc_train_label_mean
                    self.logs['best_train_loss'] = loss_train_mean

                # åˆå¹¶è®­ç»ƒå’ŒéªŒè¯æŒ‡æ ‡åˆ°ä¸€è¡Œ
                logging.info(
                    f"è½®æ¬¡{epoch+1} | Train Loss:{loss_train_mean:.4f} Acc:{acc_train_label_mean:.4f} AUC:{auc_train:.4f} | "
                    f"Val Loss:{loss_val_mean:.4f} Acc:{acc_val_label_mean:.4f} AUC:{auc_val:.4f} | "
                    f"Best Acc:{self.logs['highest_acc_ever']:.4f} Best AUC:{self.logs['highest_auc_ever']:.4f}")
                
                # MultiLossç»Ÿè®¡ä¿¡æ¯ï¼ˆä»…æ¯10è½®æ‰“å°ï¼‰
                if hasattr(self.trainer, 'multi_loss') and (epoch + 1) % 10 == 0:
                    stats = self.trainer.multi_loss.get_stats()
                    logging.info(f"MultiLoss - GM:{stats['prevGM']:.6f} GH:{stats['prevGH']:.6f} Ratio:{stats['prevRatio']:.6f}")
                
                # è®°å½•æœ¬è½®ç»Ÿè®¡æ•°æ®
                stats_row = {
                    'round': epoch + 1,
                    'lr': self.lr,
                    'train_loss': float(loss_train_mean),
                    'val_loss': float(loss_val_mean),
                    'train_acc_label': float(acc_train_label_mean),
                    'train_auc': float(auc_train),
                    'val_acc_label': float(acc_val_label_mean),
                    'val_auc': float(auc_val),
                    'best_val_acc_so_far': float(self.logs['highest_acc_ever']),
                    'best_val_auc_so_far': float(self.logs['highest_auc_ever']),
                    'train_acc_sample': float(acc_train_sample_mean),
                    'val_acc_sample': float(acc_val_sample_mean),
                }
                
                # Early Stoppingï¼šChestMNIST åŸºäº AUCï¼Œå…¶å®ƒæ•°æ®é›†åŸºäºå‡†ç¡®ç‡
                if select_by_auc:
                    if auc_val > best_val_auc:
                        best_val_auc = auc_val
                        early_stop_counter = 0
                    else:
                        early_stop_counter += 1
                        if early_stop_counter >= patience:
                            logging.info(f'Early stopping triggered at epoch {epoch + 1}. Best Val AUC: {best_val_auc:.4f}')
                            break
                else:
                    if acc_val_label_mean > best_val_acc:
                        best_val_acc = acc_val_label_mean
                        early_stop_counter = 0
                    else:
                        early_stop_counter += 1
                        if early_stop_counter >= patience:
                            logging.info(f'Early stopping triggered at epoch {epoch + 1}. Best Val ACC: {best_val_acc:.4f}')
                            break
                
                # æ¯è½®è®­ç»ƒåæ¸…ç†å†…å­˜
                self._cleanup_memory()
                
                # æ¸…ç†ä¸´æ—¶å˜é‡
                del train_metrics, val_metrics
                del loss_train_mean, acc_train_label_mean, auc_train, acc_train_sample_mean
                del loss_val_mean, acc_val_label_mean, auc_val, acc_val_sample_mean
                
                # æ·»åŠ è‡ªç¼–ç å™¨å¾®è°ƒç»Ÿè®¡ä¿¡æ¯
                if self.autoencoder_finetuner is not None and hasattr(self.trainer, 'autoencoder'):
                    try:
                        # è·å–å½“å‰è‡ªç¼–ç å™¨æ€§èƒ½
                        current_performance = self.autoencoder_finetuner.evaluate_encoder_performance(
                            self.trainer.autoencoder, 
                            test_samples=500  # ä½¿ç”¨è¾ƒå°‘æ ·æœ¬è¿›è¡Œå¿«é€Ÿè¯„ä¼°
                        )
                        stats_row['autoencoder_performance'] = float(current_performance)
                        # è‡ªç¼–ç å™¨æ€§èƒ½ï¼ˆä»…æ¯20è½®æ˜¾ç¤ºï¼‰
                        if (epoch + 1) % 20 == 0:
                            logging.info(f'ğŸ“Š è‡ªç¼–ç å™¨æ€§èƒ½: {current_performance:.6f}')
                    except Exception as e:
                        stats_row['autoencoder_performance'] = float('inf')
                        logging.warning(f'âš ï¸ æ— æ³•è¯„ä¼°è‡ªç¼–ç å™¨æ€§èƒ½: {e}')
                else:
                    stats_row['autoencoder_performance'] = None
                
                # æ·»åŠ å¢å¼ºæ°´å°ç³»ç»Ÿç»Ÿè®¡ä¿¡æ¯
                if hasattr(self.trainer, 'multi_loss'):
                    multi_loss_stats = self.trainer.multi_loss.get_stats()
                    stats_row.update({
                        'prevGM': float(multi_loss_stats['prevGM']),
                        'prevGH': float(multi_loss_stats['prevGH']),
                        'prevRatio': float(multi_loss_stats['prevRatio']),
                        'current_grad_M': float(multi_loss_stats['current_grad_M']),
                        'current_grad_H': float(multi_loss_stats['current_grad_H']),
                        'current_var_M': float(multi_loss_stats['current_var_M']),
                        'current_var_H': float(multi_loss_stats['current_var_H']),
                    })
                
                stats_rows.append(stats_row)

        logging.info('='*60 + ' è®­ç»ƒç»“æœ ' + '='*60)
        logging.info(
            f'æœ€ä½³æ¨¡å‹ | Loss:{self.logs["best_model_loss"]:.4f} Acc:{self.logs["best_model_acc"]:.4f} AUC:{self.logs["best_model_auc"]:.4f}')
        logging.info(
            f'å†å²æœ€é«˜ | Acc:{self.logs["highest_acc_ever"]:.4f}(AUC:{self.logs["auc_when_highest_acc"]:.4f}) | '
            f'AUC:{self.logs["highest_auc_ever"]:.4f}(Acc:{self.logs["acc_when_highest_auc"]:.4f})')
        end = time.time()
        logging.info(f'è®­ç»ƒè€—æ—¶: {(end - start) / 60:.1f} åˆ†é’Ÿ')
        
        # æœ€ç»ˆå†…å­˜æ¸…ç†
        self._cleanup_memory()
        logging.info('ğŸ§¹ è®­ç»ƒå®Œæˆï¼Œå·²æ¸…ç†å†…å­˜ç¼“å­˜')

        # å¯¼å‡ºExcel
        try:
            os.makedirs(self.args.save_excel_dir, exist_ok=True)
            # åŸºç¡€åˆ— + å¢å¼ºæ°´å°ç³»ç»Ÿç»Ÿè®¡åˆ— + è‡ªç¼–ç å™¨æ€§èƒ½åˆ—
            columns = ['round', 'lr', 'train_loss', 'val_loss', 'train_acc_label', 'train_auc', 
                     'val_acc_label', 'val_auc', 'best_val_acc_so_far', 'best_val_auc_so_far', 
                     'train_acc_sample', 'val_acc_sample', 'autoencoder_performance',
                     'prevGM', 'prevGH', 'prevRatio', 
                     'current_grad_M', 'current_grad_H', 'current_var_M', 'current_var_H']
            df = pd.DataFrame(stats_rows, columns=columns)
            now = datetime.now().strftime('%Y%m%d%H%M%S')
            excel_path = f'{self.args.save_excel_dir}/metrics_{self.model_name}_{self.dataset}_{now}.xlsx'
            df.to_excel(excel_path, index=False, engine='openpyxl')
            logging.info(f'Excel metrics saved to: {excel_path}')
        except Exception as e:
            logging.warning(f'Failed to export Excel metrics: {e}')

        # è¿”å›ç”¨äºæ¨¡å‹é€‰æ‹©çš„æŒ‡æ ‡ä¸å…¶åç§°ï¼Œä¾¿äºä¸Šå±‚å‘½åä¿å­˜
        best_metric_value = self.logs['best_model_auc'] if select_by_auc else self.logs['best_model_acc']
        best_metric_name = 'auc' if select_by_auc else 'acc'
        return self.logs, best_metric_value, best_metric_name


    def _fed_avg(self, local_ws, client_weights, idxs_users):
        """è”é‚¦å¹³å‡ç®—æ³•ï¼ŒFedAvg with exclusive watermark aggregation"""
        # è®¡ç®—å‚ä¸è®­ç»ƒçš„å®¢æˆ·ç«¯æƒé‡æ€»å’Œ
        total_weight = sum(client_weights)
        
        # å½’ä¸€åŒ–æƒé‡ï¼Œç¡®ä¿æƒé‡å’Œä¸º1
        normalized_weights = [w / total_weight for w in client_weights]
        
        # éªŒè¯æƒé‡å’Œæ˜¯å¦ä¸º1
        weight_sum = sum(normalized_weights)
        if abs(weight_sum - 1.0) > 1e-6:
            logging.warning(f"Weight sum is {weight_sum:.6f}, not 1.0. Normalizing...")
            normalized_weights = [w / weight_sum for w in normalized_weights]
        
        # åˆå§‹åŒ–å¹³å‡æƒé‡
        w_avg = {}
        for k in local_ws[0].keys():
            w_avg[k] = local_ws[0][k].clone() * normalized_weights[0]

        # ç´¯åŠ å…¶ä»–å®¢æˆ·ç«¯çš„æƒé‡
        for i in range(1, len(local_ws)):
            for k in w_avg.keys():
                w_avg[k] += local_ws[i][k] * normalized_weights[i]

        # æ°´å°èšåˆï¼šä½¿ç”¨å¯†é’¥çŸ©é˜µçš„ç‹¬å å¼èšåˆ
        self._watermark_aggregation(local_ws, idxs_users, w_avg)

        # æ›´æ–°å…¨å±€æ¨¡å‹æƒé‡
        for k in w_avg.keys():
            self.w_t[k] = w_avg[k]

    def _watermark_aggregation(self, local_ws, idxs_users, w_avg):
        """
        æ°´å°èšåˆï¼šä½¿ç”¨å¯†é’¥çŸ©é˜µçš„ç‹¬å å¼èšåˆ
        
        Args:
            local_ws: æœ¬åœ°æ¨¡å‹æƒé‡åˆ—è¡¨
            idxs_users: å‚ä¸è®­ç»ƒçš„å®¢æˆ·ç«¯IDåˆ—è¡¨
            w_avg: å¹³å‡èšåˆåçš„æƒé‡å­—å…¸
        """
        try:
            from utils.key_matrix_utils import KeyMatrixManager
            
            # åŠ è½½å¯†é’¥çŸ©é˜µç®¡ç†å™¨
            key_manager = KeyMatrixManager(self.args.key_matrix_path, args=self.args)
            
            # å¯¹æ¯ä¸ªå®¢æˆ·ç«¯çš„æ°´å°ä½ç½®è¿›è¡Œç‹¬å å¼èšåˆ
            for i, client_id in enumerate(idxs_users):
                try:
                    # è·å–è¯¥å®¢æˆ·ç«¯çš„æ°´å°ä½ç½®
                    positions = key_manager.load_positions(client_id)
                    
                    # å¯¹è¯¥å®¢æˆ·ç«¯çš„æ°´å°ä½ç½®ä½¿ç”¨ç‹¬å å¼èšåˆ
                    for param_name, param_idx in positions:
                        if param_name in local_ws[i] and param_name in w_avg:
                            # è·å–å‚æ•°å¼ é‡
                            local_param = local_ws[i][param_name]
                            avg_param = w_avg[param_name]
                            
                            # ç¡®ä¿å‚æ•°å½¢çŠ¶ä¸€è‡´
                            if local_param.shape == avg_param.shape:
                                # å°†å±€éƒ¨ç´¢å¼•è½¬æ¢ä¸ºæ‰å¹³åŒ–ç´¢å¼•
                                param_flat = avg_param.view(-1)
                                local_flat = local_param.view(-1)
                                
                                # ä½¿ç”¨å±€éƒ¨ç´¢å¼•ç›´æ¥æ›¿æ¢
                                if param_idx < param_flat.numel():
                                    param_flat[param_idx] = local_flat[param_idx]
                                    
                except Exception as e:
                    logging.warning(f"Failed to apply watermark aggregation for client {client_id}: {e}")
                    continue
                    
        except Exception as e:
            logging.warning(f"Failed to load key matrix manager for watermark aggregation: {e}")

def main(args):
    logs = {'net_info': None,
            'arguments': {
                'frac': args.frac,
                'local_ep': args.local_ep,
                'local_bs': args.batch_size,
                'lr_inner': args.lr,
                'iid': args.iid,
                'wd': args.wd,
                'optim': args.optim,
                'model_name': args.model_name,
                'dataset': args.dataset,
                'log_interval': args.log_interval,
                'num_classes': args.num_classes,
                'epochs': args.epochs,
                'client_num': args.client_num,
                'console_log': os.path.basename(log_file_name),
            }
            }
    fl = FederatedLearningOnChestMNIST(args)
    logg, best_metric_value, best_metric_name = fl.training()
    logs['net_info'] = logg
    # å…¼å®¹ï¼šæ€»æ˜¯è®°å½• test_aucï¼›å½“æŒ‡æ ‡ä¸º acc æ—¶ä¹Ÿé¢å¤–è®°å½•
    logs['test_auc'] = {'value': logg.get('best_model_auc', best_metric_value if best_metric_name == 'auc' else 0.0)}
    if best_metric_name == 'acc':
        logs['test_acc'] = {'value': best_metric_value}
    logs['bp_local'] = {'value': False}

    save_dir = os.path.join(args.save_model_dir, args.model_name, args.dataset)
    os.makedirs(save_dir, exist_ok=True)

    now = datetime.now()
    formatted_now = now.strftime("%Y%m%d%H%M")
    # æ„å»ºæ–‡ä»¶å
    enhanced = "_enhanced" if args.watermark_mode == 'enhanced' else ""

    # æ ¹æ®é€‰æ‹©æŒ‡æ ‡å‘½åæ–‡ä»¶
    watermark_suffix = f"wm_{args.watermark_mode}" if hasattr(args, 'watermark_mode') and args.watermark_mode else "wm_basic"
    file_name = '{}_Dp_{}_iid_{}_{}_ep_{}_le_{}_cn_{}_fra_{:.4f}_{}_{{:.4f}}{}.pkl'.format(
        formatted_now, args.sigma, args.iid, watermark_suffix,
        args.epochs, args.local_ep, args.client_num, args.frac, best_metric_name, enhanced
    )
    file_name = file_name.format(best_metric_value)
    torch.save(logs, os.path.join(save_dir, file_name))
    logging.info(f"è®­ç»ƒæ—¥å¿—å·²ä¿å­˜: {file_name}")
    logging.info('-------------------------------Finish--------------------------------------')

    return

if __name__ == '__main__':
    os.environ["CUDA_DEVICE_ORDER"] = "PCI_BUS_ID"
    os.environ["CUDA_VISIBLE_DEVICES"] = args.gpu
    main(args)