import copy
import os
import sys
import time
from datetime import datetime
import logging
import gc

import numpy as np
import torch
from torch.utils.data import DataLoader
from tqdm import tqdm

from config.globals import set_seed
from models.alexnet import AlexNet
from models.resnet import resnet18
from utils.args import parser_args
from utils.base import Experiment
from utils.dataset import get_data, DatasetSplit
from utils.trainer_private import TrainerPrivate, TesterPrivate
from utils.trainer_private_enhanced import TrainerPrivateEnhanced
from utils.autoencoder_finetuner import AutoencoderFinetuner, finetune_autoencoder_encoder
import pandas as pd

set_seed()

# é…ç½® logging
args = parser_args()
log_file_name = args.log_file
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    datefmt='%Y-%m-%d %H-%M-%S',  # æ—¥æœŸæ ¼å¼
    handlers=[
        logging.StreamHandler(sys.stdout),  # è¾“å‡ºåˆ°æ§åˆ¶å°
        logging.FileHandler(log_file_name, mode='a', encoding='utf-8')  # è¿½åŠ æ¨¡å¼
    ]
)


class FederatedLearningOnChestMNIST(Experiment):
    def __init__(self, args):
        super().__init__(args)
        
        self.random_positions = None
        self.args = args
        self.dp = args.dp
        self.sigma = args.sigma
        self.key_matrix_dir = getattr(args, 'key_matrix_dir', './save/key_matrix')
        
        logging.info('--------------------------------Start--------------------------------------')
        logging.info(args)
        logging.info('==> Preparing data...')
        
        # æ•°æ®é›†é…ç½®
        self.num_classes = args.num_classes
        self.in_channels = args.in_channels
            
        self.train_set, self.test_set, self.dict_users = get_data(dataset_name=self.dataset,
                                                                  data_root=self.data_root,
                                                                  iid=self.iid,
                                                                  client_num=self.client_num,
                                                                  )
        logging.info('==> Training model...')
        self.logs = {'best_train_acc': -np.inf, 'best_train_loss': -np.inf,
                     'val_acc': [], 'val_loss': [],
                     'best_model_acc': -np.inf, 'best_model_loss': -np.inf,
                     'best_model_auc': -np.inf,
                     'best_model': [],
                     'local_loss': [],
                     # ç‹¬ç«‹è·Ÿè¸ªå†å²æœ€é«˜æŒ‡æ ‡
                     'highest_acc_ever': -np.inf,     # å†å²æœ€é«˜å‡†ç¡®ç‡ï¼ˆçº¯å‡†ç¡®ç‡ï¼‰
                     'highest_auc_ever': -np.inf,     # å†å²æœ€é«˜AUCï¼ˆçº¯AUCï¼‰
                     'acc_when_highest_auc': -np.inf, # è¾¾åˆ°å†å²æœ€é«˜AUCæ—¶çš„å‡†ç¡®ç‡
                     'auc_when_highest_acc': -np.inf, # è¾¾åˆ°å†å²æœ€é«˜å‡†ç¡®ç‡æ—¶çš„AUC
                     }

        self.construct_model()
        self.w_t = copy.deepcopy(self.model.state_dict())

        # æ ¹æ®å‚æ•°é€‰æ‹©æ°´å°æ¨¡å¼
        self.random_positions = {}
        # è®¾ç½®å¯†é’¥çŸ©é˜µç›®å½•
        self.args.key_matrix_dir = self.key_matrix_dir
        self.args.use_key_matrix = True
        
        # æ ¹æ®watermark_modeå‚æ•°é€‰æ‹©trainer
        if self.args.watermark_mode == 'enhanced':
            logging.info('==> ä½¿ç”¨å¢å¼ºæ°´å°ç³»ç»Ÿï¼ˆå¯†é’¥çŸ©é˜µ + è‡ªç¼–ç å™¨ï¼‰')
            self.trainer = TrainerPrivateEnhanced(self.model, self.device, self.dp, self.sigma, self.random_positions, self.args)
            
            # åˆå§‹åŒ–è‡ªç¼–ç å™¨å¾®è°ƒå™¨
            self.autoencoder_finetuner = AutoencoderFinetuner(self.device)
            logging.info('==> è‡ªç¼–ç å™¨å¾®è°ƒå™¨å·²åˆå§‹åŒ–')
        else:
            logging.info('==> ä½¿ç”¨æ™®é€šæ°´å°ç³»ç»Ÿ')
            self.trainer = TrainerPrivate(self.model, self.device, self.dp, self.sigma, self.random_positions, self.args)
            self.autoencoder_finetuner = None
            
        self.tester = TesterPrivate(self.model, self.device)

    def construct_model(self):
        if self.model_name == 'resnet':
            model = resnet18(num_classes=self.num_classes, in_channels=self.in_channels, input_size=28)
        else:
            model = AlexNet(self.in_channels, self.num_classes)
        self.model = model.to(self.device)

    def _cleanup_memory(self):
        """æ¸…ç†å†…å­˜å’ŒGPUç¼“å­˜"""
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        gc.collect()

    def _optimize_model_storage(self, model_state):
        """ä¼˜åŒ–æ¨¡å‹çŠ¶æ€å­˜å‚¨ï¼Œå‡å°‘å†…å­˜å ç”¨"""
        # å°†æ¨¡å‹çŠ¶æ€ç§»åˆ°CPUï¼Œä½¿ç”¨detach()é¿å…æ¢¯åº¦è¿½è¸ª
        optimized_state = {}
        for key, value in model_state.items():
            optimized_state[key] = value.detach().cpu()
        return optimized_state

    def training(self):
        start = time.time()
        # these dataloader would only be used in calculating accuracy and loss
        train_ldr = DataLoader(self.train_set, batch_size=self.batch_size, shuffle=False, num_workers=0, pin_memory=False)
        val_ldr = DataLoader(self.test_set, batch_size=self.batch_size * 2, shuffle=False, num_workers=0, pin_memory=False)

        local_train_loader = []

        for i in range(self.client_num):
            local_train_ldr = DataLoader(DatasetSplit(self.train_set, self.dict_users[i]),
                                         batch_size=self.batch_size,
                                         shuffle=True, num_workers=0, pin_memory=False)
            local_train_loader.append(local_train_ldr)

        idxs_users = []

        # Early Stopping é…ç½® - åŸºäºå‡†ç¡®ç‡
        patience = self.args.patience  # ä»å‚æ•°ä¸­è·å–è€å¿ƒå€¼
        early_stop_counter = 0
        best_val_acc = -np.inf
        best_val_auc = -np.inf

        # ç»Ÿè®¡è®°å½•
        stats_rows = []

        for epoch in range(self.epochs): # å‡åŒ€é‡‡æ ·ï¼Œfrac é»˜è®¤ä¸º 1ï¼Œå³æ¯è½®ä¸­å…¨ä½“å®¢æˆ·ç«¯å‚ä¸è®­ç»ƒ
            if self.sampling_type == 'uniform':
                self.m = max(int(self.frac * self.client_num), 1)
                idxs_users = np.random.choice(range(self.client_num), self.m, replace=False)

            local_ws, local_losses = [], []

            logging.info('Epoch: %d / %d, lr: %f' % (epoch + 1, self.epochs, self.lr))
            
            # è‡ªç¼–ç å™¨å¾®è°ƒï¼šæ¯ä¸€è½®è”é‚¦è®­ç»ƒå¼€å§‹æ—¶éƒ½æ‰§è¡Œ
            if self.autoencoder_finetuner is not None and hasattr(self.trainer, 'autoencoder'):
                logging.info(f'==> ç¬¬{epoch+1}è½®è”é‚¦è®­ç»ƒå¼€å§‹ï¼Œå¾®è°ƒè‡ªç¼–ç å™¨...')
                try:
                    # å¾®è°ƒè‡ªç¼–ç å™¨çš„ç¼–ç å™¨éƒ¨åˆ†
                    success = self.autoencoder_finetuner.finetune_encoder(
                        autoencoder=self.trainer.autoencoder,
                        epochs=1,  # æ¯è½®åªå¾®è°ƒ1ä¸ªepoch
                        lr=0.005,
                        batch_size=128  # å‡å°‘æ‰¹å¤„ç†å¤§å°ä»¥é™ä½å†…å­˜ä½¿ç”¨
                    )
                    
                    if success:
                        logging.info(f'âœ“ ç¬¬{epoch+1}è½®è‡ªç¼–ç å™¨å¾®è°ƒå®Œæˆï¼Œæ€§èƒ½åŸºå‡†å·²æ›´æ–°')
                        
                        # è¯„ä¼°å¾®è°ƒåçš„æ€§èƒ½
                        performance = self.autoencoder_finetuner.evaluate_encoder_performance(
                            self.trainer.autoencoder, 
                            test_samples=1000
                        )
                        logging.info(f'ğŸ“Š ç¬¬{epoch+1}è½®å¾®è°ƒåç¼–ç å™¨æ€§èƒ½: {performance:.6f}')
                    else:
                        logging.warning(f'âš ï¸ ç¬¬{epoch+1}è½®è‡ªç¼–ç å™¨å¾®è°ƒå¤±è´¥ï¼Œç»§ç»­ä½¿ç”¨åŸå§‹å‚æ•°')
                        
                except Exception as e:
                    logging.error(f'âŒ ç¬¬{epoch+1}è½®è‡ªç¼–ç å™¨å¾®è°ƒè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}')
                    logging.info('ç»§ç»­ä½¿ç”¨åŸå§‹è‡ªç¼–ç å™¨å‚æ•°')
            for i, idx in enumerate(tqdm(idxs_users, desc='Progress: %d / %d' % (epoch + 1, self.epochs))):
                self.model.load_state_dict(self.w_t)

                # ç»Ÿä¸€è°ƒç”¨ï¼šå§‹ç»ˆä¼ å…¥ current_epoch/total_epochsï¼›
                # æ™®é€š Trainer ä¼šé€šè¿‡ **kwargs å¿½ç•¥
                local_w, local_loss, local_acc = self.trainer.local_update(
                    dataloader=local_train_loader[idx], 
                    local_ep=self.local_ep, 
                    lr=self.lr, 
                    client_id=idx,
                    current_epoch=epoch,
                    total_epochs=self.epochs
                )

                local_ws.append(copy.deepcopy(local_w))
                local_losses.append(local_loss)
                
                # æ¸…ç†ä¸´æ—¶å˜é‡ï¼Œé‡Šæ”¾å†…å­˜
                del local_w, local_loss, local_acc

            # å­¦ä¹ ç‡è°ƒåº¦ - MultiStepLR
            milestones = [int(self.epochs * m) for m in self.args.lr_decay_milestones]
            if (epoch + 1) in milestones:
                self.lr *= self.args.lr_decay_gamma
                logging.info(f'LR decayed at epoch {epoch + 1} (milestone: {milestones}). New lr: {self.lr}')

            # è®¡ç®—å‚ä¸è®­ç»ƒçš„å®¢æˆ·ç«¯çš„æƒé‡ï¼ˆç›¸å¯¹äºæ€»æ•°æ®é›†ï¼‰
            client_weights = []
            for idx in idxs_users:
                client_weight = len(DatasetSplit(self.train_set, self.dict_users[idx])) / len(self.train_set)
                client_weights.append(client_weight)

            # æ›´æ–°å…¨å±€æ¨¡å‹æƒé‡
            self._fed_avg(local_ws, client_weights, idxs_users)
            
            # åŠ è½½èšåˆåçš„æƒé‡å¹¶ç¡®ä¿æ¨¡å‹å¤„äºæ­£ç¡®çŠ¶æ€
            self.model.load_state_dict(self.w_t)
            self.model.train()  # ç¡®ä¿æ¨¡å‹å¤„äºè®­ç»ƒçŠ¶æ€
            
            # æ¸…ç†ä¼˜åŒ–å™¨çŠ¶æ€ï¼Œé¿å…æ¢¯åº¦ç´¯ç§¯
            if hasattr(self.trainer, 'optimizer') and self.trainer.optimizer is not None:
                self.trainer.optimizer.zero_grad()
            
            # æ¢¯åº¦ç»Ÿè®¡å·²åœ¨æ¯ä¸ªå®¢æˆ·ç«¯çš„æœ¬åœ°è®­ç»ƒè¿‡ç¨‹ä¸­æ›´æ–°ï¼Œæ— éœ€é¢å¤–å¤„ç†
            if epoch >= 0 and hasattr(self.trainer, 'multi_loss'):
                try:
                    # æ‰“å°å½“å‰æ¢¯åº¦ç»Ÿè®¡ä¿¡æ¯ï¼ˆæ¥è‡ªæœ€åä¸€ä¸ªå®¢æˆ·ç«¯çš„ç»Ÿè®¡ï¼‰
                    stats = self.trainer.get_gradient_stats()
                    if stats:
                        logging.info(f'ç¬¬{epoch+1}è½®è”é‚¦è®­ç»ƒåæ¢¯åº¦ç»Ÿè®¡:')
                        logging.info(f'  prevGM: {stats.get("prevGM", 0):.6f}')
                        logging.info(f'  prevGH: {stats.get("prevGH", 0):.6f}')
                        logging.info(f'  prevRatio: {stats.get("prevRatio", 1):.6f}')
                except Exception as e:
                    logging.error(f'è·å–æ¢¯åº¦ç»Ÿè®¡å¤±è´¥: {e}')


            if (epoch + 1) == self.epochs or (epoch + 1) % 1 == 0:
                train_metrics = self.trainer.test(train_ldr)
                val_metrics = self.trainer.test(val_ldr)

                # (loss, acc_label, auc, acc_sample)
                loss_train_mean, acc_train_label_mean, auc_train, acc_train_sample_mean = train_metrics
                loss_val_mean, acc_val_label_mean, auc_val, acc_val_sample_mean = val_metrics

                self.logs['val_acc'].append(acc_val_label_mean)
                self.logs['val_loss'].append(loss_val_mean)
                self.logs['local_loss'].append(np.mean(local_losses))

                # æ›´æ–°å†å²æœ€é«˜å€¼è·Ÿè¸ª
                if self.logs['highest_acc_ever'] < acc_val_label_mean:
                    self.logs['highest_acc_ever'] = acc_val_label_mean
                    self.logs['auc_when_highest_acc'] = auc_val
                    
                if self.logs['highest_auc_ever'] < auc_val:
                    self.logs['highest_auc_ever'] = auc_val
                    self.logs['acc_when_highest_auc'] = acc_val_label_mean

                # æ¨¡å‹é€‰æ‹©æ ‡å‡†ï¼šä»¥éªŒè¯é›†AUCä¸ºå‡†ï¼Œåªæœ‰AUCæå‡æ‰ä¿å­˜æ¨¡å‹
                if self.logs['best_model_auc'] < auc_val:
                    self.logs['best_model_acc'] = acc_val_label_mean
                    self.logs['best_model_loss'] = loss_val_mean
                    self.logs['best_model_auc'] = auc_val
                    # ä¼˜åŒ–æ¨¡å‹å­˜å‚¨ï¼Œå‡å°‘å†…å­˜å ç”¨
                    optimized_state = self._optimize_model_storage(self.model.state_dict())
                    self.logs['best_model'] = [optimized_state]
                    logging.info(f'New best model saved! AUC improved to {auc_val:.4f}')

                if self.logs['best_train_acc'] < acc_train_label_mean:
                    self.logs['best_train_acc'] = acc_train_label_mean
                    self.logs['best_train_loss'] = loss_train_mean

                logging.info(
                    f"Train Loss {loss_train_mean:.4f} --- Val Loss {loss_val_mean:.4f}")
                logging.info(
                    f"Train: acc(label) {acc_train_label_mean:.4f}, acc(sample) {acc_train_sample_mean:.4f} (AUC {auc_train:.4f}) | "
                    f"Val: acc(label) {acc_val_label_mean:.4f}, acc(sample) {acc_val_sample_mean:.4f} (AUC {auc_val:.4f}) | "
                    f"Highest ACC: {self.logs['highest_acc_ever']:.4f} | Highest AUC: {self.logs['highest_auc_ever']:.4f}")
                
                # æ‰“å°å¢å¼ºæ°´å°ç³»ç»Ÿç»Ÿè®¡ä¿¡æ¯
                if hasattr(self.trainer, 'multi_loss'):
                    stats = self.trainer.multi_loss.get_stats()
                    logging.info(f"MultiLossç»Ÿè®¡ - prevGM: {stats['prevGM']:.6f}, prevGH: {stats['prevGH']:.6f}, prevRatio: {stats['prevRatio']:.6f}")
                
                # è®°å½•æœ¬è½®ç»Ÿè®¡æ•°æ®
                stats_row = {
                    'round': epoch + 1,
                    'lr': self.lr,
                    'train_loss': float(loss_train_mean),
                    'val_loss': float(loss_val_mean),
                    'train_acc_label': float(acc_train_label_mean),
                    'train_auc': float(auc_train),
                    'val_acc_label': float(acc_val_label_mean),
                    'val_auc': float(auc_val),
                    'best_val_acc_so_far': float(self.logs['highest_acc_ever']),
                    'best_val_auc_so_far': float(self.logs['highest_auc_ever']),
                    'train_acc_sample': float(acc_train_sample_mean),
                    'val_acc_sample': float(acc_val_sample_mean),
                }
                
                # Early Stoppingï¼šåŸºäºéªŒè¯AUC
                if auc_val > best_val_auc:
                    best_val_auc = auc_val
                    early_stop_counter = 0
                else:
                    early_stop_counter += 1
                    if early_stop_counter >= patience:
                        logging.info(f'Early stopping triggered at epoch {epoch + 1}. Best Val AUC: {best_val_auc:.4f}')
                        break
                
                # æ¯è½®è®­ç»ƒåæ¸…ç†å†…å­˜
                self._cleanup_memory()
                
                # æ¸…ç†ä¸´æ—¶å˜é‡
                del train_metrics, val_metrics
                del loss_train_mean, acc_train_label_mean, auc_train, acc_train_sample_mean
                del loss_val_mean, acc_val_label_mean, auc_val, acc_val_sample_mean
                
                # æ·»åŠ è‡ªç¼–ç å™¨å¾®è°ƒç»Ÿè®¡ä¿¡æ¯
                if self.autoencoder_finetuner is not None and hasattr(self.trainer, 'autoencoder'):
                    try:
                        # è·å–å½“å‰è‡ªç¼–ç å™¨æ€§èƒ½
                        current_performance = self.autoencoder_finetuner.evaluate_encoder_performance(
                            self.trainer.autoencoder, 
                            test_samples=500  # ä½¿ç”¨è¾ƒå°‘æ ·æœ¬è¿›è¡Œå¿«é€Ÿè¯„ä¼°
                        )
                        stats_row['autoencoder_performance'] = float(current_performance)
                        # ç®€åŒ–è¾“å‡ºï¼šåªåœ¨ç‰¹å®šè½®æ¬¡æ˜¾ç¤ºæ€§èƒ½
                        if (epoch + 1) % 10 == 0 or epoch == 0:
                            logging.info(f'ğŸ“Š è½®æ¬¡ {epoch + 1} è‡ªç¼–ç å™¨æ€§èƒ½: {current_performance:.6f}')
                    except Exception as e:
                        stats_row['autoencoder_performance'] = float('inf')
                        logging.warning(f'âš ï¸ æ— æ³•è¯„ä¼°è‡ªç¼–ç å™¨æ€§èƒ½: {e}')
                else:
                    stats_row['autoencoder_performance'] = None
                
                # æ·»åŠ å¢å¼ºæ°´å°ç³»ç»Ÿç»Ÿè®¡ä¿¡æ¯
                if hasattr(self.trainer, 'multi_loss'):
                    multi_loss_stats = self.trainer.multi_loss.get_stats()
                    stats_row.update({
                        'prevGM': float(multi_loss_stats['prevGM']),
                        'prevGH': float(multi_loss_stats['prevGH']),
                        'prevRatio': float(multi_loss_stats['prevRatio']),
                        'current_grad_M': float(multi_loss_stats['current_grad_M']),
                        'current_grad_H': float(multi_loss_stats['current_grad_H']),
                        'current_var_M': float(multi_loss_stats['current_var_M']),
                        'current_var_H': float(multi_loss_stats['current_var_H']),
                    })
                
                stats_rows.append(stats_row)

        logging.info('-------------------------------Result--------------------------------------')
        logging.info(
            f'Test loss: {self.logs["best_model_loss"]:.4f} --- Test acc: {self.logs["best_model_acc"]:.4f} --- Test auc: {self.logs["best_model_auc"]:.4f}')
        logging.info('å†å²æœ€é«˜ç»Ÿè®¡:')
        logging.info(
            f'  å†å²æœ€é«˜å‡†ç¡®ç‡: {self.logs["highest_acc_ever"]:.4f} (å¯¹åº”AUC: {self.logs["auc_when_highest_acc"]:.4f})')
        logging.info(
            f'  å†å²æœ€é«˜AUC: {self.logs["highest_auc_ever"]:.4f} (å¯¹åº”å‡†ç¡®ç‡: {self.logs["acc_when_highest_auc"]:.4f})')
        end = time.time()
        logging.info('Time: {:.1f} min'.format((end - start) / 60))
        logging.info('-------------------------------Finish--------------------------------------')
        
        # æœ€ç»ˆå†…å­˜æ¸…ç†
        self._cleanup_memory()
        logging.info('ğŸ§¹ è®­ç»ƒå®Œæˆï¼Œå·²æ¸…ç†å†…å­˜ç¼“å­˜')

        # å¯¼å‡ºExcel
        try:
            os.makedirs(self.args.save_excel_dir, exist_ok=True)
            # åŸºç¡€åˆ— + å¢å¼ºæ°´å°ç³»ç»Ÿç»Ÿè®¡åˆ— + è‡ªç¼–ç å™¨æ€§èƒ½åˆ—
            columns = ['round', 'lr', 'train_loss', 'val_loss', 'train_acc_label', 'train_auc', 
                     'val_acc_label', 'val_auc', 'best_val_acc_so_far', 'best_val_auc_so_far', 
                     'train_acc_sample', 'val_acc_sample', 'autoencoder_performance',
                     'prevGM', 'prevGH', 'prevRatio', 
                     'current_grad_M', 'current_grad_H', 'current_var_M', 'current_var_H']
            df = pd.DataFrame(stats_rows, columns=columns)
            now = datetime.now().strftime('%Y%m%d%H%M%S')
            excel_path = f'{self.args.save_excel_dir}/metrics_{self.model_name}_{self.dataset}_{now}.xlsx'
            df.to_excel(excel_path, index=False, engine='openpyxl')
            logging.info(f'Excel metrics saved to: {excel_path}')
        except Exception as e:
            logging.warning(f'Failed to export Excel metrics: {e}')

        return self.logs, self.logs['best_model_auc']


    def _fed_avg(self, local_ws, client_weights, idxs_users):
        """è”é‚¦å¹³å‡ç®—æ³•ï¼ŒFedAvg with exclusive watermark aggregation"""
        # è®¡ç®—å‚ä¸è®­ç»ƒçš„å®¢æˆ·ç«¯æƒé‡æ€»å’Œ
        total_weight = sum(client_weights)
        
        # å½’ä¸€åŒ–æƒé‡ï¼Œç¡®ä¿æƒé‡å’Œä¸º1
        normalized_weights = [w / total_weight for w in client_weights]
        
        # éªŒè¯æƒé‡å’Œæ˜¯å¦ä¸º1
        weight_sum = sum(normalized_weights)
        if abs(weight_sum - 1.0) > self.args.weight_tolerance:
            logging.warning(f"Weight sum is {weight_sum:.6f}, not 1.0. Normalizing...")
            normalized_weights = [w / weight_sum for w in normalized_weights]
        
        # åˆå§‹åŒ–å¹³å‡æƒé‡
        w_avg = {}
        for k in local_ws[0].keys():
            w_avg[k] = local_ws[0][k].clone() * normalized_weights[0]

        # ç´¯åŠ å…¶ä»–å®¢æˆ·ç«¯çš„æƒé‡
        for i in range(1, len(local_ws)):
            for k in w_avg.keys():
                w_avg[k] += local_ws[i][k] * normalized_weights[i]

        # æ°´å°èšåˆï¼šä½¿ç”¨å¯†é’¥çŸ©é˜µçš„ç‹¬å å¼èšåˆ
        self._watermark_aggregation(local_ws, idxs_users, w_avg)

        # æ›´æ–°å…¨å±€æ¨¡å‹æƒé‡
        for k in w_avg.keys():
            self.w_t[k] = w_avg[k]

    def _watermark_aggregation(self, local_ws, idxs_users, w_avg):
        """
        æ°´å°èšåˆï¼šä½¿ç”¨å¯†é’¥çŸ©é˜µçš„ç‹¬å å¼èšåˆ
        
        Args:
            local_ws: æœ¬åœ°æ¨¡å‹æƒé‡åˆ—è¡¨
            idxs_users: å‚ä¸è®­ç»ƒçš„å®¢æˆ·ç«¯IDåˆ—è¡¨
            w_avg: å¹³å‡èšåˆåçš„æƒé‡å­—å…¸
        """
        try:
            from utils.key_matrix_utils import KeyMatrixManager
            
            # åŠ è½½å¯†é’¥çŸ©é˜µç®¡ç†å™¨
            key_manager = KeyMatrixManager(self.key_matrix_dir, args=self.args)
            
            # å¯¹æ¯ä¸ªå®¢æˆ·ç«¯çš„æ°´å°ä½ç½®è¿›è¡Œç‹¬å å¼èšåˆ
            for i, client_id in enumerate(idxs_users):
                try:
                    # è·å–è¯¥å®¢æˆ·ç«¯çš„æ°´å°ä½ç½®
                    positions = key_manager.load_positions(client_id)
                    
                    # å¯¹è¯¥å®¢æˆ·ç«¯çš„æ°´å°ä½ç½®ä½¿ç”¨ç‹¬å å¼èšåˆ
                    for param_name, param_idx in positions:
                        if param_name in local_ws[i] and param_name in w_avg:
                            # è·å–å‚æ•°å¼ é‡
                            local_param = local_ws[i][param_name]
                            avg_param = w_avg[param_name]
                            
                            # ç¡®ä¿å‚æ•°å½¢çŠ¶ä¸€è‡´
                            if local_param.shape == avg_param.shape:
                                # å°†å±€éƒ¨ç´¢å¼•è½¬æ¢ä¸ºæ‰å¹³åŒ–ç´¢å¼•
                                param_flat = avg_param.view(-1)
                                local_flat = local_param.view(-1)
                                
                                # ä½¿ç”¨å±€éƒ¨ç´¢å¼•ç›´æ¥æ›¿æ¢
                                if param_idx < param_flat.numel():
                                    param_flat[param_idx] = local_flat[param_idx]
                                    
                except Exception as e:
                    logging.warning(f"Failed to apply watermark aggregation for client {client_id}: {e}")
                    continue
                    
        except Exception as e:
            logging.warning(f"Failed to load key matrix manager for watermark aggregation: {e}")

def main(args):
    logs = {'net_info': None,
            'arguments': {
                'frac': args.frac,
                'local_ep': args.local_ep,
                'local_bs': args.batch_size,
                'lr_outer': args.lr_outer,
                'lr_inner': args.lr,
                'iid': args.iid,
                'wd': args.wd,
                'optim': args.optim,
                'model_name': args.model_name,
                'dataset': args.dataset,
                'log_interval': args.log_interval,
                'num_classes': args.num_classes,
                'epochs': args.epochs,
                'client_num': args.client_num,
                'console_log': os.path.basename(log_file_name),
            }
            }
    fl = FederatedLearningOnChestMNIST(args)
    logg, test_auc = fl.training()
    logs['net_info'] = logg
    logs['test_auc'] = {'value': test_auc}
    logs['bp_local'] = {'value': True if args.bp_interval == 0 else False}

    save_dir = os.path.join(args.save_model_dir, args.model_name, args.dataset)
    os.makedirs(save_dir, exist_ok=True)

    now = datetime.now()
    formatted_now = now.strftime("%Y%m%d%H%M")
    # æ„å»ºæ–‡ä»¶å
    enhanced = "_enhanced" if args.watermark_mode == 'enhanced' else ""

    file_name = '{}_Dp_{}_iid_{}_ns_{}_wt_{}_lt_{}_ep_{}_le_{}_cn_{}_fra_{:.4f}_auc_{:.4f}{}.pkl'.format(
        formatted_now, args.sigma, args.iid, args.num_sign, args.weight_type, args.loss_type,
        args.epochs, args.local_ep, args.client_num, args.frac, test_auc, enhanced
    )
    torch.save(logs, os.path.join(save_dir, file_name))
    logging.info(f"è®­ç»ƒæ—¥å¿—å·²ä¿å­˜: {file_name}")

    return

if __name__ == '__main__':
    os.environ["CUDA_DEVICE_ORDER"] = "PCI_BUS_ID"
    os.environ["CUDA_VISIBLE_DEVICES"] = args.gpu
    main(args)