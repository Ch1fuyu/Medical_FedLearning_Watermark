import copy
import os
import pandas as pd
import numpy as np
from datetime import datetime

import torch
from torch.utils.data import DataLoader
from torchvision import datasets, transforms

from models.light_autoencoder import LightAutoencoder
from models.resnet import resnet18
from utils.watermark_reconstruction import WatermarkReconstructor
from utils.delta_pcc_utils import evaluate_delta_pcc, calculate_fixed_tau, format_delta_pcc_result, print_delta_pcc_summary

def load_test_data(dataset_name: str, batch_size: int = 128, data_dir: str = './data'):
    """
    æ ¹æ®æ•°æ®é›†åç§°åŠ è½½ç›¸åº”çš„æµ‹è¯•æ•°æ®
    
    Args:
        dataset_name: æ•°æ®é›†åç§° ('cifar10', 'cifar100', 'chestmnist', 'mnist')
        batch_size: æ‰¹æ¬¡å¤§å°
        data_dir: æ•°æ®ç›®å½•
        
    Returns:
        æµ‹è¯•æ•°æ®åŠ è½½å™¨
    """
    if dataset_name.lower() == 'cifar10':
        transform = transforms.Compose([
            transforms.ToTensor(),
            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))
        ])
        test_dataset = datasets.CIFAR10(data_dir, train=False, download=True, transform=transform)
        
    elif dataset_name.lower() == 'cifar100':
        transform = transforms.Compose([
            transforms.ToTensor(),
            transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))
        ])
        test_dataset = datasets.CIFAR100(data_dir, train=False, download=True, transform=transform)
        
    elif dataset_name.lower() == 'chestmnist':
        normalize = transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])
        transform = transforms.Compose([
            transforms.ToTensor(),
            normalize,
        ])

        from utils.dataset import LocalChestMNISTDataset
        dataset_path = os.path.join(data_dir, 'chestmnist.npz')
        if not os.path.exists(dataset_path):
            raise FileNotFoundError(f"ChestMNISTæ•°æ®é›†æ–‡ä»¶ä¸å­˜åœ¨: {dataset_path}")
        
        test_dataset = LocalChestMNISTDataset(dataset_path, split='test', transform=transform)
        
    elif dataset_name.lower() == 'mnist':
        transform = transforms.Compose([
            transforms.ToTensor(),
            transforms.Normalize((0.1307,), (0.3081,))
        ])
        test_dataset = datasets.MNIST(data_dir, train=False, download=True, transform=transform)
        
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„æ•°æ®é›†: {dataset_name}")
    
    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)
    print(f"âœ“ å·²åŠ è½½{dataset_name.upper()}æµ‹è¯•é›†: {len(test_dataset)} ä¸ªæ ·æœ¬")
    return test_loader

def load_mnist_test_data(batch_size: int = 128, data_dir: str = './data'):
    """
    åŠ è½½MNISTæµ‹è¯•æ•°æ®ï¼Œä½¿ç”¨ä¸è®­ç»ƒæ—¶ç›¸åŒçš„æ•°æ®é¢„å¤„ç†ç­–ç•¥
    
    Args:
        batch_size: æ‰¹æ¬¡å¤§å°
        data_dir: æ•°æ®ç›®å½•
        
    Returns:
        MNISTæµ‹è¯•æ•°æ®åŠ è½½å™¨
    """
    return load_test_data('mnist', batch_size, data_dir)

def build_autoencoder_from_watermark(watermark_params, decoder_path: str, device: str = 'cuda'):
    """
    ä»æ°´å°å‚æ•°æ„å»ºè‡ªç¼–ç å™¨
    
    Args:
        watermark_params: ä»ä¸»æ¨¡å‹ä¸­æå–çš„æ°´å°å‚æ•°
        decoder_path: è§£ç å™¨æƒé‡æ–‡ä»¶è·¯å¾„
        device: è®¾å¤‡ç±»å‹
        
    Returns:
        æ„å»ºçš„è‡ªç¼–ç å™¨æ¨¡å‹
    """
    try:
        # åˆ›å»ºè‡ªç¼–ç å™¨å®ä¾‹
        autoencoder = LightAutoencoder().to(device)
        
        # ä»æ°´å°å‚æ•°æ„å»ºç¼–ç å™¨
        # è¿™é‡Œéœ€è¦æ ¹æ®å®é™…çš„æ°´å°å‚æ•°ç»“æ„æ¥æ˜ å°„åˆ°ç¼–ç å™¨å±‚
        # å‡è®¾watermark_paramsæ˜¯ä¸€ä¸ªå­—å…¸ï¼ŒåŒ…å«ç¼–ç å™¨å„å±‚çš„å‚æ•°
        if isinstance(watermark_params, dict):
            encoder_state_dict = {}
            for name, param in watermark_params.items():
                if 'encoder' in name:
                    encoder_state_dict[name] = param
            autoencoder.encoder.load_state_dict(encoder_state_dict)
        else:
            # å¦‚æœwatermark_paramsæ˜¯å¼ é‡ï¼Œéœ€è¦é‡æ–°æ•´å½¢å¹¶åˆ†é…åˆ°ç¼–ç å™¨å±‚
            print("âš ï¸  æ°´å°å‚æ•°æ ¼å¼éœ€è¦è¿›ä¸€æ­¥å¤„ç†")
            return None
        
        # åŠ è½½é¢„è®­ç»ƒçš„è§£ç å™¨
        if os.path.exists(decoder_path):
            decoder_state_dict = torch.load(decoder_path, map_location=device, weights_only=False)
            autoencoder.decoder.load_state_dict(decoder_state_dict)
            print(f"âœ“ å·²åŠ è½½è§£ç å™¨: {decoder_path}")
        else:
            print(f"âŒ è§£ç å™¨æ–‡ä»¶ä¸å­˜åœ¨: {decoder_path}")
            return None
        
        autoencoder.eval()
        return autoencoder
        
    except Exception as e:
        print(f"âŒ æ„å»ºè‡ªç¼–ç å™¨å¤±è´¥: {e}")
        return None



def load_main_task_model(model_path: str, device: str = 'cuda'):
    """
    åŠ è½½ä¸»ä»»åŠ¡æ¨¡å‹ï¼Œè‡ªåŠ¨ä»checkpointæ¨æ–­æ•°æ®é›†å‚æ•°
    
    Args:
        model_path: æ¨¡å‹æ–‡ä»¶è·¯å¾„
        device: è®¾å¤‡ç±»å‹
        
    Returns:
        tuple: (åŠ è½½çš„æ¨¡å‹, æ¨¡å‹ä¿¡æ¯å­—å…¸)
    """
    if not os.path.exists(model_path):
        print(f"âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
        return None, None
    
    # åŠ è½½checkpointè·å–å‚æ•°ä¿¡æ¯
    checkpoint = torch.load(model_path, map_location=device, weights_only=False)
    
    # ä»checkpointä¸­è·å–æ•°æ®é›†å‚æ•°
    net_info = checkpoint.get('net_info', {})
    arguments = checkpoint.get('arguments', {})
    
    # ä¼˜å…ˆä»argumentsè·å–ï¼Œå¦åˆ™ä½¿ç”¨é»˜è®¤å€¼
    num_classes = arguments.get('num_classes', 14)
    in_channels = arguments.get('in_channels', 3)
    dataset = arguments.get('dataset', 'chestmnist')
    model_name = arguments.get('model_name', 'unknown')
    
    # å¦‚æœæ— æ³•ä»argumentsè·å–model_nameï¼Œå°è¯•ä»è·¯å¾„æå–
    if model_name == 'unknown':
        # ä»æ¨¡å‹è·¯å¾„æå–æ¨¡å‹åç§°
        normalized_path = model_path.replace('\\', '/')
        path_parts = normalized_path.split('/')
        for part in path_parts:
            if part in ['resnet', 'alexnet']:
                model_name = part
                break
        else:
            model_name = 'resnet'  # é»˜è®¤ä½¿ç”¨resnet
    
    # æ ¹æ®æ•°æ®é›†è®¾ç½®input_size
    if dataset.lower() == 'cifar10' or dataset.lower() == 'cifar100':
        input_size = 32
    elif dataset.lower() == 'imagenet':
        input_size = 224
    else:  # chestmnistç­‰
        input_size = 28
    
    # æ„å»ºæ¨¡å‹ä¿¡æ¯å­—å…¸
    model_info = {
        'dataset': dataset,
        'num_classes': num_classes,
        'in_channels': in_channels,
        'input_size': input_size,
        'model_name': model_name,
        'model_path': model_path,
        'model_filename': os.path.basename(model_path)
    }
    
    print(f"âœ“ æ£€æµ‹åˆ°æ•°æ®é›†: {dataset}, ç±»åˆ«æ•°: {num_classes}, è¾“å…¥é€šé“: {in_channels}, è¾“å…¥å°ºå¯¸: {input_size}")
    print(f"âœ“ æ¨¡å‹ç±»å‹: {model_name}")
    
    # æ ¹æ®æ¨¡å‹åç§°åˆ›å»ºå¯¹åº”çš„æ¨¡å‹å®ä¾‹
    if model_name in ['alexnet']:
        from models.alexnet import alexnet
        model = alexnet(num_classes=num_classes, in_channels=in_channels, input_size=input_size)
    elif model_name in ['resnet', 'resnet18']:
        model = resnet18(num_classes=num_classes, in_channels=in_channels, input_size=input_size)
    else:
        print(f"âš ï¸  æœªçŸ¥æ¨¡å‹ç±»å‹: {model_name}ï¼Œä½¿ç”¨é»˜è®¤resnet18")
        model = resnet18(num_classes=num_classes, in_channels=in_channels, input_size=input_size)

    # åŠ è½½æ¨¡å‹æƒé‡
    model.load_state_dict(checkpoint['net_info']['best_model'][0])
    print(f"âœ“ å·²åŠ è½½ä¸»ä»»åŠ¡æ¨¡å‹: {model_path}")
    
    model = model.to(device)
    model.eval()
    
    return model, model_info

def evaluate_model_accuracy(model, test_loader, device='cuda', dataset_type='chestmnist'):
    """
    è¯„ä¼°æ¨¡å‹åœ¨æµ‹è¯•é›†ä¸Šçš„å‡†ç¡®ç‡
    
    Args:
        model: å¾…è¯„ä¼°çš„æ¨¡å‹
        test_loader: æµ‹è¯•æ•°æ®åŠ è½½å™¨
        device: è®¾å¤‡ç±»å‹
        dataset_type: æ•°æ®é›†ç±»å‹ï¼Œç”¨äºç¡®å®šè¯„ä¼°æ–¹å¼
        
    Returns:
        float: æ¨¡å‹å‡†ç¡®ç‡
    """
    model.eval()
    correct = 0
    total = 0
    
    with torch.no_grad():
        for data, target in test_loader:
            data, target = data.to(device), target.to(device)
            outputs = model(data)
            
            if dataset_type.lower() == 'chestmnist':
                # å¤šæ ‡ç­¾åˆ†ç±»ï¼šä½¿ç”¨sigmoid + é˜ˆå€¼0.5
                predicted = (torch.sigmoid(outputs) > 0.5).float()
                # è®¡ç®—æ ·æœ¬çº§å‡†ç¡®ç‡ï¼ˆæ‰€æœ‰æ ‡ç­¾éƒ½æ­£ç¡®æ‰ç®—æ­£ç¡®ï¼‰
                correct += (predicted == target).all(dim=1).sum().item()
            else:
                # å•æ ‡ç­¾åˆ†ç±»ï¼šä½¿ç”¨argmax
                _, predicted = torch.max(outputs.data, 1)
                correct += (predicted == target).sum().item()
            
            total += target.size(0)
    
    accuracy = correct / total
    return accuracy

def evaluate_model_auc(model, test_loader, device='cuda', dataset_type='chestmnist'):
    """
    è¯„ä¼°æ¨¡å‹åœ¨æµ‹è¯•é›†ä¸Šçš„AUC
    
    Args:
        model: å¾…è¯„ä¼°çš„æ¨¡å‹
        test_loader: æµ‹è¯•æ•°æ®åŠ è½½å™¨
        device: è®¾å¤‡ç±»å‹
        dataset_type: æ•°æ®é›†ç±»å‹ï¼Œç”¨äºç¡®å®šè¯„ä¼°æ–¹å¼
        
    Returns:
        float: æ¨¡å‹AUCï¼ˆå¤šæ ‡ç­¾ä»»åŠ¡å–å¹³å‡AUCï¼‰
    """
    import numpy as np
    from sklearn.metrics import roc_auc_score
    
    model.eval()
    all_predictions = []
    all_targets = []
    
    with torch.no_grad():
        for data, target in test_loader:
            data, target = data.to(device), target.to(device)
            outputs = model(data)
            
            if dataset_type.lower() == 'chestmnist':
                # å¤šæ ‡ç­¾åˆ†ç±»ï¼šä½¿ç”¨sigmoidè¾“å‡ºæ¦‚ç‡
                predictions = torch.sigmoid(outputs).cpu().numpy()
                targets = target.cpu().numpy()
            else:
                # å•æ ‡ç­¾åˆ†ç±»ï¼šä½¿ç”¨softmaxè¾“å‡ºæ¦‚ç‡
                predictions = torch.softmax(outputs, dim=1).cpu().numpy()
                targets = target.cpu().numpy()
            
            all_predictions.append(predictions)
            all_targets.append(targets)
    
    # åˆå¹¶æ‰€æœ‰æ‰¹æ¬¡çš„ç»“æœ
    all_predictions = np.vstack(all_predictions)
    all_targets = np.vstack(all_targets) if dataset_type.lower() == 'chestmnist' else np.hstack(all_targets)
    
    if dataset_type.lower() == 'chestmnist':
        # å¤šæ ‡ç­¾åˆ†ç±»ï¼šè®¡ç®—æ¯ä¸ªæ ‡ç­¾çš„AUCç„¶åå–å¹³å‡
        auc_scores = []
        for i in range(all_targets.shape[1]):
            try:
                auc = roc_auc_score(all_targets[:, i], all_predictions[:, i])
                auc_scores.append(auc)
            except ValueError:
                # å¦‚æœæŸä¸ªæ ‡ç­¾åªæœ‰ä¸€ç§ç±»åˆ«ï¼Œè·³è¿‡
                continue
        
        if auc_scores:
            avg_auc = np.mean(auc_scores)
        else:
            avg_auc = 0.0
    else:
        # å•æ ‡ç­¾åˆ†ç±»ï¼šè®¡ç®—å¤šç±»AUC
        try:
            avg_auc = roc_auc_score(all_targets, all_predictions, multi_class='ovr', average='macro')
        except ValueError:
            avg_auc = 0.0
    
    return avg_auc

def threshold_pruning(model, pruning_ratio: float):
    """
    å¯¹æ¨¡å‹è¿›è¡Œé˜ˆå€¼å‰ªæ
    
    Args:
        model: å¾…å‰ªæçš„æ¨¡å‹
        pruning_ratio: å‰ªææ¯”ä¾‹ (0.0-1.0)
        
    Returns:
        å‰ªæåçš„æ¨¡å‹
    """
    # å§‹ç»ˆåˆ›å»ºæ¨¡å‹å‰¯æœ¬ï¼Œå³ä½¿å‰ªææ¯”ä¾‹ä¸º0
    pruned_model = copy.deepcopy(model)
    
    if pruning_ratio <= 0:
        return pruned_model
    
    # æ”¶é›†æ‰€æœ‰æƒé‡å‚æ•°
    all_weights = []
    for name, param in pruned_model.named_parameters():
        if 'weight' in name:  # åªå¯¹æƒé‡è¿›è¡Œå‰ªæï¼Œä¸åŒ…æ‹¬åç½®
            all_weights.append(param.data.view(-1))
    
    if not all_weights:
        print("è­¦å‘Š: æ²¡æœ‰æ‰¾åˆ°å¯å‰ªæçš„æƒé‡å‚æ•°")
        return pruned_model
    
    # åˆå¹¶æ‰€æœ‰æƒé‡
    all_weights = torch.cat(all_weights)
    
    # è®¡ç®—å‰ªæé˜ˆå€¼
    total_params = len(all_weights)
    prune_count = int(total_params * pruning_ratio)
    keep_params = total_params - prune_count
    
    print(f"  å‰ªæé…ç½®: æ€»å‚æ•°={total_params}, ä¿ç•™={keep_params}, å‰ªæ={prune_count}")

    abs_weights = torch.abs(all_weights)
    # percentile ä¼šè¿”å›ç¬¬ rate% ä½ç½®çš„å€¼ä½œä¸ºé˜ˆå€¼ï¼Œä¾‹å¦‚ pruning_ratio=0.1 è¡¨ç¤ºä¿ç•™90%ï¼Œpercentile=10 è¡¨ç¤º10%å¤„çš„å€¼
    percentile = pruning_ratio * 100

    threshold_value = np.percentile(abs_weights.detach().cpu().numpy(), percentile)
    threshold = torch.tensor(threshold_value, dtype=abs_weights.dtype, device=abs_weights.device)
    
    print(f"  é˜ˆå€¼è®¡ç®—: percentile={percentile:.2f}%, threshold={threshold:.9f}")
    
    # åº”ç”¨å‰ªæ
    pruned_count = 0
    total_count = 0
    
    for name, param in pruned_model.named_parameters():
        if 'weight' in name:
            mask = torch.abs(param.data) > threshold
            pruned_count += (~mask).sum().item()
            total_count += param.data.numel()

            param.data *= mask.float()
    
    print(f"å‰ªæå®Œæˆ: é˜ˆå€¼={threshold:.9f}, å‰ªæ={pruned_count}/{total_count} ({pruned_count/total_count:.1%}), å‰©ä½™={total_count-pruned_count} ({1-pruned_count/total_count:.1%})")
    
    return pruned_model

def evaluate_watermark_after_pruning(model, reconstructor):
    """
    è¯„ä¼°å‰ªæåæ¨¡å‹çš„æ°´å°å®Œæ•´æ€§
    
    Args:
        model: å‰ªæåçš„æ¨¡å‹
        reconstructor: æ°´å°é‡å»ºå™¨å®ä¾‹
        
    Returns:
        æ°´å°é‡å»ºç»“æœ
    """
    try:
        
        # è·å–æ¨¡å‹çŠ¶æ€å­—å…¸
        model_state_dict = model.state_dict()
        
        # ä»æ‰€æœ‰å®¢æˆ·ç«¯é‡å»ºè‡ªç¼–ç å™¨
        reconstructed_autoencoder = reconstructor.reconstruct_autoencoder_from_all_clients(model_state_dict)
        
        if reconstructed_autoencoder is None:
            print("âŒ æ°´å°é‡å»ºå¤±è´¥")
            return None
        
        # è®¡ç®—æ°´å°å‚æ•°ç»Ÿè®¡
        key_manager = reconstructor.key_manager
        all_client_ids = key_manager.list_clients()
        
        total_watermark_params = 0
        damaged_watermark_params = 0
        
        for client_id in all_client_ids:
            try:
                # æå–æ°´å°å‚æ•°å¹¶æ£€æŸ¥å‰ªæå½±å“
                watermark_values = key_manager.extract_watermark(model_state_dict, client_id, check_pruning=True)
                total_watermark_params += len(watermark_values)
                
                # æ£€æŸ¥è¢«å‰ªæçš„æ°´å°å‚æ•°ï¼ˆå®Œå…¨ç­‰äº0çš„å‚æ•°ï¼‰
                damaged_count = (watermark_values == 0.0).sum().item()
                damaged_watermark_params += damaged_count
                
            except Exception as e:
                pass  # é™é»˜å¤„ç†é”™è¯¯
        
        # è®¡ç®—æ°´å°å®Œæ•´æ€§æŒ‡æ ‡
        if total_watermark_params > 0:
            watermark_integrity = 1.0 - (damaged_watermark_params / total_watermark_params)
        else:
            watermark_integrity = 0.0
        
        return {
            'reconstructed_autoencoder': reconstructed_autoencoder,
            'watermark_integrity': watermark_integrity,
            'total_watermark_params': total_watermark_params,
            'damaged_watermark_params': damaged_watermark_params
        }
        
    except Exception as e:
        print(f"âŒ æ°´å°è¯„ä¼°å¤±è´¥: {e}")
        return None

def save_pruning_results(results, model_info, save_dir='./save/pruning_results'):
    """
    ä¿å­˜å‰ªææ”»å‡»å®éªŒç»“æœ
    
    Args:
        results: å®éªŒç»“æœåˆ—è¡¨
        model_info: æ¨¡å‹ä¿¡æ¯å­—å…¸
        save_dir: ä¿å­˜ç›®å½•
    """
    os.makedirs(save_dir, exist_ok=True)
    
    # ç”Ÿæˆæ—¶é—´æˆ³
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    
    # ä»æ¨¡å‹ä¿¡æ¯ä¸­æå–æ•°æ®é›†å’Œæ¨¡å‹åç§°
    dataset = model_info.get('dataset', 'unknown')
    model_name = model_info.get('model_name', 'unknown')
    
    # ç”Ÿæˆæ–‡ä»¶å
    filename_prefix = f'pruning_attack_{dataset}_{timestamp}'
    csv_file = os.path.join(save_dir, f'{filename_prefix}.csv')
    
    df_data = []
    for result in results:
        df_data.append({
            'pruning_ratio': result['pruning_ratio'],
            'auc': result.get('auc_after', 0.0),
            'accuracy': result.get('accuracy_after', 0.0),
            'watermark_integrity': result['watermark_integrity'],
            'total_watermark_params': result['total_watermark_params'],
            'damaged_watermark_params': result['damaged_watermark_params'],
            'delta_pcc': result.get('delta_pcc', float('inf')),
            'is_infringement': result.get('is_infringement', False),
            'result_text': result.get('result_text', 'è¯„ä¼°å¤±è´¥')
        })
    
    df = pd.DataFrame(df_data)
    
    # æ ¼å¼åŒ–æ•°å€¼åˆ—ï¼Œä¿ç•™6ä½å°æ•°
    numeric_columns = ['pruning_ratio', 'auc', 'accuracy', 'watermark_integrity', 
                      'total_watermark_params', 'damaged_watermark_params', 'delta_pcc']
    for col in numeric_columns:
        if col in df.columns:
            df[col] = df[col].apply(lambda x: f"{x:.6f}" if pd.notna(x) and isinstance(x, (int, float)) else x)
    
    # åœ¨CSVæ–‡ä»¶å¼€å¤´æ·»åŠ æ¨¡å‹æ–‡ä»¶åä¿¡æ¯
    model_filename = model_info.get('model_filename', 'unknown')
    
    # å…ˆå†™å…¥æ³¨é‡Šè¡Œï¼Œç„¶åå†™å…¥æ•°æ®
    with open(csv_file, 'w', encoding='utf-8-sig') as f:
        f.write(f"# æ¨¡å‹æ–‡ä»¶: {model_filename}\n")
        f.write(f"# æ•°æ®é›†: {model_info.get('dataset', 'unknown')}\n")
        f.write(f"# ç”Ÿæˆæ—¶é—´: {timestamp}\n")
        f.write("#\n")
    
    # è¿½åŠ æ•°æ®åˆ°æ–‡ä»¶
    df.to_csv(csv_file, mode='a', index=False, encoding='utf-8-sig')
    
    print(f"âœ“ ç»“æœå·²ä¿å­˜åˆ°: {save_dir}")
    print(f"  - CSVæ–‡ä»¶: {csv_file}")

def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser(description='å‰ªææ”»å‡»å®éªŒ')
    parser.add_argument('--model_path', type=str, 
                       default='./save/alexnet/chestmnist/202510301443_Dp_0.1_iid_True_wm_enhanced_ep_150_le_2_cn_5_fra_1.0000_auc_0.6783_enhanced.pkl',
                       help='æ¨¡å‹æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--key_matrix_dir', type=str, default='./save/key_matrix',
                       help='å¯†é’¥çŸ©é˜µåŸºç¡€ç›®å½•')
    parser.add_argument('--autoencoder_dir', type=str, default='./save/autoencoder',
                       help='è‡ªç¼–ç å™¨ç›®å½•')
    parser.add_argument('--model_type', type=str, default='alexnet',
                       choices=['resnet', 'alexnet'],
                       help='æ¨¡å‹ç±»å‹')
    parser.add_argument('--client_num', type=int, default=5,
                       help='å®¢æˆ·ç«¯æ•°é‡')
    args = parser.parse_args()
    
    # ç”Ÿæˆå¯†é’¥çŸ©é˜µè·¯å¾„
    from utils.key_matrix_utils import get_key_matrix_path
    args.key_matrix_path = get_key_matrix_path(args.key_matrix_dir, args.model_type, args.client_num)
    
    # è®¾ç½®è®¾å¤‡
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    # æ¨¡å‹è·¯å¾„
    model_path = args.model_path
    
    # å¯†é’¥çŸ©é˜µç›®å½•
    key_matrix_dir = args.key_matrix_path
    autoencoder_dir = args.autoencoder_dir
    
    # åŠ è½½ä¸»ä»»åŠ¡æ¨¡å‹
    model, model_info = load_main_task_model(model_path, device)
    
    # æ ¹æ®æ¨¡å‹ä¿¡æ¯åŠ è½½ç›¸åº”çš„æµ‹è¯•æ•°æ®
    dataset_name = model_info.get('dataset', 'chestmnist')
    test_loader = load_test_data(dataset_name, batch_size=128)
    
    if model is not None and model_info is not None:
        print(f"å¼€å§‹å‰ªææ”»å‡»å®éªŒ (è®¾å¤‡: {device})")
        print(f"æ¨¡å‹ç±»å‹: {args.model_type}, å®¢æˆ·ç«¯æ•°é‡: {args.client_num}")
        print(f"å¯†é’¥çŸ©é˜µè·¯å¾„: {args.key_matrix_path}")
        
        # åˆå§‹åŒ–æ°´å°é‡å»ºå™¨
        reconstructor = WatermarkReconstructor(
            key_matrix_dir=args.key_matrix_path, 
            autoencoder_weights_dir=args.autoencoder_dir
        )
        
        # ==================== æ°´å°æ£€æµ‹å®¹å¿åº¦è®¾ç½® ====================
        PERF_FAIL_RATIO = 0.3
        # =========================================================
        print(f"æ°´å°æ£€æµ‹å®¹å¿åº¦è®¾ç½®: {PERF_FAIL_RATIO}")
        # è®¡ç®—å›ºå®šé˜ˆå€¼Ï„ï¼ˆåŸºäºåŸå§‹æœªå‰ªææ¨¡å‹ï¼‰
        print("è®¡ç®—å›ºå®šé˜ˆå€¼Ï„...")
        fixed_tau = calculate_fixed_tau(model.state_dict(), reconstructor, test_loader, device, perf_fail_ratio=PERF_FAIL_RATIO)
        if fixed_tau is None:
            print("âŒ æ— æ³•è®¡ç®—å›ºå®šé˜ˆå€¼ï¼Œå°†ä½¿ç”¨åŠ¨æ€é˜ˆå€¼")
        
        # å®šä¹‰å‰ªææ¯”ä¾‹ï¼šä»0%åˆ°100%ï¼Œæ­¥é•¿10%
        pruning_ratios = [i/10.0 for i in range(0, 11)]  # [0.0, 0.1, 0.2, ..., 1.0]
        
        # å­˜å‚¨å®éªŒç»“æœ
        results = []
        
        # é¦–å…ˆè¯„ä¼°åŸå§‹æ¨¡å‹çš„æ€§èƒ½
        original_auc = evaluate_model_auc(model, test_loader, device, dataset_type=dataset_name)
        original_accuracy = evaluate_model_accuracy(model, test_loader, device, dataset_type=dataset_name)
        print(f"åŸå§‹æ¨¡å‹AUC: {original_auc:.4f}, å‡†ç¡®ç‡: {original_accuracy:.4f}")
        
        for ratio in pruning_ratios:
            print(f"\n--- å‰ªææ¯”ä¾‹: {ratio:.0%} ---")
            
            # å¯¹æ¨¡å‹è¿›è¡Œå‰ªæ
            pruned_model = threshold_pruning(model, ratio)
            
            # è¯„ä¼°å‰ªæåæ¨¡å‹çš„AUCå’Œå‡†ç¡®ç‡
            pruned_auc = evaluate_model_auc(pruned_model, test_loader, device, dataset_type=dataset_name)
            pruned_accuracy = evaluate_model_accuracy(pruned_model, test_loader, device, dataset_type=dataset_name)
            
            print(f"å‰ªæåæ¨¡å‹AUC: {pruned_auc:.4f}, å‡†ç¡®ç‡: {pruned_accuracy:.4f}")
            
            # è¯„ä¼°æ°´å°å®Œæ•´æ€§
            watermark_result = evaluate_watermark_after_pruning(pruned_model, reconstructor)
            
            # è¯„ä¼°Î”PCC
            # ä½¿ç”¨åŸå§‹æ¨¡å‹ä½œä¸ºåŸºå‡†ï¼Œæ¯”è¾ƒå‰ªæå‰åçš„æ€§èƒ½
            original_state = model.state_dict()
            pruned_state = pruned_model.state_dict()
            
            # ğŸ” è°ƒè¯•ï¼šæ£€æŸ¥ä¸¤ä¸ªçŠ¶æ€å­—å…¸æ˜¯å¦çœŸçš„ä¸åŒ
            print(f"    ğŸ” çŠ¶æ€å­—å…¸æ£€æŸ¥: åŸå§‹ID={id(original_state)}, å‰ªæåID={id(pruned_state)}, æ˜¯å¦åŒä¸€å¯¹è±¡={original_state is pruned_state}")
            
            delta_pcc_result = evaluate_delta_pcc(
                original_state, pruned_state, reconstructor, test_loader, device, 
                perf_fail_ratio=PERF_FAIL_RATIO, fixed_tau=fixed_tau
            )
            
            # è®°å½•ç»“æœ
            result = {
                'pruning_ratio': ratio,
                'auc_before': original_auc,
                'auc_after': pruned_auc,
                'accuracy_before': original_accuracy,
                'accuracy_after': pruned_accuracy,
                'accuracy_drop': original_accuracy - pruned_accuracy
            }
            
            if watermark_result is not None:
                result.update({
                    'watermark_integrity': watermark_result['watermark_integrity'],
                    'total_watermark_params': watermark_result['total_watermark_params'],
                    'damaged_watermark_params': watermark_result['damaged_watermark_params']
                })
            else:
                result.update({
                    'watermark_integrity': 0.0,
                    'total_watermark_params': 0,
                    'damaged_watermark_params': 0
                })
            
            # æ·»åŠ Î”PCCç»“æœ
            delta_pcc_default = {
                'perf_before': float('inf'),
                'perf_after': float('inf'),
                'perf_fail': float('inf'),
                'tau': float('inf'),
                'delta_perf': float('inf'),
                'delta_pcc': float('inf'),
                'is_infringement': False,
                'result_text': 'è¯„ä¼°å¤±è´¥'
            }
            result.update(format_delta_pcc_result(delta_pcc_result, delta_pcc_default))
            
            results.append(result)
            
            # æ ¹æ®æ•°æ®é›†ç±»å‹è°ƒæ•´è¾“å‡ºæ ¼å¼
            if dataset_name == 'chestmnist':
                print(f"å‰ªæ{ratio:.0%}: AUC{pruned_auc:.4f} [ä¸»è¦] | å‡†ç¡®ç‡{pruned_accuracy:.4f} [å‚è€ƒ] | æ°´å°å®Œæ•´æ€§{result['watermark_integrity']:.2%} | Î”PCC{result['delta_pcc']:.6f} | {result['result_text']}")
            else:
                print(f"å‰ªæ{ratio:.0%}: AUC{pruned_auc:.4f} [å‚è€ƒ] | å‡†ç¡®ç‡{pruned_accuracy:.4f} [ä¸»è¦] | æ°´å°å®Œæ•´æ€§{result['watermark_integrity']:.2%} | Î”PCC{result['delta_pcc']:.6f} | {result['result_text']}")
        
        # æ ¹æ®æ•°æ®é›†ç±»å‹è°ƒæ•´æ€»ç»“è¡¨æ ¼æ ¼å¼
        print(f"\n{'='*80}")
        print("å®éªŒç»“æœæ€»ç»“")
        print(f"{'='*80}")
        
        if dataset_name == 'chestmnist':
            print(f"{'å‰ªæ%':<6} {'AUC[ä¸»è¦]':<10} {'å‡†ç¡®ç‡[å‚è€ƒ]':<10} {'æ°´å°å®Œæ•´æ€§%':<13} {'Î”PCC':<9} {'ä¾µæƒåˆ¤æ–­':<8}")
            print("-" * 70)
            
            for result in results:
                print(f"{result['pruning_ratio']:>4.0%} "
                      f"{result['auc_after']:>8.4f} "
                      f"{result['accuracy_after']:>8.4f} "
                      f"{result['watermark_integrity']:>11.2%} {result['delta_pcc']:>9.6f} "
                      f"{result['result_text']:>6}")
        else:
            print(f"{'å‰ªæ%':<6} {'AUC[å‚è€ƒ]':<10} {'å‡†ç¡®ç‡[ä¸»è¦]':<10} {'æ°´å°å®Œæ•´æ€§%':<13} {'Î”PCC':<9} {'ä¾µæƒåˆ¤æ–­':<8}")
            print("-" * 70)
            
            for result in results:
                print(f"{result['pruning_ratio']:>4.0%} "
                      f"{result['auc_after']:>8.4f} "
                      f"{result['accuracy_after']:>8.4f} "
                      f"{result['watermark_integrity']:>11.2%} {result['delta_pcc']:>9.6f} "
                      f"{result['result_text']:>6}")
        
        # ä¿å­˜å®éªŒç»“æœ
        print("\nä¿å­˜å®éªŒç»“æœ...")
        save_pruning_results(results, model_info)
            
    else:
        print("ä¸»ä»»åŠ¡æ¨¡å‹åŠ è½½å¤±è´¥")

if __name__ == '__main__':
    main()
