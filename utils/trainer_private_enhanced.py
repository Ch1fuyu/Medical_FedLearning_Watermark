import os
import numpy as np
import torch
import torch.nn.functional as F
from sklearn.metrics import roc_auc_score

from models.light_autoencoder import LightAutoencoder
from models.losses.multi_loss import MultiLoss
from utils.key_matrix_utils import KeyMatrixManager
from utils.mask_utils import MaskManager


def accuracy(output, target, top_k=(1,)):
    """è®¡ç®—å¤šæ ‡ç­¾åˆ†ç±»çš„å‡†ç¡®ç‡"""
    with torch.no_grad():
        pred_prob = torch.sigmoid(output)
        pred_binary = pred_prob > 0.5
        
        # æ ‡ç­¾çº§å‡†ç¡®ç‡
        label_correct = (pred_binary == target).float().mean()
        # æ ·æœ¬çº§å‡†ç¡®ç‡
        sample_correct = torch.all(pred_binary == target, dim=1).float().mean()
        
        return [label_correct * 100.0, sample_correct * 100.0]

class TesterPrivate:
    """æµ‹è¯•å™¨ç±»ï¼Œç”¨äºæ¨¡å‹è¯„ä¼°"""
    
    def __init__(self, model, device, verbose=False):
        self.model = model
        self.device = device
        self.verbose = verbose

    def test(self, dataloader):
        """æµ‹è¯•æ¨¡å‹æ€§èƒ½"""
        self.model.to(self.device)
        self.model.eval()

        loss_meter = 0
        acc_meter = 0
        sample_acc_meter = 0
        run_count = 0
        
        all_y_true = []
        all_y_score = []

        with torch.no_grad():
            for load in dataloader:
                data, target = load[:2]
                data = data.to(self.device)
                target = target.to(self.device)

                pred = self.model(data)
                
                loss_meter += F.binary_cross_entropy_with_logits(pred, target.float(), reduction='sum').item()
                acc_results = accuracy(pred, target)
                label_acc = acc_results[0]
                sample_acc = acc_results[1]
                acc_meter += label_acc.item() * data.size(0) / 100.0
                sample_acc_meter += sample_acc.item() * data.size(0) / 100.0
                
                pred_prob = torch.sigmoid(pred)
                
                if self.verbose and run_count == 0:
                    pred_normal = torch.all(pred_prob < 0.5, dim=1).sum().item()
                    print(f"First batch - Label-level accuracy: {label_acc:.4f}%, Sample-level accuracy: {sample_acc:.4f}%, Predicted normal samples: {pred_normal}/{data.size(0)}")
                
                all_y_true.append(target.detach().cpu().numpy())
                all_y_score.append(pred_prob.detach().cpu().numpy())
                run_count += data.size(0)

        loss_meter /= run_count
        acc_meter /= run_count
        sample_acc_meter /= run_count

        if hasattr(acc_meter, 'item'):
            acc_meter = acc_meter.item()

        # è®¡ç®—AUC
        auc_val = 0.0
        if len(all_y_true) > 0:
            try:
                y_true_all = np.concatenate(all_y_true, axis=0)
                y_score_all = np.concatenate(all_y_score, axis=0)
                
                valid_classes = []
                for i in range(y_true_all.shape[1]):
                    if len(np.unique(y_true_all[:, i])) > 1:
                        valid_classes.append(i)
                
                if len(valid_classes) > 0:
                    auc_scores = []
                    for i in valid_classes:
                        try:
                            auc_i = roc_auc_score(y_true_all[:, i], y_score_all[:, i])
                            auc_scores.append(auc_i)
                        except Exception:
                            continue
                    
                    if len(auc_scores) > 0:
                        auc_val = np.mean(auc_scores)
            except Exception as e:
                print(f"AUCè®¡ç®—é”™è¯¯: {e}")
                auc_val = 0.0

        return loss_meter, acc_meter, float(auc_val), sample_acc_meter

class TrainerPrivateEnhanced:
    """å¢å¼ºç‰ˆè®­ç»ƒå™¨ï¼Œæ”¯æŒMultiLosså’Œè‡ªç¼–ç å™¨æ°´å°"""
    
    def __init__(self, model, device, dp, sigma, random_positions, args=None):
        self.optimizer = None
        self.model = model
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.tester = TesterPrivate(model, device)
        self.dp = dp
        self.sigma = sigma
        self.random_positions = random_positions
        self.args = args
        self._key_manager = None
        self.position_dict = random_positions
        
        # MultiLosså’Œæ©ç ç®¡ç†å™¨
        self.multi_loss = MultiLoss()
        self.mask_manager = None
        self.autoencoder = None
        
        # åˆå§‹åŒ–æ©ç ç®¡ç†å™¨
        if args and getattr(args, 'use_key_matrix', False):
            try:
                from utils.mask_utils import create_mask_manager
                self.mask_manager = create_mask_manager(model, args.key_matrix_dir, args)
                # åˆå§‹åŒ–æ—¶æ›´æ–°æ‰€æœ‰å®¢æˆ·ç«¯çš„ç¼–ç å™¨æ©ç 
                if self.mask_manager:
                    self.mask_manager.update_encoder_mask()  # ä¸ä¼ client_idï¼Œæ›´æ–°æ‰€æœ‰å®¢æˆ·ç«¯
            except Exception as e:
                print(f"åˆå§‹åŒ–æ©ç ç®¡ç†å™¨å¤±è´¥: {e}")
                self.mask_manager = None
        
        # åˆå§‹åŒ–è‡ªç¼–ç å™¨ï¼ˆå¦‚æœä½¿ç”¨å¢å¼ºæ°´å°æ¨¡å¼ï¼‰
        if args and getattr(args, 'watermark_mode', '') == 'enhanced':
            try:
                self._initialize_autoencoder()
                print("âœ“ è‡ªç¼–ç å™¨å·²è‡ªåŠ¨åˆå§‹åŒ–")
            except Exception as e:
                print(f"åˆå§‹åŒ–è‡ªç¼–ç å™¨å¤±è´¥: {e}")
                self.autoencoder = None

    def get_loss_function(self, pred, target):
        """è®¡ç®—æŸå¤±å‡½æ•°ï¼Œæ”¯æŒç±»åˆ«æƒé‡"""
        if self.args is None or not self.args.class_weights:
            return F.binary_cross_entropy_with_logits(pred, target.float())
        
        pos_counts = target.sum(dim=0)
        neg_counts = target.shape[0] - pos_counts
        pos_weights = torch.zeros_like(pos_counts, dtype=torch.float32, device=pred.device)
        
        for i in range(len(pos_counts)):
            if pos_counts[i] > 0 and neg_counts[i] > 0:
                pos_weights[i] = (neg_counts[i] / pos_counts[i]) * self.args.pos_weight_factor
            else:
                pos_weights[i] = 1.0
        
        pos_weights = torch.clamp(pos_weights, min=0.1, max=10.0)
        return F.binary_cross_entropy_with_logits(pred, target.float(), pos_weight=pos_weights)

    def _initialize_autoencoder(self):
        """åˆå§‹åŒ–è‡ªç¼–ç å™¨ï¼Œåˆ†åˆ«åŠ è½½ç¼–ç å™¨å’Œè§£ç å™¨"""
        if self.autoencoder is None:
            # ä½¿ç”¨å•é€šé“è¾“å…¥çš„è‡ªç¼–ç å™¨
            self.autoencoder = LightAutoencoder(input_channels=1).to(self.device)
            
            # æ£€æŸ¥æ˜¯å¦æœ‰è‡ªç¼–ç å™¨æƒé‡
            weights_dir = './save/autoencoder'
            encoder_path = os.path.join(weights_dir, 'encoder.pth')
            decoder_path = os.path.join(weights_dir, 'decoder.pth')
            
            if os.path.exists(encoder_path) and os.path.exists(decoder_path):
                print("âœ“ è‡ªç¼–ç å™¨æƒé‡å·²åŠ è½½")
                load_weights = True
            else:
                print("âš ï¸ ä½¿ç”¨éšæœºåˆå§‹åŒ–æƒé‡")
                load_weights = False
            
            # åªåœ¨æœ‰å…¼å®¹æƒé‡æ—¶æ‰åŠ è½½
            if load_weights:
                # åŠ è½½ç¼–ç å™¨
                if os.path.exists(encoder_path):
                    self.autoencoder.encoder.load_state_dict(
                        torch.load(encoder_path, map_location=self.device, weights_only=False)
                    )
                
                # åŠ è½½è§£ç å™¨
                if os.path.exists(decoder_path):
                    self.autoencoder.decoder.load_state_dict(
                        torch.load(decoder_path, map_location=self.device, weights_only=False)
                    )
            # ä½¿ç”¨éšæœºåˆå§‹åŒ–çš„è‡ªç¼–ç å™¨æƒé‡
    
    def _fine_tune_autoencoder(self, epochs=1, lr=0.005):
        """åœ¨æ¯è½®è”é‚¦å­¦ä¹ å¼€å§‹å‰å¾®è°ƒè‡ªç¼–ç å™¨ï¼Œç¡®ä¿æ€§èƒ½ç¨³å®š
        æ³¨æ„ï¼šåªåœ¨å†…å­˜ä¸­æ›´æ–°ç¼–ç å™¨å‚æ•°ï¼Œä¸ä¿®æ”¹åŸå§‹.pthæ–‡ä»¶
        è§£ç å™¨å‚æ•°ä¿æŒä¸å˜ï¼Œç”±ç¬¬ä¸‰æ–¹ä¿ç®¡
        """
        if self.autoencoder is None:
            self._initialize_autoencoder()
        
        # ä½¿ç”¨å¤–éƒ¨è‡ªç¼–ç å™¨å¾®è°ƒæ¨¡å—
        from .autoencoder_finetuner import finetune_autoencoder_encoder
        
        success = finetune_autoencoder_encoder(
            autoencoder=self.autoencoder,
            device=self.device,
            epochs=epochs,
            lr=lr,
            batch_size=128
        )
        
        # ç®€åŒ–è¾“å‡ºï¼šç§»é™¤å†—ä½™ä¿¡æ¯


    def _extract_encoder_parameters(self):
        """æå–ç¼–ç å™¨å‚æ•°ä½œä¸ºæ°´å°"""
        if self.autoencoder is None:
            self._initialize_autoencoder()
        
        # ä½¿ç”¨å¤–éƒ¨æ¨¡å—æå–ç¼–ç å™¨å‚æ•°
        from .autoencoder_finetuner import extract_encoder_parameters
        return extract_encoder_parameters(self.autoencoder)

    def _embed_watermark(self, client_id, current_epoch):
        """åµŒå…¥æ°´å°åˆ°ç›®æ ‡æ¨¡å‹"""
        if not self.args or not getattr(self.args, 'use_key_matrix', False):
            return
        
        try:
            if self.mask_manager:
                self.mask_manager.update_encoder_mask(client_id)
            
            encoder_params = self._extract_encoder_parameters()
            if encoder_params is None:
                return
            
            if self._key_manager is None:
                try:
                    self._key_manager = KeyMatrixManager(
                        self.args.key_matrix_dir,
                        args=self.args
                    )
                except Exception as e:
                    print(f"åŠ è½½å¯†é’¥çŸ©é˜µç®¡ç†å™¨å¤±è´¥: {e}")
                    return
            
            try:
                position_dict = self._key_manager.load_positions(client_id)
            except Exception as e:
                print(f"åŠ è½½å®¢æˆ·ç«¯ {client_id} ä½ç½®ä¿¡æ¯å¤±è´¥: {e}")
                return
            
            with torch.no_grad():
                model_params = dict(self.model.named_parameters())
                watermarked_params = self._key_manager.embed_watermark(
                    model_params, client_id, encoder_params
                )
                
                for name, param in self.model.named_parameters():
                    if name in watermarked_params:
                        param.data.copy_(watermarked_params[name])
                
                print(f"ğŸ”§ æ°´å°åµŒå…¥å®Œæˆï¼Œä½¿ç”¨KeyMatrixManagerè‡ªåŠ¨ç¼©æ”¾")
                                
        except Exception as e:
            print(f"åµŒå…¥æ°´å°å¤±è´¥: {e}")

    def local_update(self, dataloader, local_ep, lr, client_id, current_epoch=0, total_epochs=100):
        """æœ¬åœ°æ›´æ–°ï¼Œæ”¯æŒMultiLosså’Œè‡ªç¼–ç å™¨è®­ç»ƒ"""
        # åœ¨æ¯è½®è”é‚¦å­¦ä¹ å¼€å§‹å‰å¾®è°ƒè‡ªç¼–ç å™¨ï¼ˆä»…åœ¨ç¬¬ä¸€ä¸ªå®¢æˆ·ç«¯æ‰§è¡Œï¼Œé¿å…é‡å¤ï¼‰
        if client_id == 0 and current_epoch == 0:
            # å¾®è°ƒè‡ªç¼–ç å™¨ï¼ˆé™é»˜æ‰§è¡Œï¼‰
            self._fine_tune_autoencoder(epochs=1, lr=0.005)
        
        self.model.train()
        self.optimizer = torch.optim.SGD(self.model.parameters(), lr=lr, momentum=0.9)

        epoch_loss, epoch_acc = [], []

        for epoch in range(local_ep):
            loss_meter = 0.0
            acc_meter = 0.0
            run_count = 0

            for batch_idx, (x, y) in enumerate(dataloader):
                x, y = x.to(self.device), y.to(self.device)
                self.optimizer.zero_grad()

                pred = self.model(x)
                main_loss = self.get_loss_function(pred, y)

                if current_epoch == 0:
                    total_loss = main_loss
                else:
                    total_loss = self.multi_loss.compute_loss(main_loss, current_epoch, total_epochs)

                total_loss.backward()

                if current_epoch > 0 and self.mask_manager:
                    try:
                        gradients = torch.cat([p.grad.view(-1) for p in self.model.parameters()])
                        target_mask, encoder_mask, effective_mask = self.mask_manager.get_masks(self.device)
                        
                        # è®¡ç®—ç¼–ç å™¨åŒºåŸŸçš„æ¢¯åº¦ï¼ˆç”¨äºprevGHï¼‰
                        encoder_gradients = torch.mul(gradients, effective_mask)
                        
                        self.multi_loss.update_gradient_stats(
                            gradients, encoder_gradients, target_mask, encoder_mask, effective_mask
                        )
                    except Exception as e:
                        print(f"æ›´æ–°æ¢¯åº¦ç»Ÿè®¡å¤±è´¥: {e}")

                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
                self.optimizer.step()

                acc_results = accuracy(pred, y)
                acc_meter += acc_results[0].item() * x.size(0) / 100.0
                loss_meter += main_loss.item() * x.size(0)
                run_count += x.size(0)

            loss_meter /= run_count
            acc_meter /= run_count

            epoch_loss.append(loss_meter)
            epoch_acc.append(acc_meter)

            if epoch + 1 == local_ep:
                print(f"Client {client_id} - Epoch {epoch+1}/{local_ep}: "
                      f"Loss={loss_meter:.4f}, Acc={acc_meter:.4f}, LR={lr:.6f}")

        # æ¯5ä¸ªepochè¿›è¡Œæ°´å°èåˆ
        if (current_epoch + 1) % 5 == 0:
            self._embed_watermark(client_id, current_epoch)
            print(f"Client {client_id} - Watermark embedded at epoch {current_epoch + 1}")

        # å·®åˆ†éšç§å™ªå£°
        if self.dp:
            for param in self.model.parameters():
                param.data = param.data + torch.normal(torch.zeros(param.size()), self.sigma).to(self.device)

        return self.model.state_dict(), np.mean(epoch_loss), np.mean(epoch_acc)

    def test(self, dataloader):
        """æµ‹è¯•æ¨¡å‹"""
        return self.tester.test(dataloader)
